{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6222232a"
      },
      "source": [
        "# Attentional Networks in Computer Vision\n",
        "\n",
        "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
        "\n",
        "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
        "\n",
        "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
      ],
      "id": "6222232a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08b5a0e2"
      },
      "source": [
        "# Part I. Preparation\n",
        "\n",
        "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
        "\n",
        "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
      ],
      "id": "08b5a0e2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9f04db01"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np"
      ],
      "id": "9f04db01"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "9abbbe7d21a941e5aebb8c5cdc615d2b",
            "1c14e91c65e64acbabfa94a7ebd0236d",
            "552c3089fce24d3a8373550a8a03f51f",
            "46742caa7e7e4ae3aa8044fa86578ac0",
            "ecf9f1794d534c62b1dd11f3cb6db8b8",
            "cb9a53244e5743bf9fc2bfdc654a365f",
            "109ce988c5174e22b2f122819a37b982",
            "f9763b792e4f4e278be75608c10f73ac",
            "7aa0066fd1f74602ae8787ee1e7f0740",
            "4df72aa4154443bf9a178de78035b89a",
            "d411c27c8c464da29d7a20cc2b425d2f"
          ]
        },
        "id": "688da876",
        "outputId": "9e14461a-b321-4b3f-a97d-9e71a17363e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./comp411/datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9abbbe7d21a941e5aebb8c5cdc615d2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./comp411/datasets/cifar-10-python.tar.gz to ./comp411/datasets\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_TRAIN = 49000\n",
        "\n",
        "# The torchvision.transforms package provides tools for preprocessing data\n",
        "# and for performing data augmentation; here we set up a transform to\n",
        "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
        "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
        "transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "\n",
        "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
        "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
        "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
        "# training set into train and val sets by passing a Sampler object to the\n",
        "# DataLoader telling how it should sample from the underlying Dataset.\n",
        "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
        "                             transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
        "                           transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
        "                            transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)"
      ],
      "id": "688da876"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7171ad7"
      },
      "source": [
        "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
        "\n",
        "The global variables `dtype` and `device` will control the data types throughout this assignment. "
      ],
      "id": "b7171ad7"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5de4376",
        "outputId": "50e1fcb9-24aa-4921-f3d9-f5c6c27efd22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ],
      "id": "b5de4376"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "754a8c82"
      },
      "source": [
        "# Part II. Barebones Transformers: Self-Attentional Layer\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
        "\n",
        "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
        "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
        "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
        "\n",
        "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
        "\n",
        "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
        "\n",
        "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
        "\n",
        "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
        "\n",
        "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
        "\n",
        "5. Reassemble heads into one flat vector and return the output.\n",
        "\n",
        "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
      ],
      "id": "754a8c82"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "79a63f8a"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        \n",
        "        ## initialize module's instance variables\n",
        "        self.input_dims = input_dims\n",
        "        self.head_dims = head_dims\n",
        "        self.num_heads = num_heads\n",
        "        self.proj_dims = head_dims * num_heads\n",
        "        \n",
        "        ## Declare module's parameters\n",
        "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Input of shape, [B, N, D] where:\n",
        "        ## - B denotes the batch size\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
        "        ## - D corresponds to model dimensionality\n",
        "        b,n,d = x.shape\n",
        "        \n",
        "        ## Construct queries,keys,values\n",
        "        q_ = self.W_Q(x)\n",
        "        k_ = self.W_K(x)\n",
        "        v_ = self.W_V(x)\n",
        "        \n",
        "        ## Seperate q,k,v into their corresponding heads,\n",
        "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
        "        ## - B denotes the batch size\n",
        "        ## - H denotes number of heads\n",
        "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
        "        ## - D//H corresponds to per head dimensionality\n",
        "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
        "       \n",
        "        #########################################################################################\n",
        "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
        "        #########################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        ## Compute attention logits. Note that this operation is conducted as a\n",
        "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
        "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
        "        ## Output Attention logits should have the size: [B,H,N,N]\n",
        "\n",
        "        attent_logit =  torch.matmul(q , k.transpose(-2, -1))\n",
        "\n",
        "        scaling = 1/(self.head_dims) ** (1/2)\n",
        "        attent_logit= attent_logit*scaling\n",
        "        pass\n",
        "        \n",
        "        ## Compute attention Weights. Note that this operation is conducted as a\n",
        "        ## Softmax Normalization across the keys dimension. \n",
        "        ## Hint: You can apply the Softmax operation across the final dimension\n",
        "\n",
        "        att = F.softmax(attent_logit, dim=-1) \n",
        "        pass\n",
        "        \n",
        "        ## Compute output values. Note that this operation is conducted as a \n",
        "        ## batched matrix multiplication between the Attention Weights matrix and \n",
        "        ## the values tensor. After computing output values, the output should be reshaped\n",
        "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
        "        ## Output should be of size [B, N, D]\n",
        "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
        "\n",
        "        # sum up over the third axis\n",
        "\n",
        "        out = torch.matmul(att,v).permute(0,2,1,3)\n",
        "\n",
        "        attn_out = torch.reshape(out, (b,n,self.proj_dims))\n",
        "     \n",
        "\n",
        "        pass\n",
        "        \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             \n",
        "        ################################################################################\n",
        "    \n",
        "        return attn_out"
      ],
      "id": "79a63f8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5af64053"
      },
      "source": [
        "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ],
      "id": "5af64053"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edd71dfb",
        "outputId": "dae618e6-c988-4d11-fb8b-87aa330f3a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16, 256])\n"
          ]
        }
      ],
      "source": [
        "def test_self_attn_layer():\n",
        "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
        "    layer = SelfAttention(32,64,4)\n",
        "    out = layer(x)\n",
        "    print(out.size())  # you should see [64,16,256]\n",
        "test_self_attn_layer()"
      ],
      "id": "edd71dfb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0803090"
      },
      "source": [
        "# Part III. Barebones Transformers: Transformer Encoder Block\n",
        "\n",
        "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
        "\n"
      ],
      "id": "b0803090"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b8a51a7c"
      },
      "outputs": [],
      "source": [
        "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
        "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        o = F.elu(self.fc_1(x))\n",
        "        o = self.fc_2(o)\n",
        "        return o"
      ],
      "id": "b8a51a7c"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1b48c6d3"
      },
      "outputs": [],
      "source": [
        "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
        "## module follows a simple computational pipeline:\n",
        "## input --> layernorm --> SelfAttention --> skip connection \n",
        "##       --> layernorm --> MLP ---> skip connection ---> output\n",
        "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
        "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
        "## to the SelfAttention block.\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "    ###############################################################\n",
        "    # TODO: Complete the consturctor of  TransformerBlock module  #\n",
        "    ###############################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "\n",
        "        self.attention_layer = SelfAttention(input_dims = hidden_dims , head_dims= hidden_dims//num_heads , num_heads=num_heads,  bias=bias)\n",
        "        self.norm_layer1 = nn.LayerNorm(hidden_dims) \n",
        "        self.linear_layer = nn.Linear(hidden_dims*num_heads, hidden_dims)\n",
        "        self.MLP =  MLP(hidden_dims,hidden_dims,hidden_dims)\n",
        "        pass\n",
        "        \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###################################################################\n",
        "    #                                 END OF YOUR CODE                #             \n",
        "    ###################################################################\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "    ##############################################################\n",
        "    # TODO: Complete the forward of TransformerBlock module      #\n",
        "    ##############################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
        "\n",
        "        norm1 = self.norm_layer1(x)\n",
        "   \n",
        "        attention = self.attention_layer(norm1)\n",
        "   \n",
        "        attent_skip  =  attention + x\n",
        "\n",
        "        norm2 = self.norm_layer1(attent_skip)\n",
        "        mlp = self.MLP(norm2)\n",
        "        MLP_skip  = attent_skip + mlp\n",
        "       \n",
        "        output = MLP_skip\n",
        "\n",
        "\n",
        "        pass\n",
        "        return output\n",
        "        \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###################################################################\n",
        "    #                                 END OF YOUR CODE                #             \n",
        "    ###################################################################"
      ],
      "id": "1b48c6d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e0c80b3"
      },
      "source": [
        "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ],
      "id": "8e0c80b3"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9ac80a9",
        "outputId": "69798d43-242d-42f8-86bb-8b13e8247f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16, 128])\n"
          ]
        }
      ],
      "source": [
        "def test_transfomerblock_layer():\n",
        "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
        "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
        "    out = layer(x)\n",
        "    print(out.size())  # you should see [64,16,128]\n",
        "test_transfomerblock_layer()"
      ],
      "id": "f9ac80a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "916ef578"
      },
      "source": [
        "# Part IV The Vision Transformer (ViT)\n",
        "\n",
        "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
      ],
      "id": "916ef578"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fa07d697"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
        "        super(ViT, self).__init__()\n",
        "                \n",
        "        ## initialize module's instance variables\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.input_dims = input_dims\n",
        "        self.output_dims = output_dims\n",
        "        self.num_trans_layers = num_trans_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.image_k = image_k\n",
        "        self.patch_k = patch_k\n",
        "        \n",
        "        self.image_height = self.image_width = image_k\n",
        "        self.patch_height = self.patch_width = patch_k\n",
        "        \n",
        "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
        "                'Image size must be divisible by the patch size.'\n",
        "\n",
        "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
        "        self.patch_flat_len = self.patch_height * self.patch_width\n",
        "        \n",
        "        ## Declare module's parameters\n",
        "        \n",
        "        ## ViT's flattened patch embedding projection:\n",
        "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
        "        \n",
        "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
        "        \n",
        "        ## Learnable classt token and its index among attention sequence elements.\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
        "        self.cls_index = torch.LongTensor([0])\n",
        "        \n",
        "        ## Declare cascaded Transformer blocks:\n",
        "        transformer_encoder_list = []\n",
        "        for _ in range(self.num_trans_layers):\n",
        "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
        "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
        "        \n",
        "        ## Declare the output mlp:\n",
        "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
        "         \n",
        "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
        "        ## Create sliding window pathes using nn.Functional.unfold\n",
        "        ## Input dimensions: [B,D,H,W] where\n",
        "        ## --B : input batch size\n",
        "        ## --D : input channels\n",
        "        ## --H, W: input height and width\n",
        "        ## Output dimensions: [B,N,H*W,D]\n",
        "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
        "        ##      sliding window stride and padding.\n",
        "        b,d,h,w = x.shape\n",
        "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
        "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
        "        n = x_unf.size(1)\n",
        "        return x_unf,n\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b = x.size(0)\n",
        "        ## create sliding window patches from the input image\n",
        "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
        "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
        "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
        "        ## linearly embed each flattened patch\n",
        "        x_embed = self.linear_embed(x_patch_flat)\n",
        "        \n",
        "        ## retrieve class token \n",
        "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
        "        ## concatanate class token to input patches\n",
        "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
        "        \n",
        "        ## add positional embedding to input patches + class token \n",
        "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
        "        \n",
        "        ## pass through the transformer encoder\n",
        "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
        "        \n",
        "        ## select the class token \n",
        "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
        "        \n",
        "        ## create output\n",
        "        out = self.out_mlp(out_cls_token)\n",
        "        \n",
        "        return out.squeeze(-2)"
      ],
      "id": "fa07d697"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160e9abc"
      },
      "source": [
        "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
        "\n",
        "When you run this function, output should have shape (64, 16, 64)."
      ],
      "id": "160e9abc"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02afa746",
        "outputId": "6fe5c4b7-ff46-49f7-9fd2-0a8f41b677d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "def test_vit():\n",
        "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
        "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
        "    out = model(x)\n",
        "    print(out.size())  # you should see [64,10]\n",
        "test_vit()"
      ],
      "id": "02afa746"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7f3dd00"
      },
      "source": [
        "# Part V. Train the ViT"
      ],
      "id": "e7f3dd00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ac4301"
      },
      "source": [
        "### Check Accuracy\n",
        "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
        "\n",
        "The check_batch_accuracy function is provided for you below:"
      ],
      "id": "36ac4301"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dc6edcd7"
      },
      "outputs": [],
      "source": [
        "def check_batch_accuracy(out, target,eps=1e-7):\n",
        "    b, c = out.shape\n",
        "    with torch.no_grad():\n",
        "        _, pred = out.max(-1) \n",
        "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
        "    return correct, np.float(correct) / (b)"
      ],
      "id": "dc6edcd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ff388e4"
      },
      "source": [
        "### Training Loop\n",
        "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
      ],
      "id": "3ff388e4"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "158cd33e"
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer, trainloader):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "    \n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
        "    \n",
        "    Returns: overall training accuracy for the epoch\n",
        "    \"\"\"\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    network.train()  # put model to training mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
        "            \n",
        "        outputs = network(inputs)\n",
        "        loss =  F.cross_entropy(outputs, targets)\n",
        "            \n",
        "        # Zero out all of the gradients for the variables which the optimizer\n",
        "        # will update.\n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        # This is the backwards pass: compute the gradient of the loss with\n",
        "        # respect to each  parameter of the model.\n",
        "        loss.backward()\n",
        "            \n",
        "        # Actually update the parameters of the model using the gradients\n",
        "        # computed by the backwards pass.\n",
        "        optimizer.step()\n",
        "            \n",
        "        loss = loss.detach()\n",
        "        train_loss += loss.item()\n",
        "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
        "        correct += correct_p\n",
        "        total += targets.size(0)\n",
        "\n",
        "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        \n",
        "    return 100.*correct/total"
      ],
      "id": "158cd33e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd10534c"
      },
      "source": [
        "### Evaluation Loop\n",
        "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
      ],
      "id": "dd10534c"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9345e6c9"
      },
      "outputs": [],
      "source": [
        "def evaluate(network, evalloader):\n",
        "    \"\"\"\n",
        "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
        "    \n",
        "    Inputs:\n",
        "    - network: A PyTorch Module giving the model to train.\n",
        "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
        "    \n",
        "    Returns: overall evaluation accuracy for the epoch\n",
        "    \"\"\"\n",
        "    network.eval() # put model to evaluation mode\n",
        "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n---- Evaluation in process ----')\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
        "            outputs = network(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
        "            correct += correct_p\n",
        "            total += targets.size(0)\n",
        "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return 100.*correct/total"
      ],
      "id": "9345e6c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03e63b67"
      },
      "source": [
        "### Overfit a ViT\n",
        "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
        "\n",
        "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
        "\n",
        "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
        "\n",
        "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
      ],
      "id": "03e63b67"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d903afa",
        "outputId": "d3eef957-791b-459d-acbc-822aa64ed63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
            "==> Data ready, batchsize = 25\n"
          ]
        }
      ],
      "source": [
        "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
        "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
        "\n",
        "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
        "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
        "\n",
        "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
        "\n",
        "batch_size_sub = 25\n",
        "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
        "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
        "\n",
        "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
      ],
      "id": "4d903afa"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9a3430b",
        "scrolled": true,
        "outputId": "8115039b-2c69-4ed5-b84b-f21717622b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return correct, np.float(correct) / (b)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3.316 | Acc: 4.000% (1/25)\n",
            "Loss: 6.270 | Acc: 12.000% (6/50)\n",
            "Loss: 5.851 | Acc: 9.333% (7/75)\n",
            "Loss: 5.827 | Acc: 12.000% (12/100)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 12.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 4.865 | Acc: 8.000% (2/25)\n",
            "Loss: 5.004 | Acc: 10.000% (5/50)\n",
            "Loss: 4.775 | Acc: 12.000% (9/75)\n",
            "Loss: 4.961 | Acc: 13.000% (13/100)\n",
            "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 13.0\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 3.828 | Acc: 20.000% (5/25)\n",
            "Loss: 4.377 | Acc: 16.000% (8/50)\n",
            "Loss: 3.979 | Acc: 13.333% (10/75)\n",
            "Loss: 3.683 | Acc: 15.000% (15/100)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 15.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 3.371 | Acc: 8.000% (2/25)\n",
            "Loss: 3.003 | Acc: 20.000% (10/50)\n",
            "Loss: 3.121 | Acc: 17.333% (13/75)\n",
            "Loss: 2.954 | Acc: 18.000% (18/100)\n",
            "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 18.0\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 2.886 | Acc: 24.000% (6/25)\n",
            "Loss: 2.726 | Acc: 26.000% (13/50)\n",
            "Loss: 2.795 | Acc: 22.667% (17/75)\n",
            "Loss: 2.766 | Acc: 21.000% (21/100)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 21.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.514 | Acc: 24.000% (6/25)\n",
            "Loss: 2.477 | Acc: 26.000% (13/50)\n",
            "Loss: 2.467 | Acc: 20.000% (15/75)\n",
            "Loss: 2.529 | Acc: 17.000% (17/100)\n",
            "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 17.0\n",
            "\n",
            "Epoch: 3\n",
            "Loss: 1.965 | Acc: 16.000% (4/25)\n",
            "Loss: 1.975 | Acc: 20.000% (10/50)\n",
            "Loss: 2.071 | Acc: 22.667% (17/75)\n",
            "Loss: 2.083 | Acc: 24.000% (24/100)\n",
            "Epoch 3 of training is completed, Training accuracy for this epoch is 24.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.615 | Acc: 12.000% (3/25)\n",
            "Loss: 2.505 | Acc: 24.000% (12/50)\n",
            "Loss: 2.596 | Acc: 18.667% (14/75)\n",
            "Loss: 2.600 | Acc: 17.000% (17/100)\n",
            "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 17.0\n",
            "\n",
            "Epoch: 4\n",
            "Loss: 1.351 | Acc: 52.000% (13/25)\n",
            "Loss: 1.477 | Acc: 50.000% (25/50)\n",
            "Loss: 1.727 | Acc: 41.333% (31/75)\n",
            "Loss: 1.775 | Acc: 39.000% (39/100)\n",
            "Epoch 4 of training is completed, Training accuracy for this epoch is 39.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.989 | Acc: 32.000% (8/25)\n",
            "Loss: 2.097 | Acc: 30.000% (15/50)\n",
            "Loss: 2.248 | Acc: 29.333% (22/75)\n",
            "Loss: 2.237 | Acc: 30.000% (30/100)\n",
            "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 30.0\n",
            "\n",
            "Epoch: 5\n",
            "Loss: 1.846 | Acc: 36.000% (9/25)\n",
            "Loss: 1.698 | Acc: 40.000% (20/50)\n",
            "Loss: 1.659 | Acc: 42.667% (32/75)\n",
            "Loss: 1.606 | Acc: 44.000% (44/100)\n",
            "Epoch 5 of training is completed, Training accuracy for this epoch is 44.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.583 | Acc: 8.000% (2/25)\n",
            "Loss: 2.293 | Acc: 24.000% (12/50)\n",
            "Loss: 2.342 | Acc: 22.667% (17/75)\n",
            "Loss: 2.391 | Acc: 22.000% (22/100)\n",
            "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 22.0\n",
            "\n",
            "Epoch: 6\n",
            "Loss: 1.337 | Acc: 56.000% (14/25)\n",
            "Loss: 1.306 | Acc: 60.000% (30/50)\n",
            "Loss: 1.223 | Acc: 60.000% (45/75)\n",
            "Loss: 1.230 | Acc: 57.000% (57/100)\n",
            "Epoch 6 of training is completed, Training accuracy for this epoch is 57.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.113 | Acc: 16.000% (4/25)\n",
            "Loss: 2.100 | Acc: 24.000% (12/50)\n",
            "Loss: 2.231 | Acc: 28.000% (21/75)\n",
            "Loss: 2.271 | Acc: 30.000% (30/100)\n",
            "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 30.0\n",
            "\n",
            "Epoch: 7\n",
            "Loss: 1.025 | Acc: 72.000% (18/25)\n",
            "Loss: 0.934 | Acc: 70.000% (35/50)\n",
            "Loss: 0.877 | Acc: 73.333% (55/75)\n",
            "Loss: 0.877 | Acc: 74.000% (74/100)\n",
            "Epoch 7 of training is completed, Training accuracy for this epoch is 74.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.327 | Acc: 28.000% (7/25)\n",
            "Loss: 2.445 | Acc: 24.000% (12/50)\n",
            "Loss: 2.392 | Acc: 24.000% (18/75)\n",
            "Loss: 2.466 | Acc: 26.000% (26/100)\n",
            "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 26.0\n",
            "\n",
            "Epoch: 8\n",
            "Loss: 0.718 | Acc: 80.000% (20/25)\n",
            "Loss: 0.645 | Acc: 80.000% (40/50)\n",
            "Loss: 0.654 | Acc: 81.333% (61/75)\n",
            "Loss: 0.583 | Acc: 85.000% (85/100)\n",
            "Epoch 8 of training is completed, Training accuracy for this epoch is 85.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.347 | Acc: 20.000% (5/25)\n",
            "Loss: 2.321 | Acc: 30.000% (15/50)\n",
            "Loss: 2.270 | Acc: 30.667% (23/75)\n",
            "Loss: 2.361 | Acc: 28.000% (28/100)\n",
            "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 28.0\n",
            "\n",
            "Epoch: 9\n",
            "Loss: 0.339 | Acc: 100.000% (25/25)\n",
            "Loss: 0.389 | Acc: 96.000% (48/50)\n",
            "Loss: 0.419 | Acc: 93.333% (70/75)\n",
            "Loss: 0.377 | Acc: 95.000% (95/100)\n",
            "Epoch 9 of training is completed, Training accuracy for this epoch is 95.0\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 2.654 | Acc: 20.000% (5/25)\n",
            "Loss: 2.687 | Acc: 24.000% (12/50)\n",
            "Loss: 2.588 | Acc: 26.667% (20/75)\n",
            "Loss: 2.618 | Acc: 30.000% (30/100)\n",
            "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 30.0\n",
            "\n",
            "Final train set accuracy is 95.0\n",
            "Final val set accuracy is 30.0\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims=10\n",
        "num_trans_layers = 4\n",
        "num_heads=4\n",
        "image_k=32\n",
        "patch_k=4\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims, output_dims=output_dims, num_trans_layers = num_trans_layers, num_heads=num_heads, image_k=image_k, patch_k=patch_k)\n",
        "optimizer = torch.optim.Adam( network.parameters() , lr= learning_rate )\n",
        "    \n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             \n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "eval_accs=[]\n",
        "for epoch in range(10):\n",
        "    tr_acc = train(network, optimizer, trainloader_sub)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))  \n",
        "    \n",
        "    eval_acc = evaluate(network, valloader_sub)\n",
        "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
        "              .format(epoch, eval_acc))  \n",
        "    tr_accs.append(tr_acc)\n",
        "    eval_accs.append(eval_acc)\n",
        "    \n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
      ],
      "id": "a9a3430b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a015c5b"
      },
      "source": [
        "## Train the net\n",
        "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
      ],
      "id": "7a015c5b"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5089f05c",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdddc52-5c94-44bd-f74d-4918f716d289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Loss: 3.882 | Acc: 9.375% (6/64)\n",
            "Loss: 8.786 | Acc: 9.375% (12/128)\n",
            "Loss: 8.110 | Acc: 8.854% (17/192)\n",
            "Loss: 7.651 | Acc: 10.547% (27/256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return correct, np.float(correct) / (b)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 7.126 | Acc: 13.125% (42/320)\n",
            "Loss: 6.738 | Acc: 13.281% (51/384)\n",
            "Loss: 6.342 | Acc: 14.062% (63/448)\n",
            "Loss: 5.941 | Acc: 13.672% (70/512)\n",
            "Loss: 5.691 | Acc: 13.542% (78/576)\n",
            "Loss: 5.387 | Acc: 13.594% (87/640)\n",
            "Loss: 5.150 | Acc: 13.636% (96/704)\n",
            "Loss: 4.952 | Acc: 13.411% (103/768)\n",
            "Loss: 4.769 | Acc: 13.702% (114/832)\n",
            "Loss: 4.618 | Acc: 13.616% (122/896)\n",
            "Loss: 4.459 | Acc: 13.958% (134/960)\n",
            "Loss: 4.327 | Acc: 14.453% (148/1024)\n",
            "Loss: 4.239 | Acc: 14.706% (160/1088)\n",
            "Loss: 4.145 | Acc: 15.104% (174/1152)\n",
            "Loss: 4.064 | Acc: 15.378% (187/1216)\n",
            "Loss: 4.002 | Acc: 15.312% (196/1280)\n",
            "Loss: 3.942 | Acc: 15.699% (211/1344)\n",
            "Loss: 3.872 | Acc: 15.767% (222/1408)\n",
            "Loss: 3.798 | Acc: 15.965% (235/1472)\n",
            "Loss: 3.728 | Acc: 15.820% (243/1536)\n",
            "Loss: 3.674 | Acc: 15.750% (252/1600)\n",
            "Loss: 3.626 | Acc: 15.805% (263/1664)\n",
            "Loss: 3.586 | Acc: 15.567% (269/1728)\n",
            "Loss: 3.539 | Acc: 15.569% (279/1792)\n",
            "Loss: 3.497 | Acc: 15.463% (287/1856)\n",
            "Loss: 3.454 | Acc: 15.781% (303/1920)\n",
            "Loss: 3.409 | Acc: 15.978% (317/1984)\n",
            "Loss: 3.370 | Acc: 16.016% (328/2048)\n",
            "Loss: 3.331 | Acc: 16.335% (345/2112)\n",
            "Loss: 3.296 | Acc: 16.636% (362/2176)\n",
            "Loss: 3.266 | Acc: 16.696% (374/2240)\n",
            "Loss: 3.228 | Acc: 16.884% (389/2304)\n",
            "Loss: 3.197 | Acc: 17.061% (404/2368)\n",
            "Loss: 3.166 | Acc: 17.229% (419/2432)\n",
            "Loss: 3.148 | Acc: 17.147% (428/2496)\n",
            "Loss: 3.132 | Acc: 17.188% (440/2560)\n",
            "Loss: 3.107 | Acc: 17.302% (454/2624)\n",
            "Loss: 3.086 | Acc: 17.299% (465/2688)\n",
            "Loss: 3.063 | Acc: 17.624% (485/2752)\n",
            "Loss: 3.044 | Acc: 17.791% (501/2816)\n",
            "Loss: 3.023 | Acc: 17.847% (514/2880)\n",
            "Loss: 2.999 | Acc: 18.105% (533/2944)\n",
            "Loss: 2.979 | Acc: 18.152% (546/3008)\n",
            "Loss: 2.961 | Acc: 18.229% (560/3072)\n",
            "Loss: 2.942 | Acc: 18.304% (574/3136)\n",
            "Loss: 2.929 | Acc: 18.281% (585/3200)\n",
            "Loss: 2.913 | Acc: 18.260% (596/3264)\n",
            "Loss: 2.898 | Acc: 18.480% (615/3328)\n",
            "Loss: 2.881 | Acc: 18.603% (631/3392)\n",
            "Loss: 2.867 | Acc: 18.750% (648/3456)\n",
            "Loss: 2.854 | Acc: 18.778% (661/3520)\n",
            "Loss: 2.840 | Acc: 18.834% (675/3584)\n",
            "Loss: 2.821 | Acc: 19.161% (699/3648)\n",
            "Loss: 2.807 | Acc: 19.343% (718/3712)\n",
            "Loss: 2.794 | Acc: 19.465% (735/3776)\n",
            "Loss: 2.786 | Acc: 19.453% (747/3840)\n",
            "Loss: 2.775 | Acc: 19.442% (759/3904)\n",
            "Loss: 2.760 | Acc: 19.657% (780/3968)\n",
            "Loss: 2.746 | Acc: 19.866% (801/4032)\n",
            "Loss: 2.735 | Acc: 19.995% (819/4096)\n",
            "Loss: 2.721 | Acc: 20.168% (839/4160)\n",
            "Loss: 2.708 | Acc: 20.312% (858/4224)\n",
            "Loss: 2.696 | Acc: 20.429% (876/4288)\n",
            "Loss: 2.685 | Acc: 20.519% (893/4352)\n",
            "Loss: 2.676 | Acc: 20.516% (906/4416)\n",
            "Loss: 2.668 | Acc: 20.759% (930/4480)\n",
            "Loss: 2.663 | Acc: 20.775% (944/4544)\n",
            "Loss: 2.653 | Acc: 20.833% (960/4608)\n",
            "Loss: 2.645 | Acc: 20.869% (975/4672)\n",
            "Loss: 2.639 | Acc: 20.967% (993/4736)\n",
            "Loss: 2.630 | Acc: 21.021% (1009/4800)\n",
            "Loss: 2.620 | Acc: 21.197% (1031/4864)\n",
            "Loss: 2.613 | Acc: 21.165% (1043/4928)\n",
            "Loss: 2.609 | Acc: 21.174% (1057/4992)\n",
            "Loss: 2.601 | Acc: 21.104% (1067/5056)\n",
            "Loss: 2.593 | Acc: 21.270% (1089/5120)\n",
            "Loss: 2.585 | Acc: 21.451% (1112/5184)\n",
            "Loss: 2.578 | Acc: 21.551% (1131/5248)\n",
            "Loss: 2.571 | Acc: 21.555% (1145/5312)\n",
            "Loss: 2.565 | Acc: 21.596% (1161/5376)\n",
            "Loss: 2.557 | Acc: 21.691% (1180/5440)\n",
            "Loss: 2.548 | Acc: 21.802% (1200/5504)\n",
            "Loss: 2.542 | Acc: 21.785% (1213/5568)\n",
            "Loss: 2.536 | Acc: 21.786% (1227/5632)\n",
            "Loss: 2.529 | Acc: 21.875% (1246/5696)\n",
            "Loss: 2.523 | Acc: 21.892% (1261/5760)\n",
            "Loss: 2.516 | Acc: 22.064% (1285/5824)\n",
            "Loss: 2.509 | Acc: 22.079% (1300/5888)\n",
            "Loss: 2.504 | Acc: 22.211% (1322/5952)\n",
            "Loss: 2.499 | Acc: 22.274% (1340/6016)\n",
            "Loss: 2.495 | Acc: 22.303% (1356/6080)\n",
            "Loss: 2.491 | Acc: 22.331% (1372/6144)\n",
            "Loss: 2.485 | Acc: 22.390% (1390/6208)\n",
            "Loss: 2.479 | Acc: 22.449% (1408/6272)\n",
            "Loss: 2.475 | Acc: 22.554% (1429/6336)\n",
            "Loss: 2.468 | Acc: 22.766% (1457/6400)\n",
            "Loss: 2.463 | Acc: 22.772% (1472/6464)\n",
            "Loss: 2.460 | Acc: 22.779% (1487/6528)\n",
            "Loss: 2.454 | Acc: 22.861% (1507/6592)\n",
            "Loss: 2.447 | Acc: 23.017% (1532/6656)\n",
            "Loss: 2.442 | Acc: 22.991% (1545/6720)\n",
            "Loss: 2.438 | Acc: 23.010% (1561/6784)\n",
            "Loss: 2.435 | Acc: 23.043% (1578/6848)\n",
            "Loss: 2.430 | Acc: 23.119% (1598/6912)\n",
            "Loss: 2.425 | Acc: 23.208% (1619/6976)\n",
            "Loss: 2.420 | Acc: 23.310% (1641/7040)\n",
            "Loss: 2.414 | Acc: 23.381% (1661/7104)\n",
            "Loss: 2.409 | Acc: 23.438% (1680/7168)\n",
            "Loss: 2.405 | Acc: 23.451% (1696/7232)\n",
            "Loss: 2.400 | Acc: 23.424% (1709/7296)\n",
            "Loss: 2.395 | Acc: 23.478% (1728/7360)\n",
            "Loss: 2.392 | Acc: 23.491% (1744/7424)\n",
            "Loss: 2.387 | Acc: 23.544% (1763/7488)\n",
            "Loss: 2.383 | Acc: 23.663% (1787/7552)\n",
            "Loss: 2.379 | Acc: 23.674% (1803/7616)\n",
            "Loss: 2.374 | Acc: 23.802% (1828/7680)\n",
            "Loss: 2.369 | Acc: 23.889% (1850/7744)\n",
            "Loss: 2.366 | Acc: 23.886% (1865/7808)\n",
            "Loss: 2.362 | Acc: 23.908% (1882/7872)\n",
            "Loss: 2.359 | Acc: 23.879% (1895/7936)\n",
            "Loss: 2.356 | Acc: 23.887% (1911/8000)\n",
            "Loss: 2.353 | Acc: 23.946% (1931/8064)\n",
            "Loss: 2.349 | Acc: 24.003% (1951/8128)\n",
            "Loss: 2.348 | Acc: 23.938% (1961/8192)\n",
            "Loss: 2.344 | Acc: 24.043% (1985/8256)\n",
            "Loss: 2.340 | Acc: 24.171% (2011/8320)\n",
            "Loss: 2.336 | Acc: 24.296% (2037/8384)\n",
            "Loss: 2.332 | Acc: 24.396% (2061/8448)\n",
            "Loss: 2.331 | Acc: 24.436% (2080/8512)\n",
            "Loss: 2.327 | Acc: 24.545% (2105/8576)\n",
            "Loss: 2.324 | Acc: 24.595% (2125/8640)\n",
            "Loss: 2.321 | Acc: 24.632% (2144/8704)\n",
            "Loss: 2.317 | Acc: 24.703% (2166/8768)\n",
            "Loss: 2.315 | Acc: 24.762% (2187/8832)\n",
            "Loss: 2.312 | Acc: 24.831% (2209/8896)\n",
            "Loss: 2.309 | Acc: 24.877% (2229/8960)\n",
            "Loss: 2.306 | Acc: 24.956% (2252/9024)\n",
            "Loss: 2.304 | Acc: 24.989% (2271/9088)\n",
            "Loss: 2.301 | Acc: 24.978% (2286/9152)\n",
            "Loss: 2.297 | Acc: 25.033% (2307/9216)\n",
            "Loss: 2.294 | Acc: 25.086% (2328/9280)\n",
            "Loss: 2.292 | Acc: 25.043% (2340/9344)\n",
            "Loss: 2.290 | Acc: 25.053% (2357/9408)\n",
            "Loss: 2.286 | Acc: 25.053% (2373/9472)\n",
            "Loss: 2.284 | Acc: 25.084% (2392/9536)\n",
            "Loss: 2.280 | Acc: 25.167% (2416/9600)\n",
            "Loss: 2.277 | Acc: 25.166% (2432/9664)\n",
            "Loss: 2.274 | Acc: 25.226% (2454/9728)\n",
            "Loss: 2.272 | Acc: 25.204% (2468/9792)\n",
            "Loss: 2.270 | Acc: 25.254% (2489/9856)\n",
            "Loss: 2.267 | Acc: 25.312% (2511/9920)\n",
            "Loss: 2.263 | Acc: 25.411% (2537/9984)\n",
            "Loss: 2.260 | Acc: 25.428% (2555/10048)\n",
            "Loss: 2.257 | Acc: 25.465% (2575/10112)\n",
            "Loss: 2.255 | Acc: 25.482% (2593/10176)\n",
            "Loss: 2.252 | Acc: 25.518% (2613/10240)\n",
            "Loss: 2.249 | Acc: 25.582% (2636/10304)\n",
            "Loss: 2.247 | Acc: 25.646% (2659/10368)\n",
            "Loss: 2.245 | Acc: 25.681% (2679/10432)\n",
            "Loss: 2.242 | Acc: 25.715% (2699/10496)\n",
            "Loss: 2.239 | Acc: 25.777% (2722/10560)\n",
            "Loss: 2.237 | Acc: 25.847% (2746/10624)\n",
            "Loss: 2.234 | Acc: 25.870% (2765/10688)\n",
            "Loss: 2.231 | Acc: 25.921% (2787/10752)\n",
            "Loss: 2.228 | Acc: 25.971% (2809/10816)\n",
            "Loss: 2.226 | Acc: 26.002% (2829/10880)\n",
            "Loss: 2.223 | Acc: 26.042% (2850/10944)\n",
            "Loss: 2.220 | Acc: 26.099% (2873/11008)\n",
            "Loss: 2.217 | Acc: 26.201% (2901/11072)\n",
            "Loss: 2.215 | Acc: 26.284% (2927/11136)\n",
            "Loss: 2.212 | Acc: 26.321% (2948/11200)\n",
            "Loss: 2.209 | Acc: 26.385% (2972/11264)\n",
            "Loss: 2.205 | Acc: 26.501% (3002/11328)\n",
            "Loss: 2.202 | Acc: 26.545% (3024/11392)\n",
            "Loss: 2.198 | Acc: 26.615% (3049/11456)\n",
            "Loss: 2.196 | Acc: 26.684% (3074/11520)\n",
            "Loss: 2.194 | Acc: 26.692% (3092/11584)\n",
            "Loss: 2.191 | Acc: 26.760% (3117/11648)\n",
            "Loss: 2.190 | Acc: 26.742% (3132/11712)\n",
            "Loss: 2.187 | Acc: 26.800% (3156/11776)\n",
            "Loss: 2.185 | Acc: 26.824% (3176/11840)\n",
            "Loss: 2.183 | Acc: 26.857% (3197/11904)\n",
            "Loss: 2.181 | Acc: 26.939% (3224/11968)\n",
            "Loss: 2.178 | Acc: 26.986% (3247/12032)\n",
            "Loss: 2.176 | Acc: 27.067% (3274/12096)\n",
            "Loss: 2.173 | Acc: 27.146% (3301/12160)\n",
            "Loss: 2.170 | Acc: 27.184% (3323/12224)\n",
            "Loss: 2.169 | Acc: 27.189% (3341/12288)\n",
            "Loss: 2.166 | Acc: 27.259% (3367/12352)\n",
            "Loss: 2.165 | Acc: 27.295% (3389/12416)\n",
            "Loss: 2.164 | Acc: 27.324% (3410/12480)\n",
            "Loss: 2.161 | Acc: 27.352% (3431/12544)\n",
            "Loss: 2.159 | Acc: 27.372% (3451/12608)\n",
            "Loss: 2.157 | Acc: 27.375% (3469/12672)\n",
            "Loss: 2.155 | Acc: 27.410% (3491/12736)\n",
            "Loss: 2.153 | Acc: 27.469% (3516/12800)\n",
            "Loss: 2.151 | Acc: 27.495% (3537/12864)\n",
            "Loss: 2.150 | Acc: 27.491% (3554/12928)\n",
            "Loss: 2.148 | Acc: 27.517% (3575/12992)\n",
            "Loss: 2.147 | Acc: 27.566% (3599/13056)\n",
            "Loss: 2.144 | Acc: 27.614% (3623/13120)\n",
            "Loss: 2.143 | Acc: 27.632% (3643/13184)\n",
            "Loss: 2.141 | Acc: 27.657% (3664/13248)\n",
            "Loss: 2.139 | Acc: 27.674% (3684/13312)\n",
            "Loss: 2.138 | Acc: 27.706% (3706/13376)\n",
            "Loss: 2.135 | Acc: 27.775% (3733/13440)\n",
            "Loss: 2.133 | Acc: 27.851% (3761/13504)\n",
            "Loss: 2.130 | Acc: 27.955% (3793/13568)\n",
            "Loss: 2.127 | Acc: 27.986% (3815/13632)\n",
            "Loss: 2.126 | Acc: 27.986% (3833/13696)\n",
            "Loss: 2.124 | Acc: 27.987% (3851/13760)\n",
            "Loss: 2.122 | Acc: 28.038% (3876/13824)\n",
            "Loss: 2.122 | Acc: 28.039% (3894/13888)\n",
            "Loss: 2.122 | Acc: 28.003% (3907/13952)\n",
            "Loss: 2.119 | Acc: 28.018% (3927/14016)\n",
            "Loss: 2.118 | Acc: 28.054% (3950/14080)\n",
            "Loss: 2.115 | Acc: 28.118% (3977/14144)\n",
            "Loss: 2.112 | Acc: 28.174% (4003/14208)\n",
            "Loss: 2.111 | Acc: 28.188% (4023/14272)\n",
            "Loss: 2.110 | Acc: 28.188% (4041/14336)\n",
            "Loss: 2.108 | Acc: 28.229% (4065/14400)\n",
            "Loss: 2.106 | Acc: 28.298% (4093/14464)\n",
            "Loss: 2.104 | Acc: 28.373% (4122/14528)\n",
            "Loss: 2.103 | Acc: 28.358% (4138/14592)\n",
            "Loss: 2.102 | Acc: 28.384% (4160/14656)\n",
            "Loss: 2.101 | Acc: 28.438% (4186/14720)\n",
            "Loss: 2.099 | Acc: 28.470% (4209/14784)\n",
            "Loss: 2.098 | Acc: 28.455% (4225/14848)\n",
            "Loss: 2.098 | Acc: 28.460% (4244/14912)\n",
            "Loss: 2.096 | Acc: 28.512% (4270/14976)\n",
            "Loss: 2.096 | Acc: 28.537% (4292/15040)\n",
            "Loss: 2.093 | Acc: 28.595% (4319/15104)\n",
            "Loss: 2.092 | Acc: 28.619% (4341/15168)\n",
            "Loss: 2.090 | Acc: 28.644% (4363/15232)\n",
            "Loss: 2.088 | Acc: 28.681% (4387/15296)\n",
            "Loss: 2.087 | Acc: 28.698% (4408/15360)\n",
            "Loss: 2.085 | Acc: 28.715% (4429/15424)\n",
            "Loss: 2.084 | Acc: 28.771% (4456/15488)\n",
            "Loss: 2.082 | Acc: 28.807% (4480/15552)\n",
            "Loss: 2.080 | Acc: 28.881% (4510/15616)\n",
            "Loss: 2.079 | Acc: 28.935% (4537/15680)\n",
            "Loss: 2.076 | Acc: 29.002% (4566/15744)\n",
            "Loss: 2.075 | Acc: 29.011% (4586/15808)\n",
            "Loss: 2.074 | Acc: 29.026% (4607/15872)\n",
            "Loss: 2.073 | Acc: 29.022% (4625/15936)\n",
            "Loss: 2.071 | Acc: 29.075% (4652/16000)\n",
            "Loss: 2.070 | Acc: 29.109% (4676/16064)\n",
            "Loss: 2.068 | Acc: 29.167% (4704/16128)\n",
            "Loss: 2.067 | Acc: 29.163% (4722/16192)\n",
            "Loss: 2.067 | Acc: 29.183% (4744/16256)\n",
            "Loss: 2.065 | Acc: 29.240% (4772/16320)\n",
            "Loss: 2.063 | Acc: 29.272% (4796/16384)\n",
            "Loss: 2.061 | Acc: 29.329% (4824/16448)\n",
            "Loss: 2.059 | Acc: 29.330% (4843/16512)\n",
            "Loss: 2.058 | Acc: 29.386% (4871/16576)\n",
            "Loss: 2.056 | Acc: 29.459% (4902/16640)\n",
            "Loss: 2.055 | Acc: 29.478% (4924/16704)\n",
            "Loss: 2.054 | Acc: 29.526% (4951/16768)\n",
            "Loss: 2.053 | Acc: 29.581% (4979/16832)\n",
            "Loss: 2.051 | Acc: 29.640% (5008/16896)\n",
            "Loss: 2.050 | Acc: 29.729% (5042/16960)\n",
            "Loss: 2.048 | Acc: 29.746% (5064/17024)\n",
            "Loss: 2.047 | Acc: 29.752% (5084/17088)\n",
            "Loss: 2.046 | Acc: 29.804% (5112/17152)\n",
            "Loss: 2.044 | Acc: 29.839% (5137/17216)\n",
            "Loss: 2.043 | Acc: 29.867% (5161/17280)\n",
            "Loss: 2.042 | Acc: 29.907% (5187/17344)\n",
            "Loss: 2.040 | Acc: 29.940% (5212/17408)\n",
            "Loss: 2.039 | Acc: 29.951% (5233/17472)\n",
            "Loss: 2.038 | Acc: 29.967% (5255/17536)\n",
            "Loss: 2.037 | Acc: 29.960% (5273/17600)\n",
            "Loss: 2.036 | Acc: 29.976% (5295/17664)\n",
            "Loss: 2.035 | Acc: 29.992% (5317/17728)\n",
            "Loss: 2.032 | Acc: 30.087% (5353/17792)\n",
            "Loss: 2.031 | Acc: 30.102% (5375/17856)\n",
            "Loss: 2.030 | Acc: 30.123% (5398/17920)\n",
            "Loss: 2.028 | Acc: 30.149% (5422/17984)\n",
            "Loss: 2.028 | Acc: 30.120% (5436/18048)\n",
            "Loss: 2.027 | Acc: 30.146% (5460/18112)\n",
            "Loss: 2.026 | Acc: 30.172% (5484/18176)\n",
            "Loss: 2.024 | Acc: 30.219% (5512/18240)\n",
            "Loss: 2.023 | Acc: 30.239% (5535/18304)\n",
            "Loss: 2.021 | Acc: 30.275% (5561/18368)\n",
            "Loss: 2.020 | Acc: 30.290% (5583/18432)\n",
            "Loss: 2.019 | Acc: 30.342% (5612/18496)\n",
            "Loss: 2.018 | Acc: 30.361% (5635/18560)\n",
            "Loss: 2.017 | Acc: 30.343% (5651/18624)\n",
            "Loss: 2.016 | Acc: 30.362% (5674/18688)\n",
            "Loss: 2.015 | Acc: 30.391% (5699/18752)\n",
            "Loss: 2.013 | Acc: 30.405% (5721/18816)\n",
            "Loss: 2.012 | Acc: 30.424% (5744/18880)\n",
            "Loss: 2.011 | Acc: 30.474% (5773/18944)\n",
            "Loss: 2.010 | Acc: 30.529% (5803/19008)\n",
            "Loss: 2.008 | Acc: 30.537% (5824/19072)\n",
            "Loss: 2.007 | Acc: 30.576% (5851/19136)\n",
            "Loss: 2.006 | Acc: 30.620% (5879/19200)\n",
            "Loss: 2.005 | Acc: 30.617% (5898/19264)\n",
            "Loss: 2.004 | Acc: 30.639% (5922/19328)\n",
            "Loss: 2.003 | Acc: 30.688% (5951/19392)\n",
            "Loss: 2.001 | Acc: 30.746% (5982/19456)\n",
            "Loss: 2.000 | Acc: 30.774% (6007/19520)\n",
            "Loss: 2.000 | Acc: 30.790% (6030/19584)\n",
            "Loss: 1.999 | Acc: 30.807% (6053/19648)\n",
            "Loss: 1.997 | Acc: 30.834% (6078/19712)\n",
            "Loss: 1.996 | Acc: 30.871% (6105/19776)\n",
            "Loss: 1.995 | Acc: 30.882% (6127/19840)\n",
            "Loss: 1.994 | Acc: 30.908% (6152/19904)\n",
            "Loss: 1.993 | Acc: 30.924% (6175/19968)\n",
            "Loss: 1.993 | Acc: 30.960% (6202/20032)\n",
            "Loss: 1.992 | Acc: 30.976% (6225/20096)\n",
            "Loss: 1.990 | Acc: 30.987% (6247/20160)\n",
            "Loss: 1.989 | Acc: 31.018% (6273/20224)\n",
            "Loss: 1.988 | Acc: 31.033% (6296/20288)\n",
            "Loss: 1.987 | Acc: 31.088% (6327/20352)\n",
            "Loss: 1.985 | Acc: 31.118% (6353/20416)\n",
            "Loss: 1.984 | Acc: 31.147% (6379/20480)\n",
            "Loss: 1.983 | Acc: 31.182% (6406/20544)\n",
            "Loss: 1.982 | Acc: 31.211% (6432/20608)\n",
            "Loss: 1.981 | Acc: 31.240% (6458/20672)\n",
            "Loss: 1.980 | Acc: 31.255% (6481/20736)\n",
            "Loss: 1.979 | Acc: 31.293% (6509/20800)\n",
            "Loss: 1.978 | Acc: 31.308% (6532/20864)\n",
            "Loss: 1.977 | Acc: 31.360% (6563/20928)\n",
            "Loss: 1.976 | Acc: 31.360% (6583/20992)\n",
            "Loss: 1.975 | Acc: 31.373% (6606/21056)\n",
            "Loss: 1.974 | Acc: 31.392% (6630/21120)\n",
            "Loss: 1.973 | Acc: 31.406% (6653/21184)\n",
            "Loss: 1.971 | Acc: 31.490% (6691/21248)\n",
            "Loss: 1.971 | Acc: 31.480% (6709/21312)\n",
            "Loss: 1.971 | Acc: 31.465% (6726/21376)\n",
            "Loss: 1.971 | Acc: 31.460% (6745/21440)\n",
            "Loss: 1.969 | Acc: 31.510% (6776/21504)\n",
            "Loss: 1.968 | Acc: 31.565% (6808/21568)\n",
            "Loss: 1.967 | Acc: 31.583% (6832/21632)\n",
            "Loss: 1.966 | Acc: 31.591% (6854/21696)\n",
            "Loss: 1.965 | Acc: 31.636% (6884/21760)\n",
            "Loss: 1.964 | Acc: 31.694% (6917/21824)\n",
            "Loss: 1.962 | Acc: 31.762% (6952/21888)\n",
            "Loss: 1.961 | Acc: 31.788% (6978/21952)\n",
            "Loss: 1.960 | Acc: 31.831% (7008/22016)\n",
            "Loss: 1.958 | Acc: 31.898% (7043/22080)\n",
            "Loss: 1.957 | Acc: 31.923% (7069/22144)\n",
            "Loss: 1.956 | Acc: 31.943% (7094/22208)\n",
            "Loss: 1.955 | Acc: 32.004% (7128/22272)\n",
            "Loss: 1.953 | Acc: 32.056% (7160/22336)\n",
            "Loss: 1.952 | Acc: 32.076% (7185/22400)\n",
            "Loss: 1.952 | Acc: 32.114% (7214/22464)\n",
            "Loss: 1.951 | Acc: 32.129% (7238/22528)\n",
            "Loss: 1.949 | Acc: 32.175% (7269/22592)\n",
            "Loss: 1.949 | Acc: 32.190% (7293/22656)\n",
            "Loss: 1.947 | Acc: 32.223% (7321/22720)\n",
            "Loss: 1.946 | Acc: 32.242% (7346/22784)\n",
            "Loss: 1.945 | Acc: 32.252% (7369/22848)\n",
            "Loss: 1.945 | Acc: 32.271% (7394/22912)\n",
            "Loss: 1.944 | Acc: 32.303% (7422/22976)\n",
            "Loss: 1.943 | Acc: 32.348% (7453/23040)\n",
            "Loss: 1.942 | Acc: 32.354% (7475/23104)\n",
            "Loss: 1.941 | Acc: 32.390% (7504/23168)\n",
            "Loss: 1.941 | Acc: 32.412% (7530/23232)\n",
            "Loss: 1.940 | Acc: 32.430% (7555/23296)\n",
            "Loss: 1.939 | Acc: 32.461% (7583/23360)\n",
            "Loss: 1.939 | Acc: 32.497% (7612/23424)\n",
            "Loss: 1.938 | Acc: 32.527% (7640/23488)\n",
            "Loss: 1.937 | Acc: 32.549% (7666/23552)\n",
            "Loss: 1.936 | Acc: 32.571% (7692/23616)\n",
            "Loss: 1.936 | Acc: 32.572% (7713/23680)\n",
            "Loss: 1.935 | Acc: 32.598% (7740/23744)\n",
            "Loss: 1.934 | Acc: 32.611% (7764/23808)\n",
            "Loss: 1.934 | Acc: 32.624% (7788/23872)\n",
            "Loss: 1.932 | Acc: 32.670% (7820/23936)\n",
            "Loss: 1.931 | Acc: 32.729% (7855/24000)\n",
            "Loss: 1.930 | Acc: 32.767% (7885/24064)\n",
            "Loss: 1.930 | Acc: 32.759% (7904/24128)\n",
            "Loss: 1.929 | Acc: 32.771% (7928/24192)\n",
            "Loss: 1.929 | Acc: 32.767% (7948/24256)\n",
            "Loss: 1.928 | Acc: 32.771% (7970/24320)\n",
            "Loss: 1.927 | Acc: 32.796% (7997/24384)\n",
            "Loss: 1.926 | Acc: 32.812% (8022/24448)\n",
            "Loss: 1.925 | Acc: 32.861% (8055/24512)\n",
            "Loss: 1.924 | Acc: 32.874% (8079/24576)\n",
            "Loss: 1.923 | Acc: 32.877% (8101/24640)\n",
            "Loss: 1.923 | Acc: 32.881% (8123/24704)\n",
            "Loss: 1.922 | Acc: 32.905% (8150/24768)\n",
            "Loss: 1.921 | Acc: 32.949% (8182/24832)\n",
            "Loss: 1.921 | Acc: 32.973% (8209/24896)\n",
            "Loss: 1.920 | Acc: 32.969% (8229/24960)\n",
            "Loss: 1.919 | Acc: 32.984% (8254/25024)\n",
            "Loss: 1.918 | Acc: 33.012% (8282/25088)\n",
            "Loss: 1.917 | Acc: 33.039% (8310/25152)\n",
            "Loss: 1.917 | Acc: 33.050% (8334/25216)\n",
            "Loss: 1.915 | Acc: 33.074% (8361/25280)\n",
            "Loss: 1.915 | Acc: 33.069% (8381/25344)\n",
            "Loss: 1.914 | Acc: 33.084% (8406/25408)\n",
            "Loss: 1.913 | Acc: 33.115% (8435/25472)\n",
            "Loss: 1.912 | Acc: 33.126% (8459/25536)\n",
            "Loss: 1.912 | Acc: 33.125% (8480/25600)\n",
            "Loss: 1.911 | Acc: 33.136% (8504/25664)\n",
            "Loss: 1.911 | Acc: 33.158% (8531/25728)\n",
            "Loss: 1.910 | Acc: 33.161% (8553/25792)\n",
            "Loss: 1.909 | Acc: 33.219% (8589/25856)\n",
            "Loss: 1.908 | Acc: 33.256% (8620/25920)\n",
            "Loss: 1.908 | Acc: 33.270% (8645/25984)\n",
            "Loss: 1.908 | Acc: 33.254% (8662/26048)\n",
            "Loss: 1.907 | Acc: 33.257% (8684/26112)\n",
            "Loss: 1.906 | Acc: 33.275% (8710/26176)\n",
            "Loss: 1.906 | Acc: 33.281% (8733/26240)\n",
            "Loss: 1.905 | Acc: 33.280% (8754/26304)\n",
            "Loss: 1.905 | Acc: 33.309% (8783/26368)\n",
            "Loss: 1.904 | Acc: 33.285% (8798/26432)\n",
            "Loss: 1.904 | Acc: 33.269% (8815/26496)\n",
            "Loss: 1.904 | Acc: 33.264% (8835/26560)\n",
            "Loss: 1.903 | Acc: 33.297% (8865/26624)\n",
            "Loss: 1.903 | Acc: 33.281% (8882/26688)\n",
            "Loss: 1.903 | Acc: 33.280% (8903/26752)\n",
            "Loss: 1.903 | Acc: 33.294% (8928/26816)\n",
            "Loss: 1.902 | Acc: 33.304% (8952/26880)\n",
            "Loss: 1.901 | Acc: 33.325% (8979/26944)\n",
            "Loss: 1.900 | Acc: 33.346% (9006/27008)\n",
            "Loss: 1.899 | Acc: 33.363% (9032/27072)\n",
            "Loss: 1.899 | Acc: 33.365% (9054/27136)\n",
            "Loss: 1.898 | Acc: 33.393% (9083/27200)\n",
            "Loss: 1.897 | Acc: 33.421% (9112/27264)\n",
            "Loss: 1.896 | Acc: 33.438% (9138/27328)\n",
            "Loss: 1.896 | Acc: 33.459% (9165/27392)\n",
            "Loss: 1.895 | Acc: 33.461% (9187/27456)\n",
            "Loss: 1.895 | Acc: 33.474% (9212/27520)\n",
            "Loss: 1.894 | Acc: 33.490% (9238/27584)\n",
            "Loss: 1.893 | Acc: 33.492% (9260/27648)\n",
            "Loss: 1.893 | Acc: 33.502% (9284/27712)\n",
            "Loss: 1.893 | Acc: 33.529% (9313/27776)\n",
            "Loss: 1.892 | Acc: 33.563% (9344/27840)\n",
            "Loss: 1.891 | Acc: 33.587% (9372/27904)\n",
            "Loss: 1.890 | Acc: 33.613% (9401/27968)\n",
            "Loss: 1.889 | Acc: 33.637% (9429/28032)\n",
            "Loss: 1.889 | Acc: 33.674% (9461/28096)\n",
            "Loss: 1.888 | Acc: 33.686% (9486/28160)\n",
            "Loss: 1.887 | Acc: 33.688% (9508/28224)\n",
            "Loss: 1.886 | Acc: 33.703% (9534/28288)\n",
            "Loss: 1.886 | Acc: 33.737% (9565/28352)\n",
            "Loss: 1.885 | Acc: 33.752% (9591/28416)\n",
            "Loss: 1.884 | Acc: 33.778% (9620/28480)\n",
            "Loss: 1.884 | Acc: 33.783% (9643/28544)\n",
            "Loss: 1.883 | Acc: 33.788% (9666/28608)\n",
            "Loss: 1.882 | Acc: 33.800% (9691/28672)\n",
            "Loss: 1.882 | Acc: 33.815% (9717/28736)\n",
            "Loss: 1.881 | Acc: 33.816% (9739/28800)\n",
            "Loss: 1.881 | Acc: 33.828% (9764/28864)\n",
            "Loss: 1.880 | Acc: 33.870% (9798/28928)\n",
            "Loss: 1.879 | Acc: 33.896% (9827/28992)\n",
            "Loss: 1.879 | Acc: 33.921% (9856/29056)\n",
            "Loss: 1.877 | Acc: 33.973% (9893/29120)\n",
            "Loss: 1.877 | Acc: 33.981% (9917/29184)\n",
            "Loss: 1.876 | Acc: 34.026% (9952/29248)\n",
            "Loss: 1.875 | Acc: 34.027% (9974/29312)\n",
            "Loss: 1.874 | Acc: 34.055% (10004/29376)\n",
            "Loss: 1.874 | Acc: 34.073% (10031/29440)\n",
            "Loss: 1.873 | Acc: 34.080% (10055/29504)\n",
            "Loss: 1.872 | Acc: 34.094% (10081/29568)\n",
            "Loss: 1.871 | Acc: 34.119% (10110/29632)\n",
            "Loss: 1.871 | Acc: 34.139% (10138/29696)\n",
            "Loss: 1.870 | Acc: 34.133% (10158/29760)\n",
            "Loss: 1.870 | Acc: 34.134% (10180/29824)\n",
            "Loss: 1.869 | Acc: 34.141% (10204/29888)\n",
            "Loss: 1.868 | Acc: 34.148% (10228/29952)\n",
            "Loss: 1.868 | Acc: 34.162% (10254/30016)\n",
            "Loss: 1.867 | Acc: 34.205% (10289/30080)\n",
            "Loss: 1.867 | Acc: 34.209% (10312/30144)\n",
            "Loss: 1.867 | Acc: 34.226% (10339/30208)\n",
            "Loss: 1.867 | Acc: 34.226% (10361/30272)\n",
            "Loss: 1.866 | Acc: 34.230% (10384/30336)\n",
            "Loss: 1.866 | Acc: 34.250% (10412/30400)\n",
            "Loss: 1.866 | Acc: 34.234% (10429/30464)\n",
            "Loss: 1.865 | Acc: 34.241% (10453/30528)\n",
            "Loss: 1.865 | Acc: 34.261% (10481/30592)\n",
            "Loss: 1.865 | Acc: 34.231% (10494/30656)\n",
            "Loss: 1.864 | Acc: 34.238% (10518/30720)\n",
            "Loss: 1.863 | Acc: 34.245% (10542/30784)\n",
            "Loss: 1.863 | Acc: 34.242% (10563/30848)\n",
            "Loss: 1.863 | Acc: 34.249% (10587/30912)\n",
            "Loss: 1.862 | Acc: 34.262% (10613/30976)\n",
            "Loss: 1.862 | Acc: 34.294% (10645/31040)\n",
            "Loss: 1.862 | Acc: 34.304% (10670/31104)\n",
            "Loss: 1.861 | Acc: 34.314% (10695/31168)\n",
            "Loss: 1.860 | Acc: 34.321% (10719/31232)\n",
            "Loss: 1.860 | Acc: 34.333% (10745/31296)\n",
            "Loss: 1.859 | Acc: 34.356% (10774/31360)\n",
            "Loss: 1.858 | Acc: 34.372% (10801/31424)\n",
            "Loss: 1.857 | Acc: 34.391% (10829/31488)\n",
            "Loss: 1.856 | Acc: 34.410% (10857/31552)\n",
            "Loss: 1.856 | Acc: 34.419% (10882/31616)\n",
            "Loss: 1.855 | Acc: 34.435% (10909/31680)\n",
            "Loss: 1.855 | Acc: 34.454% (10937/31744)\n",
            "Loss: 1.854 | Acc: 34.472% (10965/31808)\n",
            "Loss: 1.854 | Acc: 34.482% (10990/31872)\n",
            "Loss: 1.853 | Acc: 34.488% (11014/31936)\n",
            "Loss: 1.853 | Acc: 34.500% (11040/32000)\n",
            "Loss: 1.852 | Acc: 34.528% (11071/32064)\n",
            "Loss: 1.851 | Acc: 34.565% (11105/32128)\n",
            "Loss: 1.851 | Acc: 34.589% (11135/32192)\n",
            "Loss: 1.850 | Acc: 34.611% (11164/32256)\n",
            "Loss: 1.849 | Acc: 34.619% (11189/32320)\n",
            "Loss: 1.849 | Acc: 34.622% (11212/32384)\n",
            "Loss: 1.848 | Acc: 34.649% (11243/32448)\n",
            "Loss: 1.847 | Acc: 34.676% (11274/32512)\n",
            "Loss: 1.847 | Acc: 34.673% (11295/32576)\n",
            "Loss: 1.846 | Acc: 34.703% (11327/32640)\n",
            "Loss: 1.846 | Acc: 34.705% (11350/32704)\n",
            "Loss: 1.846 | Acc: 34.695% (11369/32768)\n",
            "Loss: 1.845 | Acc: 34.722% (11400/32832)\n",
            "Loss: 1.845 | Acc: 34.737% (11427/32896)\n",
            "Loss: 1.845 | Acc: 34.748% (11453/32960)\n",
            "Loss: 1.845 | Acc: 34.747% (11475/33024)\n",
            "Loss: 1.844 | Acc: 34.765% (11503/33088)\n",
            "Loss: 1.844 | Acc: 34.767% (11526/33152)\n",
            "Loss: 1.843 | Acc: 34.799% (11559/33216)\n",
            "Loss: 1.843 | Acc: 34.805% (11583/33280)\n",
            "Loss: 1.843 | Acc: 34.801% (11604/33344)\n",
            "Loss: 1.842 | Acc: 34.806% (11628/33408)\n",
            "Loss: 1.842 | Acc: 34.826% (11657/33472)\n",
            "Loss: 1.841 | Acc: 34.846% (11686/33536)\n",
            "Loss: 1.841 | Acc: 34.866% (11715/33600)\n",
            "Loss: 1.840 | Acc: 34.901% (11749/33664)\n",
            "Loss: 1.840 | Acc: 34.909% (11774/33728)\n",
            "Loss: 1.839 | Acc: 34.934% (11805/33792)\n",
            "Loss: 1.839 | Acc: 34.924% (11824/33856)\n",
            "Loss: 1.838 | Acc: 34.953% (11856/33920)\n",
            "Loss: 1.838 | Acc: 34.958% (11880/33984)\n",
            "Loss: 1.837 | Acc: 34.965% (11905/34048)\n",
            "Loss: 1.836 | Acc: 34.994% (11937/34112)\n",
            "Loss: 1.836 | Acc: 35.028% (11971/34176)\n",
            "Loss: 1.836 | Acc: 35.023% (11992/34240)\n",
            "Loss: 1.835 | Acc: 35.034% (12018/34304)\n",
            "Loss: 1.835 | Acc: 35.035% (12041/34368)\n",
            "Loss: 1.834 | Acc: 35.055% (12070/34432)\n",
            "Loss: 1.834 | Acc: 35.062% (12095/34496)\n",
            "Loss: 1.833 | Acc: 35.078% (12123/34560)\n",
            "Loss: 1.832 | Acc: 35.091% (12150/34624)\n",
            "Loss: 1.832 | Acc: 35.096% (12174/34688)\n",
            "Loss: 1.832 | Acc: 35.126% (12207/34752)\n",
            "Loss: 1.831 | Acc: 35.156% (12240/34816)\n",
            "Loss: 1.830 | Acc: 35.178% (12270/34880)\n",
            "Loss: 1.830 | Acc: 35.199% (12300/34944)\n",
            "Loss: 1.829 | Acc: 35.221% (12330/35008)\n",
            "Loss: 1.828 | Acc: 35.239% (12359/35072)\n",
            "Loss: 1.828 | Acc: 35.229% (12378/35136)\n",
            "Loss: 1.828 | Acc: 35.250% (12408/35200)\n",
            "Loss: 1.827 | Acc: 35.265% (12436/35264)\n",
            "Loss: 1.827 | Acc: 35.272% (12461/35328)\n",
            "Loss: 1.826 | Acc: 35.288% (12489/35392)\n",
            "Loss: 1.826 | Acc: 35.297% (12515/35456)\n",
            "Loss: 1.825 | Acc: 35.324% (12547/35520)\n",
            "Loss: 1.824 | Acc: 35.328% (12571/35584)\n",
            "Loss: 1.824 | Acc: 35.346% (12600/35648)\n",
            "Loss: 1.824 | Acc: 35.347% (12623/35712)\n",
            "Loss: 1.823 | Acc: 35.367% (12653/35776)\n",
            "Loss: 1.822 | Acc: 35.393% (12685/35840)\n",
            "Loss: 1.822 | Acc: 35.392% (12707/35904)\n",
            "Loss: 1.821 | Acc: 35.406% (12735/35968)\n",
            "Loss: 1.821 | Acc: 35.416% (12761/36032)\n",
            "Loss: 1.821 | Acc: 35.419% (12785/36096)\n",
            "Loss: 1.820 | Acc: 35.434% (12813/36160)\n",
            "Loss: 1.820 | Acc: 35.441% (12838/36224)\n",
            "Loss: 1.819 | Acc: 35.461% (12868/36288)\n",
            "Loss: 1.819 | Acc: 35.470% (12894/36352)\n",
            "Loss: 1.818 | Acc: 35.484% (12922/36416)\n",
            "Loss: 1.817 | Acc: 35.504% (12952/36480)\n",
            "Loss: 1.817 | Acc: 35.519% (12980/36544)\n",
            "Loss: 1.816 | Acc: 35.544% (13012/36608)\n",
            "Loss: 1.815 | Acc: 35.569% (13044/36672)\n",
            "Loss: 1.815 | Acc: 35.592% (13075/36736)\n",
            "Loss: 1.815 | Acc: 35.587% (13096/36800)\n",
            "Loss: 1.814 | Acc: 35.601% (13124/36864)\n",
            "Loss: 1.814 | Acc: 35.604% (13148/36928)\n",
            "Loss: 1.814 | Acc: 35.605% (13171/36992)\n",
            "Loss: 1.813 | Acc: 35.638% (13206/37056)\n",
            "Loss: 1.813 | Acc: 35.633% (13227/37120)\n",
            "Loss: 1.812 | Acc: 35.644% (13254/37184)\n",
            "Loss: 1.812 | Acc: 35.672% (13287/37248)\n",
            "Loss: 1.811 | Acc: 35.699% (13320/37312)\n",
            "Loss: 1.811 | Acc: 35.726% (13353/37376)\n",
            "Loss: 1.810 | Acc: 35.732% (13378/37440)\n",
            "Loss: 1.810 | Acc: 35.756% (13410/37504)\n",
            "Loss: 1.809 | Acc: 35.764% (13436/37568)\n",
            "Loss: 1.809 | Acc: 35.783% (13466/37632)\n",
            "Loss: 1.808 | Acc: 35.789% (13491/37696)\n",
            "Loss: 1.808 | Acc: 35.794% (13516/37760)\n",
            "Loss: 1.807 | Acc: 35.829% (13552/37824)\n",
            "Loss: 1.807 | Acc: 35.829% (13575/37888)\n",
            "Loss: 1.807 | Acc: 35.837% (13601/37952)\n",
            "Loss: 1.806 | Acc: 35.864% (13634/38016)\n",
            "Loss: 1.805 | Acc: 35.874% (13661/38080)\n",
            "Loss: 1.805 | Acc: 35.896% (13692/38144)\n",
            "Loss: 1.804 | Acc: 35.919% (13724/38208)\n",
            "Loss: 1.804 | Acc: 35.932% (13752/38272)\n",
            "Loss: 1.803 | Acc: 35.958% (13785/38336)\n",
            "Loss: 1.802 | Acc: 35.971% (13813/38400)\n",
            "Loss: 1.802 | Acc: 35.979% (13839/38464)\n",
            "Loss: 1.802 | Acc: 35.992% (13867/38528)\n",
            "Loss: 1.801 | Acc: 36.020% (13901/38592)\n",
            "Loss: 1.801 | Acc: 36.036% (13930/38656)\n",
            "Loss: 1.800 | Acc: 36.046% (13957/38720)\n",
            "Loss: 1.800 | Acc: 36.054% (13983/38784)\n",
            "Loss: 1.799 | Acc: 36.077% (14015/38848)\n",
            "Loss: 1.799 | Acc: 36.069% (14035/38912)\n",
            "Loss: 1.798 | Acc: 36.094% (14068/38976)\n",
            "Loss: 1.798 | Acc: 36.109% (14097/39040)\n",
            "Loss: 1.798 | Acc: 36.129% (14128/39104)\n",
            "Loss: 1.797 | Acc: 36.152% (14160/39168)\n",
            "Loss: 1.797 | Acc: 36.167% (14189/39232)\n",
            "Loss: 1.797 | Acc: 36.172% (14214/39296)\n",
            "Loss: 1.796 | Acc: 36.181% (14241/39360)\n",
            "Loss: 1.796 | Acc: 36.181% (14264/39424)\n",
            "Loss: 1.796 | Acc: 36.181% (14287/39488)\n",
            "Loss: 1.796 | Acc: 36.185% (14312/39552)\n",
            "Loss: 1.795 | Acc: 36.205% (14343/39616)\n",
            "Loss: 1.795 | Acc: 36.202% (14365/39680)\n",
            "Loss: 1.795 | Acc: 36.202% (14388/39744)\n",
            "Loss: 1.795 | Acc: 36.206% (14413/39808)\n",
            "Loss: 1.795 | Acc: 36.211% (14438/39872)\n",
            "Loss: 1.794 | Acc: 36.243% (14474/39936)\n",
            "Loss: 1.794 | Acc: 36.265% (14506/40000)\n",
            "Loss: 1.793 | Acc: 36.277% (14534/40064)\n",
            "Loss: 1.793 | Acc: 36.274% (14556/40128)\n",
            "Loss: 1.793 | Acc: 36.271% (14578/40192)\n",
            "Loss: 1.793 | Acc: 36.288% (14608/40256)\n",
            "Loss: 1.792 | Acc: 36.292% (14633/40320)\n",
            "Loss: 1.792 | Acc: 36.321% (14668/40384)\n",
            "Loss: 1.791 | Acc: 36.331% (14695/40448)\n",
            "Loss: 1.791 | Acc: 36.337% (14721/40512)\n",
            "Loss: 1.791 | Acc: 36.354% (14751/40576)\n",
            "Loss: 1.790 | Acc: 36.363% (14778/40640)\n",
            "Loss: 1.790 | Acc: 36.380% (14808/40704)\n",
            "Loss: 1.789 | Acc: 36.399% (14839/40768)\n",
            "Loss: 1.789 | Acc: 36.425% (14873/40832)\n",
            "Loss: 1.788 | Acc: 36.441% (14903/40896)\n",
            "Loss: 1.788 | Acc: 36.443% (14927/40960)\n",
            "Loss: 1.788 | Acc: 36.452% (14954/41024)\n",
            "Loss: 1.787 | Acc: 36.461% (14981/41088)\n",
            "Loss: 1.787 | Acc: 36.470% (15008/41152)\n",
            "Loss: 1.787 | Acc: 36.466% (15030/41216)\n",
            "Loss: 1.787 | Acc: 36.478% (15058/41280)\n",
            "Loss: 1.787 | Acc: 36.487% (15085/41344)\n",
            "Loss: 1.786 | Acc: 36.500% (15114/41408)\n",
            "Loss: 1.786 | Acc: 36.519% (15145/41472)\n",
            "Loss: 1.785 | Acc: 36.527% (15172/41536)\n",
            "Loss: 1.785 | Acc: 36.536% (15199/41600)\n",
            "Loss: 1.785 | Acc: 36.545% (15226/41664)\n",
            "Loss: 1.784 | Acc: 36.549% (15251/41728)\n",
            "Loss: 1.784 | Acc: 36.574% (15285/41792)\n",
            "Loss: 1.783 | Acc: 36.583% (15312/41856)\n",
            "Loss: 1.782 | Acc: 36.594% (15340/41920)\n",
            "Loss: 1.782 | Acc: 36.616% (15373/41984)\n",
            "Loss: 1.781 | Acc: 36.632% (15403/42048)\n",
            "Loss: 1.782 | Acc: 36.626% (15424/42112)\n",
            "Loss: 1.781 | Acc: 36.632% (15450/42176)\n",
            "Loss: 1.781 | Acc: 36.643% (15478/42240)\n",
            "Loss: 1.780 | Acc: 36.663% (15510/42304)\n",
            "Loss: 1.780 | Acc: 36.655% (15530/42368)\n",
            "Loss: 1.780 | Acc: 36.659% (15555/42432)\n",
            "Loss: 1.779 | Acc: 36.686% (15590/42496)\n",
            "Loss: 1.779 | Acc: 36.694% (15617/42560)\n",
            "Loss: 1.778 | Acc: 36.714% (15649/42624)\n",
            "Loss: 1.778 | Acc: 36.720% (15675/42688)\n",
            "Loss: 1.778 | Acc: 36.735% (15705/42752)\n",
            "Loss: 1.777 | Acc: 36.748% (15734/42816)\n",
            "Loss: 1.777 | Acc: 36.751% (15759/42880)\n",
            "Loss: 1.776 | Acc: 36.755% (15784/42944)\n",
            "Loss: 1.776 | Acc: 36.761% (15810/43008)\n",
            "Loss: 1.776 | Acc: 36.762% (15834/43072)\n",
            "Loss: 1.776 | Acc: 36.760% (15857/43136)\n",
            "Loss: 1.775 | Acc: 36.780% (15889/43200)\n",
            "Loss: 1.774 | Acc: 36.804% (15923/43264)\n",
            "Loss: 1.774 | Acc: 36.819% (15953/43328)\n",
            "Loss: 1.773 | Acc: 36.839% (15985/43392)\n",
            "Loss: 1.773 | Acc: 36.842% (16010/43456)\n",
            "Loss: 1.773 | Acc: 36.857% (16040/43520)\n",
            "Loss: 1.772 | Acc: 36.878% (16073/43584)\n",
            "Loss: 1.772 | Acc: 36.881% (16098/43648)\n",
            "Loss: 1.772 | Acc: 36.887% (16124/43712)\n",
            "Loss: 1.771 | Acc: 36.908% (16157/43776)\n",
            "Loss: 1.771 | Acc: 36.921% (16186/43840)\n",
            "Loss: 1.770 | Acc: 36.933% (16215/43904)\n",
            "Loss: 1.770 | Acc: 36.959% (16250/43968)\n",
            "Loss: 1.769 | Acc: 36.957% (16273/44032)\n",
            "Loss: 1.769 | Acc: 36.963% (16299/44096)\n",
            "Loss: 1.769 | Acc: 36.979% (16330/44160)\n",
            "Loss: 1.768 | Acc: 37.003% (16364/44224)\n",
            "Loss: 1.768 | Acc: 37.003% (16388/44288)\n",
            "Loss: 1.767 | Acc: 37.015% (16417/44352)\n",
            "Loss: 1.767 | Acc: 37.027% (16446/44416)\n",
            "Loss: 1.767 | Acc: 37.055% (16482/44480)\n",
            "Loss: 1.766 | Acc: 37.067% (16511/44544)\n",
            "Loss: 1.766 | Acc: 37.070% (16536/44608)\n",
            "Loss: 1.766 | Acc: 37.084% (16566/44672)\n",
            "Loss: 1.765 | Acc: 37.095% (16595/44736)\n",
            "Loss: 1.765 | Acc: 37.116% (16628/44800)\n",
            "Loss: 1.765 | Acc: 37.112% (16650/44864)\n",
            "Loss: 1.765 | Acc: 37.119% (16677/44928)\n",
            "Loss: 1.764 | Acc: 37.129% (16705/44992)\n",
            "Loss: 1.764 | Acc: 37.140% (16734/45056)\n",
            "Loss: 1.764 | Acc: 37.130% (16753/45120)\n",
            "Loss: 1.764 | Acc: 37.144% (16783/45184)\n",
            "Loss: 1.763 | Acc: 37.160% (16814/45248)\n",
            "Loss: 1.763 | Acc: 37.176% (16845/45312)\n",
            "Loss: 1.762 | Acc: 37.183% (16872/45376)\n",
            "Loss: 1.762 | Acc: 37.181% (16895/45440)\n",
            "Loss: 1.762 | Acc: 37.197% (16926/45504)\n",
            "Loss: 1.761 | Acc: 37.210% (16956/45568)\n",
            "Loss: 1.761 | Acc: 37.206% (16978/45632)\n",
            "Loss: 1.761 | Acc: 37.194% (16996/45696)\n",
            "Loss: 1.760 | Acc: 37.218% (17031/45760)\n",
            "Loss: 1.760 | Acc: 37.236% (17063/45824)\n",
            "Loss: 1.760 | Acc: 37.234% (17086/45888)\n",
            "Loss: 1.759 | Acc: 37.248% (17116/45952)\n",
            "Loss: 1.759 | Acc: 37.254% (17143/46016)\n",
            "Loss: 1.759 | Acc: 37.263% (17171/46080)\n",
            "Loss: 1.759 | Acc: 37.262% (17194/46144)\n",
            "Loss: 1.758 | Acc: 37.281% (17227/46208)\n",
            "Loss: 1.758 | Acc: 37.301% (17260/46272)\n",
            "Loss: 1.757 | Acc: 37.323% (17294/46336)\n",
            "Loss: 1.757 | Acc: 37.334% (17323/46400)\n",
            "Loss: 1.756 | Acc: 37.334% (17347/46464)\n",
            "Loss: 1.756 | Acc: 37.354% (17380/46528)\n",
            "Loss: 1.756 | Acc: 37.363% (17408/46592)\n",
            "Loss: 1.755 | Acc: 37.363% (17432/46656)\n",
            "Loss: 1.755 | Acc: 37.378% (17463/46720)\n",
            "Loss: 1.755 | Acc: 37.374% (17485/46784)\n",
            "Loss: 1.755 | Acc: 37.368% (17506/46848)\n",
            "Loss: 1.755 | Acc: 37.366% (17529/46912)\n",
            "Loss: 1.755 | Acc: 37.370% (17555/46976)\n",
            "Loss: 1.755 | Acc: 37.372% (17580/47040)\n",
            "Loss: 1.755 | Acc: 37.379% (17607/47104)\n",
            "Loss: 1.754 | Acc: 37.400% (17641/47168)\n",
            "Loss: 1.754 | Acc: 37.403% (17666/47232)\n",
            "Loss: 1.754 | Acc: 37.407% (17692/47296)\n",
            "Loss: 1.754 | Acc: 37.399% (17712/47360)\n",
            "Loss: 1.753 | Acc: 37.435% (17753/47424)\n",
            "Loss: 1.752 | Acc: 37.452% (17785/47488)\n",
            "Loss: 1.752 | Acc: 37.473% (17819/47552)\n",
            "Loss: 1.752 | Acc: 37.473% (17843/47616)\n",
            "Loss: 1.752 | Acc: 37.479% (17870/47680)\n",
            "Loss: 1.752 | Acc: 37.477% (17893/47744)\n",
            "Loss: 1.752 | Acc: 37.479% (17918/47808)\n",
            "Loss: 1.752 | Acc: 37.479% (17942/47872)\n",
            "Loss: 1.752 | Acc: 37.481% (17967/47936)\n",
            "Loss: 1.752 | Acc: 37.473% (17987/48000)\n",
            "Loss: 1.751 | Acc: 37.488% (18018/48064)\n",
            "Loss: 1.751 | Acc: 37.502% (18049/48128)\n",
            "Loss: 1.751 | Acc: 37.498% (18071/48192)\n",
            "Loss: 1.751 | Acc: 37.502% (18097/48256)\n",
            "Loss: 1.751 | Acc: 37.508% (18124/48320)\n",
            "Loss: 1.751 | Acc: 37.519% (18153/48384)\n",
            "Loss: 1.751 | Acc: 37.527% (18181/48448)\n",
            "Loss: 1.750 | Acc: 37.541% (18212/48512)\n",
            "Loss: 1.750 | Acc: 37.554% (18242/48576)\n",
            "Loss: 1.750 | Acc: 37.556% (18267/48640)\n",
            "Loss: 1.749 | Acc: 37.574% (18300/48704)\n",
            "Loss: 1.749 | Acc: 37.578% (18326/48768)\n",
            "Loss: 1.749 | Acc: 37.580% (18351/48832)\n",
            "Loss: 1.748 | Acc: 37.592% (18381/48896)\n",
            "Loss: 1.748 | Acc: 37.592% (18405/48960)\n",
            "Loss: 1.748 | Acc: 37.582% (18415/49000)\n",
            "Epoch 0 of training is completed, Training accuracy for this epoch is 37.58163265306123\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.514 | Acc: 45.312% (29/64)\n",
            "Loss: 1.455 | Acc: 46.094% (59/128)\n",
            "Loss: 1.451 | Acc: 46.875% (90/192)\n",
            "Loss: 1.535 | Acc: 44.141% (113/256)\n",
            "Loss: 1.527 | Acc: 43.125% (138/320)\n",
            "Loss: 1.542 | Acc: 43.490% (167/384)\n",
            "Loss: 1.563 | Acc: 41.964% (188/448)\n",
            "Loss: 1.549 | Acc: 42.383% (217/512)\n",
            "Loss: 1.515 | Acc: 43.924% (253/576)\n",
            "Loss: 1.510 | Acc: 44.531% (285/640)\n",
            "Loss: 1.523 | Acc: 44.034% (310/704)\n",
            "Loss: 1.518 | Acc: 44.531% (342/768)\n",
            "Loss: 1.529 | Acc: 44.111% (367/832)\n",
            "Loss: 1.516 | Acc: 44.420% (398/896)\n",
            "Loss: 1.508 | Acc: 44.792% (430/960)\n",
            "Loss: 1.501 | Acc: 45.410% (465/1024)\n",
            "Loss: 1.508 | Acc: 45.129% (491/1088)\n",
            "Loss: 1.507 | Acc: 45.052% (519/1152)\n",
            "Loss: 1.507 | Acc: 45.066% (548/1216)\n",
            "Loss: 1.519 | Acc: 44.453% (569/1280)\n",
            "Loss: 1.514 | Acc: 44.271% (595/1344)\n",
            "Loss: 1.516 | Acc: 43.892% (618/1408)\n",
            "Loss: 1.514 | Acc: 44.022% (648/1472)\n",
            "Loss: 1.514 | Acc: 44.141% (678/1536)\n",
            "Loss: 1.515 | Acc: 44.188% (707/1600)\n",
            "Loss: 1.516 | Acc: 44.591% (742/1664)\n",
            "Loss: 1.515 | Acc: 44.792% (774/1728)\n",
            "Loss: 1.512 | Acc: 44.866% (804/1792)\n",
            "Loss: 1.512 | Acc: 44.935% (834/1856)\n",
            "Loss: 1.512 | Acc: 45.104% (866/1920)\n",
            "Loss: 1.509 | Acc: 45.312% (899/1984)\n",
            "Loss: 1.512 | Acc: 45.068% (923/2048)\n",
            "Loss: 1.505 | Acc: 45.076% (952/2112)\n",
            "Loss: 1.503 | Acc: 45.083% (981/2176)\n",
            "Loss: 1.502 | Acc: 45.179% (1012/2240)\n",
            "Loss: 1.501 | Acc: 45.399% (1046/2304)\n",
            "Loss: 1.503 | Acc: 45.439% (1076/2368)\n",
            "Loss: 1.499 | Acc: 45.683% (1111/2432)\n",
            "Loss: 1.495 | Acc: 45.793% (1143/2496)\n",
            "Loss: 1.500 | Acc: 45.469% (1164/2560)\n",
            "Loss: 1.502 | Acc: 45.198% (1186/2624)\n",
            "Loss: 1.503 | Acc: 45.312% (1218/2688)\n",
            "Loss: 1.501 | Acc: 45.349% (1248/2752)\n",
            "Loss: 1.502 | Acc: 45.419% (1279/2816)\n",
            "Loss: 1.502 | Acc: 45.382% (1307/2880)\n",
            "Loss: 1.500 | Acc: 45.279% (1333/2944)\n",
            "Loss: 1.499 | Acc: 45.246% (1361/3008)\n",
            "Loss: 1.500 | Acc: 45.215% (1389/3072)\n",
            "Loss: 1.500 | Acc: 45.089% (1414/3136)\n",
            "Loss: 1.497 | Acc: 45.281% (1449/3200)\n",
            "Loss: 1.498 | Acc: 45.221% (1476/3264)\n",
            "Loss: 1.498 | Acc: 45.252% (1506/3328)\n",
            "Loss: 1.501 | Acc: 45.136% (1531/3392)\n",
            "Loss: 1.504 | Acc: 44.907% (1552/3456)\n",
            "Loss: 1.504 | Acc: 44.886% (1580/3520)\n",
            "Loss: 1.502 | Acc: 45.117% (1617/3584)\n",
            "Loss: 1.503 | Acc: 45.066% (1644/3648)\n",
            "Loss: 1.499 | Acc: 45.178% (1677/3712)\n",
            "Loss: 1.499 | Acc: 45.207% (1707/3776)\n",
            "Loss: 1.498 | Acc: 45.286% (1739/3840)\n",
            "Loss: 1.500 | Acc: 45.184% (1764/3904)\n",
            "Loss: 1.500 | Acc: 45.186% (1793/3968)\n",
            "Loss: 1.502 | Acc: 45.139% (1820/4032)\n",
            "Loss: 1.503 | Acc: 45.093% (1847/4096)\n",
            "Loss: 1.504 | Acc: 44.952% (1870/4160)\n",
            "Loss: 1.504 | Acc: 44.957% (1899/4224)\n",
            "Loss: 1.502 | Acc: 45.009% (1930/4288)\n",
            "Loss: 1.501 | Acc: 45.014% (1959/4352)\n",
            "Loss: 1.499 | Acc: 45.199% (1996/4416)\n",
            "Loss: 1.497 | Acc: 45.246% (2027/4480)\n",
            "Loss: 1.496 | Acc: 45.268% (2057/4544)\n",
            "Loss: 1.498 | Acc: 45.161% (2081/4608)\n",
            "Loss: 1.498 | Acc: 45.141% (2109/4672)\n",
            "Loss: 1.497 | Acc: 45.165% (2139/4736)\n",
            "Loss: 1.498 | Acc: 45.167% (2168/4800)\n",
            "Loss: 1.498 | Acc: 45.127% (2195/4864)\n",
            "Loss: 1.498 | Acc: 45.211% (2228/4928)\n",
            "Loss: 1.500 | Acc: 45.052% (2249/4992)\n",
            "Loss: 1.498 | Acc: 45.134% (2282/5056)\n",
            "Loss: 1.501 | Acc: 44.980% (2303/5120)\n",
            "Loss: 1.502 | Acc: 44.985% (2332/5184)\n",
            "Loss: 1.502 | Acc: 45.008% (2362/5248)\n",
            "Loss: 1.500 | Acc: 45.087% (2395/5312)\n",
            "Loss: 1.502 | Acc: 45.071% (2423/5376)\n",
            "Loss: 1.502 | Acc: 45.110% (2454/5440)\n",
            "Loss: 1.501 | Acc: 45.113% (2483/5504)\n",
            "Loss: 1.502 | Acc: 45.097% (2511/5568)\n",
            "Loss: 1.505 | Acc: 45.064% (2538/5632)\n",
            "Loss: 1.504 | Acc: 45.102% (2569/5696)\n",
            "Loss: 1.503 | Acc: 45.156% (2601/5760)\n",
            "Loss: 1.502 | Acc: 45.175% (2631/5824)\n",
            "Loss: 1.502 | Acc: 45.143% (2658/5888)\n",
            "Loss: 1.504 | Acc: 44.976% (2677/5952)\n",
            "Loss: 1.504 | Acc: 44.947% (2704/6016)\n",
            "Loss: 1.505 | Acc: 44.852% (2727/6080)\n",
            "Loss: 1.506 | Acc: 44.857% (2756/6144)\n",
            "Loss: 1.509 | Acc: 44.716% (2776/6208)\n",
            "Loss: 1.510 | Acc: 44.627% (2799/6272)\n",
            "Loss: 1.510 | Acc: 44.697% (2832/6336)\n",
            "Loss: 1.511 | Acc: 44.672% (2859/6400)\n",
            "Loss: 1.511 | Acc: 44.616% (2884/6464)\n",
            "Loss: 1.510 | Acc: 44.730% (2920/6528)\n",
            "Loss: 1.512 | Acc: 44.691% (2946/6592)\n",
            "Loss: 1.511 | Acc: 44.772% (2980/6656)\n",
            "Loss: 1.512 | Acc: 44.717% (3005/6720)\n",
            "Loss: 1.511 | Acc: 44.782% (3038/6784)\n",
            "Loss: 1.509 | Acc: 44.904% (3075/6848)\n",
            "Loss: 1.510 | Acc: 44.850% (3100/6912)\n",
            "Loss: 1.510 | Acc: 44.796% (3125/6976)\n",
            "Loss: 1.512 | Acc: 44.801% (3154/7040)\n",
            "Loss: 1.511 | Acc: 44.764% (3180/7104)\n",
            "Loss: 1.513 | Acc: 44.713% (3205/7168)\n",
            "Loss: 1.514 | Acc: 44.649% (3229/7232)\n",
            "Loss: 1.512 | Acc: 44.668% (3259/7296)\n",
            "Loss: 1.511 | Acc: 44.688% (3289/7360)\n",
            "Loss: 1.511 | Acc: 44.666% (3316/7424)\n",
            "Loss: 1.509 | Acc: 44.792% (3354/7488)\n",
            "Loss: 1.509 | Acc: 44.889% (3390/7552)\n",
            "Loss: 1.510 | Acc: 44.840% (3415/7616)\n",
            "Loss: 1.510 | Acc: 44.857% (3445/7680)\n",
            "Loss: 1.508 | Acc: 44.977% (3483/7744)\n",
            "Loss: 1.508 | Acc: 45.005% (3514/7808)\n",
            "Loss: 1.507 | Acc: 45.046% (3546/7872)\n",
            "Loss: 1.507 | Acc: 45.048% (3575/7936)\n",
            "Loss: 1.509 | Acc: 44.987% (3599/8000)\n",
            "Loss: 1.509 | Acc: 45.027% (3631/8064)\n",
            "Loss: 1.509 | Acc: 45.017% (3659/8128)\n",
            "Loss: 1.510 | Acc: 44.995% (3686/8192)\n",
            "Loss: 1.510 | Acc: 44.949% (3711/8256)\n",
            "Loss: 1.513 | Acc: 44.772% (3725/8320)\n",
            "Loss: 1.513 | Acc: 44.812% (3757/8384)\n",
            "Loss: 1.512 | Acc: 44.744% (3780/8448)\n",
            "Loss: 1.514 | Acc: 44.702% (3805/8512)\n",
            "Loss: 1.516 | Acc: 44.648% (3829/8576)\n",
            "Loss: 1.517 | Acc: 44.572% (3851/8640)\n",
            "Loss: 1.518 | Acc: 44.577% (3880/8704)\n",
            "Loss: 1.518 | Acc: 44.560% (3907/8768)\n",
            "Loss: 1.517 | Acc: 44.667% (3945/8832)\n",
            "Loss: 1.516 | Acc: 44.649% (3972/8896)\n",
            "Loss: 1.515 | Acc: 44.710% (4006/8960)\n",
            "Loss: 1.516 | Acc: 44.692% (4033/9024)\n",
            "Loss: 1.516 | Acc: 44.630% (4056/9088)\n",
            "Loss: 1.516 | Acc: 44.635% (4085/9152)\n",
            "Loss: 1.514 | Acc: 44.683% (4118/9216)\n",
            "Loss: 1.514 | Acc: 44.698% (4148/9280)\n",
            "Loss: 1.514 | Acc: 44.681% (4175/9344)\n",
            "Loss: 1.514 | Acc: 44.643% (4200/9408)\n",
            "Loss: 1.514 | Acc: 44.637% (4228/9472)\n",
            "Loss: 1.514 | Acc: 44.673% (4260/9536)\n",
            "Loss: 1.513 | Acc: 44.708% (4292/9600)\n",
            "Loss: 1.513 | Acc: 44.671% (4317/9664)\n",
            "Loss: 1.513 | Acc: 44.634% (4342/9728)\n",
            "Loss: 1.513 | Acc: 44.649% (4372/9792)\n",
            "Loss: 1.514 | Acc: 44.633% (4399/9856)\n",
            "Loss: 1.514 | Acc: 44.607% (4425/9920)\n",
            "Loss: 1.514 | Acc: 44.631% (4456/9984)\n",
            "Loss: 1.513 | Acc: 44.640% (4464/10000)\n",
            "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 44.64\n",
            "\n",
            "Epoch: 1\n",
            "Loss: 1.545 | Acc: 45.312% (29/64)\n",
            "Loss: 1.461 | Acc: 43.750% (56/128)\n",
            "Loss: 1.501 | Acc: 44.792% (86/192)\n",
            "Loss: 1.478 | Acc: 46.094% (118/256)\n",
            "Loss: 1.454 | Acc: 46.562% (149/320)\n",
            "Loss: 1.415 | Acc: 48.177% (185/384)\n",
            "Loss: 1.414 | Acc: 49.107% (220/448)\n",
            "Loss: 1.410 | Acc: 49.023% (251/512)\n",
            "Loss: 1.414 | Acc: 49.653% (286/576)\n",
            "Loss: 1.409 | Acc: 49.688% (318/640)\n",
            "Loss: 1.405 | Acc: 49.574% (349/704)\n",
            "Loss: 1.421 | Acc: 48.828% (375/768)\n",
            "Loss: 1.424 | Acc: 48.317% (402/832)\n",
            "Loss: 1.425 | Acc: 47.991% (430/896)\n",
            "Loss: 1.427 | Acc: 48.125% (462/960)\n",
            "Loss: 1.434 | Acc: 47.461% (486/1024)\n",
            "Loss: 1.450 | Acc: 47.426% (516/1088)\n",
            "Loss: 1.445 | Acc: 47.569% (548/1152)\n",
            "Loss: 1.446 | Acc: 47.533% (578/1216)\n",
            "Loss: 1.450 | Acc: 47.578% (609/1280)\n",
            "Loss: 1.451 | Acc: 47.396% (637/1344)\n",
            "Loss: 1.439 | Acc: 47.798% (673/1408)\n",
            "Loss: 1.448 | Acc: 47.690% (702/1472)\n",
            "Loss: 1.445 | Acc: 47.917% (736/1536)\n",
            "Loss: 1.448 | Acc: 47.625% (762/1600)\n",
            "Loss: 1.446 | Acc: 47.536% (791/1664)\n",
            "Loss: 1.439 | Acc: 47.917% (828/1728)\n",
            "Loss: 1.435 | Acc: 47.824% (857/1792)\n",
            "Loss: 1.432 | Acc: 47.683% (885/1856)\n",
            "Loss: 1.443 | Acc: 47.292% (908/1920)\n",
            "Loss: 1.440 | Acc: 47.329% (939/1984)\n",
            "Loss: 1.444 | Acc: 47.314% (969/2048)\n",
            "Loss: 1.452 | Acc: 47.017% (993/2112)\n",
            "Loss: 1.452 | Acc: 47.243% (1028/2176)\n",
            "Loss: 1.450 | Acc: 47.411% (1062/2240)\n",
            "Loss: 1.458 | Acc: 47.092% (1085/2304)\n",
            "Loss: 1.455 | Acc: 47.002% (1113/2368)\n",
            "Loss: 1.449 | Acc: 47.204% (1148/2432)\n",
            "Loss: 1.449 | Acc: 47.276% (1180/2496)\n",
            "Loss: 1.453 | Acc: 47.188% (1208/2560)\n",
            "Loss: 1.456 | Acc: 46.989% (1233/2624)\n",
            "Loss: 1.458 | Acc: 46.801% (1258/2688)\n",
            "Loss: 1.459 | Acc: 46.802% (1288/2752)\n",
            "Loss: 1.465 | Acc: 46.662% (1314/2816)\n",
            "Loss: 1.464 | Acc: 46.806% (1348/2880)\n",
            "Loss: 1.463 | Acc: 46.535% (1370/2944)\n",
            "Loss: 1.457 | Acc: 46.676% (1404/3008)\n",
            "Loss: 1.464 | Acc: 46.354% (1424/3072)\n",
            "Loss: 1.465 | Acc: 46.301% (1452/3136)\n",
            "Loss: 1.464 | Acc: 46.375% (1484/3200)\n",
            "Loss: 1.463 | Acc: 46.293% (1511/3264)\n",
            "Loss: 1.462 | Acc: 46.274% (1540/3328)\n",
            "Loss: 1.460 | Acc: 46.315% (1571/3392)\n",
            "Loss: 1.464 | Acc: 46.094% (1593/3456)\n",
            "Loss: 1.463 | Acc: 46.023% (1620/3520)\n",
            "Loss: 1.459 | Acc: 46.345% (1661/3584)\n",
            "Loss: 1.461 | Acc: 46.245% (1687/3648)\n",
            "Loss: 1.466 | Acc: 46.013% (1708/3712)\n",
            "Loss: 1.466 | Acc: 45.895% (1733/3776)\n",
            "Loss: 1.464 | Acc: 46.068% (1769/3840)\n",
            "Loss: 1.461 | Acc: 46.158% (1802/3904)\n",
            "Loss: 1.464 | Acc: 46.069% (1828/3968)\n",
            "Loss: 1.466 | Acc: 46.081% (1858/4032)\n",
            "Loss: 1.466 | Acc: 46.167% (1891/4096)\n",
            "Loss: 1.466 | Acc: 46.106% (1918/4160)\n",
            "Loss: 1.463 | Acc: 46.212% (1952/4224)\n",
            "Loss: 1.462 | Acc: 46.222% (1982/4288)\n",
            "Loss: 1.460 | Acc: 46.255% (2013/4352)\n",
            "Loss: 1.462 | Acc: 46.241% (2042/4416)\n",
            "Loss: 1.458 | Acc: 46.406% (2079/4480)\n",
            "Loss: 1.461 | Acc: 46.391% (2108/4544)\n",
            "Loss: 1.461 | Acc: 46.267% (2132/4608)\n",
            "Loss: 1.461 | Acc: 46.318% (2164/4672)\n",
            "Loss: 1.458 | Acc: 46.410% (2198/4736)\n",
            "Loss: 1.458 | Acc: 46.354% (2225/4800)\n",
            "Loss: 1.461 | Acc: 46.423% (2258/4864)\n",
            "Loss: 1.459 | Acc: 46.489% (2291/4928)\n",
            "Loss: 1.458 | Acc: 46.514% (2322/4992)\n",
            "Loss: 1.456 | Acc: 46.519% (2352/5056)\n",
            "Loss: 1.459 | Acc: 46.465% (2379/5120)\n",
            "Loss: 1.459 | Acc: 46.528% (2412/5184)\n",
            "Loss: 1.456 | Acc: 46.570% (2444/5248)\n",
            "Loss: 1.457 | Acc: 46.480% (2469/5312)\n",
            "Loss: 1.455 | Acc: 46.577% (2504/5376)\n",
            "Loss: 1.453 | Acc: 46.654% (2538/5440)\n",
            "Loss: 1.453 | Acc: 46.639% (2567/5504)\n",
            "Loss: 1.453 | Acc: 46.659% (2598/5568)\n",
            "Loss: 1.454 | Acc: 46.520% (2620/5632)\n",
            "Loss: 1.454 | Acc: 46.489% (2648/5696)\n",
            "Loss: 1.455 | Acc: 46.458% (2676/5760)\n",
            "Loss: 1.454 | Acc: 46.446% (2705/5824)\n",
            "Loss: 1.455 | Acc: 46.382% (2731/5888)\n",
            "Loss: 1.454 | Acc: 46.421% (2763/5952)\n",
            "Loss: 1.457 | Acc: 46.343% (2788/6016)\n",
            "Loss: 1.458 | Acc: 46.283% (2814/6080)\n",
            "Loss: 1.458 | Acc: 46.273% (2843/6144)\n",
            "Loss: 1.457 | Acc: 46.295% (2874/6208)\n",
            "Loss: 1.457 | Acc: 46.397% (2910/6272)\n",
            "Loss: 1.457 | Acc: 46.370% (2938/6336)\n",
            "Loss: 1.455 | Acc: 46.469% (2974/6400)\n",
            "Loss: 1.455 | Acc: 46.457% (3003/6464)\n",
            "Loss: 1.456 | Acc: 46.477% (3034/6528)\n",
            "Loss: 1.455 | Acc: 46.481% (3064/6592)\n",
            "Loss: 1.456 | Acc: 46.439% (3091/6656)\n",
            "Loss: 1.455 | Acc: 46.458% (3122/6720)\n",
            "Loss: 1.454 | Acc: 46.492% (3154/6784)\n",
            "Loss: 1.456 | Acc: 46.437% (3180/6848)\n",
            "Loss: 1.457 | Acc: 46.528% (3216/6912)\n",
            "Loss: 1.459 | Acc: 46.517% (3245/6976)\n",
            "Loss: 1.458 | Acc: 46.520% (3275/7040)\n",
            "Loss: 1.458 | Acc: 46.551% (3307/7104)\n",
            "Loss: 1.461 | Acc: 46.484% (3332/7168)\n",
            "Loss: 1.459 | Acc: 46.557% (3367/7232)\n",
            "Loss: 1.462 | Acc: 46.464% (3390/7296)\n",
            "Loss: 1.461 | Acc: 46.495% (3422/7360)\n",
            "Loss: 1.461 | Acc: 46.404% (3445/7424)\n",
            "Loss: 1.462 | Acc: 46.408% (3475/7488)\n",
            "Loss: 1.460 | Acc: 46.517% (3513/7552)\n",
            "Loss: 1.461 | Acc: 46.494% (3541/7616)\n",
            "Loss: 1.460 | Acc: 46.549% (3575/7680)\n",
            "Loss: 1.461 | Acc: 46.462% (3598/7744)\n",
            "Loss: 1.461 | Acc: 46.491% (3630/7808)\n",
            "Loss: 1.459 | Acc: 46.519% (3662/7872)\n",
            "Loss: 1.459 | Acc: 46.497% (3690/7936)\n",
            "Loss: 1.459 | Acc: 46.487% (3719/8000)\n",
            "Loss: 1.461 | Acc: 46.441% (3745/8064)\n",
            "Loss: 1.462 | Acc: 46.469% (3777/8128)\n",
            "Loss: 1.462 | Acc: 46.497% (3809/8192)\n",
            "Loss: 1.461 | Acc: 46.524% (3841/8256)\n",
            "Loss: 1.462 | Acc: 46.454% (3865/8320)\n",
            "Loss: 1.462 | Acc: 46.422% (3892/8384)\n",
            "Loss: 1.462 | Acc: 46.449% (3924/8448)\n",
            "Loss: 1.461 | Acc: 46.534% (3961/8512)\n",
            "Loss: 1.460 | Acc: 46.549% (3992/8576)\n",
            "Loss: 1.459 | Acc: 46.574% (4024/8640)\n",
            "Loss: 1.458 | Acc: 46.599% (4056/8704)\n",
            "Loss: 1.459 | Acc: 46.578% (4084/8768)\n",
            "Loss: 1.459 | Acc: 46.603% (4116/8832)\n",
            "Loss: 1.460 | Acc: 46.616% (4147/8896)\n",
            "Loss: 1.459 | Acc: 46.674% (4182/8960)\n",
            "Loss: 1.459 | Acc: 46.687% (4213/9024)\n",
            "Loss: 1.459 | Acc: 46.677% (4242/9088)\n",
            "Loss: 1.459 | Acc: 46.700% (4274/9152)\n",
            "Loss: 1.458 | Acc: 46.723% (4306/9216)\n",
            "Loss: 1.457 | Acc: 46.713% (4335/9280)\n",
            "Loss: 1.457 | Acc: 46.747% (4368/9344)\n",
            "Loss: 1.457 | Acc: 46.716% (4395/9408)\n",
            "Loss: 1.458 | Acc: 46.643% (4418/9472)\n",
            "Loss: 1.457 | Acc: 46.655% (4449/9536)\n",
            "Loss: 1.457 | Acc: 46.635% (4477/9600)\n",
            "Loss: 1.458 | Acc: 46.575% (4501/9664)\n",
            "Loss: 1.458 | Acc: 46.577% (4531/9728)\n",
            "Loss: 1.459 | Acc: 46.548% (4558/9792)\n",
            "Loss: 1.460 | Acc: 46.489% (4582/9856)\n",
            "Loss: 1.461 | Acc: 46.421% (4605/9920)\n",
            "Loss: 1.460 | Acc: 46.394% (4632/9984)\n",
            "Loss: 1.459 | Acc: 46.437% (4666/10048)\n",
            "Loss: 1.459 | Acc: 46.450% (4697/10112)\n",
            "Loss: 1.460 | Acc: 46.423% (4724/10176)\n",
            "Loss: 1.459 | Acc: 46.416% (4753/10240)\n",
            "Loss: 1.460 | Acc: 46.341% (4775/10304)\n",
            "Loss: 1.462 | Acc: 46.229% (4793/10368)\n",
            "Loss: 1.463 | Acc: 46.166% (4816/10432)\n",
            "Loss: 1.464 | Acc: 46.132% (4842/10496)\n",
            "Loss: 1.464 | Acc: 46.146% (4873/10560)\n",
            "Loss: 1.464 | Acc: 46.169% (4905/10624)\n",
            "Loss: 1.464 | Acc: 46.201% (4938/10688)\n",
            "Loss: 1.464 | Acc: 46.196% (4967/10752)\n",
            "Loss: 1.465 | Acc: 46.191% (4996/10816)\n",
            "Loss: 1.465 | Acc: 46.167% (5023/10880)\n",
            "Loss: 1.464 | Acc: 46.226% (5059/10944)\n",
            "Loss: 1.463 | Acc: 46.239% (5090/11008)\n",
            "Loss: 1.462 | Acc: 46.252% (5121/11072)\n",
            "Loss: 1.462 | Acc: 46.246% (5150/11136)\n",
            "Loss: 1.463 | Acc: 46.250% (5180/11200)\n",
            "Loss: 1.462 | Acc: 46.280% (5213/11264)\n",
            "Loss: 1.465 | Acc: 46.160% (5229/11328)\n",
            "Loss: 1.466 | Acc: 46.103% (5252/11392)\n",
            "Loss: 1.465 | Acc: 46.142% (5286/11456)\n",
            "Loss: 1.466 | Acc: 46.111% (5312/11520)\n",
            "Loss: 1.466 | Acc: 46.081% (5338/11584)\n",
            "Loss: 1.465 | Acc: 46.111% (5371/11648)\n",
            "Loss: 1.465 | Acc: 46.149% (5405/11712)\n",
            "Loss: 1.467 | Acc: 46.119% (5431/11776)\n",
            "Loss: 1.466 | Acc: 46.166% (5466/11840)\n",
            "Loss: 1.465 | Acc: 46.169% (5496/11904)\n",
            "Loss: 1.466 | Acc: 46.156% (5524/11968)\n",
            "Loss: 1.466 | Acc: 46.127% (5550/12032)\n",
            "Loss: 1.466 | Acc: 46.164% (5584/12096)\n",
            "Loss: 1.466 | Acc: 46.160% (5613/12160)\n",
            "Loss: 1.466 | Acc: 46.155% (5642/12224)\n",
            "Loss: 1.466 | Acc: 46.110% (5666/12288)\n",
            "Loss: 1.467 | Acc: 46.090% (5693/12352)\n",
            "Loss: 1.467 | Acc: 46.086% (5722/12416)\n",
            "Loss: 1.466 | Acc: 46.162% (5761/12480)\n",
            "Loss: 1.465 | Acc: 46.237% (5800/12544)\n",
            "Loss: 1.464 | Acc: 46.264% (5833/12608)\n",
            "Loss: 1.465 | Acc: 46.259% (5862/12672)\n",
            "Loss: 1.465 | Acc: 46.231% (5888/12736)\n",
            "Loss: 1.465 | Acc: 46.219% (5916/12800)\n",
            "Loss: 1.464 | Acc: 46.269% (5952/12864)\n",
            "Loss: 1.464 | Acc: 46.279% (5983/12928)\n",
            "Loss: 1.464 | Acc: 46.267% (6011/12992)\n",
            "Loss: 1.464 | Acc: 46.285% (6043/13056)\n",
            "Loss: 1.464 | Acc: 46.258% (6069/13120)\n",
            "Loss: 1.465 | Acc: 46.215% (6093/13184)\n",
            "Loss: 1.466 | Acc: 46.173% (6117/13248)\n",
            "Loss: 1.465 | Acc: 46.199% (6150/13312)\n",
            "Loss: 1.465 | Acc: 46.165% (6175/13376)\n",
            "Loss: 1.465 | Acc: 46.176% (6206/13440)\n",
            "Loss: 1.465 | Acc: 46.164% (6234/13504)\n",
            "Loss: 1.464 | Acc: 46.182% (6266/13568)\n",
            "Loss: 1.464 | Acc: 46.207% (6299/13632)\n",
            "Loss: 1.465 | Acc: 46.174% (6324/13696)\n",
            "Loss: 1.464 | Acc: 46.170% (6353/13760)\n",
            "Loss: 1.464 | Acc: 46.130% (6377/13824)\n",
            "Loss: 1.464 | Acc: 46.097% (6402/13888)\n",
            "Loss: 1.464 | Acc: 46.079% (6429/13952)\n",
            "Loss: 1.464 | Acc: 46.069% (6457/14016)\n",
            "Loss: 1.464 | Acc: 46.080% (6488/14080)\n",
            "Loss: 1.464 | Acc: 46.069% (6516/14144)\n",
            "Loss: 1.464 | Acc: 46.059% (6544/14208)\n",
            "Loss: 1.463 | Acc: 46.125% (6583/14272)\n",
            "Loss: 1.463 | Acc: 46.129% (6613/14336)\n",
            "Loss: 1.463 | Acc: 46.174% (6649/14400)\n",
            "Loss: 1.463 | Acc: 46.163% (6677/14464)\n",
            "Loss: 1.462 | Acc: 46.187% (6710/14528)\n",
            "Loss: 1.462 | Acc: 46.183% (6739/14592)\n",
            "Loss: 1.462 | Acc: 46.145% (6763/14656)\n",
            "Loss: 1.462 | Acc: 46.155% (6794/14720)\n",
            "Loss: 1.463 | Acc: 46.117% (6818/14784)\n",
            "Loss: 1.463 | Acc: 46.121% (6848/14848)\n",
            "Loss: 1.464 | Acc: 46.111% (6876/14912)\n",
            "Loss: 1.463 | Acc: 46.134% (6909/14976)\n",
            "Loss: 1.464 | Acc: 46.090% (6932/15040)\n",
            "Loss: 1.464 | Acc: 46.061% (6957/15104)\n",
            "Loss: 1.464 | Acc: 46.057% (6986/15168)\n",
            "Loss: 1.464 | Acc: 46.107% (7023/15232)\n",
            "Loss: 1.463 | Acc: 46.110% (7053/15296)\n",
            "Loss: 1.463 | Acc: 46.146% (7088/15360)\n",
            "Loss: 1.463 | Acc: 46.123% (7114/15424)\n",
            "Loss: 1.463 | Acc: 46.120% (7143/15488)\n",
            "Loss: 1.464 | Acc: 46.129% (7174/15552)\n",
            "Loss: 1.462 | Acc: 46.171% (7210/15616)\n",
            "Loss: 1.462 | Acc: 46.212% (7246/15680)\n",
            "Loss: 1.462 | Acc: 46.189% (7272/15744)\n",
            "Loss: 1.462 | Acc: 46.192% (7302/15808)\n",
            "Loss: 1.462 | Acc: 46.188% (7331/15872)\n",
            "Loss: 1.463 | Acc: 46.153% (7355/15936)\n",
            "Loss: 1.461 | Acc: 46.219% (7395/16000)\n",
            "Loss: 1.461 | Acc: 46.259% (7431/16064)\n",
            "Loss: 1.462 | Acc: 46.243% (7458/16128)\n",
            "Loss: 1.462 | Acc: 46.233% (7486/16192)\n",
            "Loss: 1.461 | Acc: 46.266% (7521/16256)\n",
            "Loss: 1.461 | Acc: 46.281% (7553/16320)\n",
            "Loss: 1.460 | Acc: 46.271% (7581/16384)\n",
            "Loss: 1.459 | Acc: 46.328% (7620/16448)\n",
            "Loss: 1.458 | Acc: 46.360% (7655/16512)\n",
            "Loss: 1.459 | Acc: 46.308% (7676/16576)\n",
            "Loss: 1.460 | Acc: 46.244% (7695/16640)\n",
            "Loss: 1.459 | Acc: 46.252% (7726/16704)\n",
            "Loss: 1.458 | Acc: 46.279% (7760/16768)\n",
            "Loss: 1.458 | Acc: 46.317% (7796/16832)\n",
            "Loss: 1.458 | Acc: 46.348% (7831/16896)\n",
            "Loss: 1.458 | Acc: 46.356% (7862/16960)\n",
            "Loss: 1.458 | Acc: 46.346% (7890/17024)\n",
            "Loss: 1.458 | Acc: 46.366% (7923/17088)\n",
            "Loss: 1.459 | Acc: 46.339% (7948/17152)\n",
            "Loss: 1.458 | Acc: 46.364% (7982/17216)\n",
            "Loss: 1.459 | Acc: 46.360% (8011/17280)\n",
            "Loss: 1.459 | Acc: 46.373% (8043/17344)\n",
            "Loss: 1.458 | Acc: 46.392% (8076/17408)\n",
            "Loss: 1.458 | Acc: 46.400% (8107/17472)\n",
            "Loss: 1.458 | Acc: 46.419% (8140/17536)\n",
            "Loss: 1.458 | Acc: 46.432% (8172/17600)\n",
            "Loss: 1.458 | Acc: 46.405% (8197/17664)\n",
            "Loss: 1.458 | Acc: 46.401% (8226/17728)\n",
            "Loss: 1.458 | Acc: 46.414% (8258/17792)\n",
            "Loss: 1.459 | Acc: 46.410% (8287/17856)\n",
            "Loss: 1.459 | Acc: 46.384% (8312/17920)\n",
            "Loss: 1.459 | Acc: 46.386% (8342/17984)\n",
            "Loss: 1.460 | Acc: 46.382% (8371/18048)\n",
            "Loss: 1.458 | Acc: 46.444% (8412/18112)\n",
            "Loss: 1.459 | Acc: 46.413% (8436/18176)\n",
            "Loss: 1.460 | Acc: 46.376% (8459/18240)\n",
            "Loss: 1.461 | Acc: 46.361% (8486/18304)\n",
            "Loss: 1.461 | Acc: 46.341% (8512/18368)\n",
            "Loss: 1.461 | Acc: 46.349% (8543/18432)\n",
            "Loss: 1.460 | Acc: 46.356% (8574/18496)\n",
            "Loss: 1.461 | Acc: 46.336% (8600/18560)\n",
            "Loss: 1.461 | Acc: 46.354% (8633/18624)\n",
            "Loss: 1.461 | Acc: 46.361% (8664/18688)\n",
            "Loss: 1.461 | Acc: 46.336% (8689/18752)\n",
            "Loss: 1.462 | Acc: 46.306% (8713/18816)\n",
            "Loss: 1.461 | Acc: 46.319% (8745/18880)\n",
            "Loss: 1.462 | Acc: 46.331% (8777/18944)\n",
            "Loss: 1.461 | Acc: 46.349% (8810/19008)\n",
            "Loss: 1.462 | Acc: 46.324% (8835/19072)\n",
            "Loss: 1.461 | Acc: 46.347% (8869/19136)\n",
            "Loss: 1.462 | Acc: 46.365% (8902/19200)\n",
            "Loss: 1.462 | Acc: 46.351% (8929/19264)\n",
            "Loss: 1.462 | Acc: 46.342% (8957/19328)\n",
            "Loss: 1.462 | Acc: 46.385% (8995/19392)\n",
            "Loss: 1.462 | Acc: 46.433% (9034/19456)\n",
            "Loss: 1.463 | Acc: 46.424% (9062/19520)\n",
            "Loss: 1.463 | Acc: 46.390% (9085/19584)\n",
            "Loss: 1.462 | Acc: 46.427% (9122/19648)\n",
            "Loss: 1.461 | Acc: 46.464% (9159/19712)\n",
            "Loss: 1.461 | Acc: 46.465% (9189/19776)\n",
            "Loss: 1.461 | Acc: 46.467% (9219/19840)\n",
            "Loss: 1.461 | Acc: 46.488% (9253/19904)\n",
            "Loss: 1.460 | Acc: 46.504% (9286/19968)\n",
            "Loss: 1.460 | Acc: 46.506% (9316/20032)\n",
            "Loss: 1.460 | Acc: 46.477% (9340/20096)\n",
            "Loss: 1.460 | Acc: 46.488% (9372/20160)\n",
            "Loss: 1.461 | Acc: 46.460% (9396/20224)\n",
            "Loss: 1.460 | Acc: 46.476% (9429/20288)\n",
            "Loss: 1.460 | Acc: 46.487% (9461/20352)\n",
            "Loss: 1.461 | Acc: 46.464% (9486/20416)\n",
            "Loss: 1.460 | Acc: 46.479% (9519/20480)\n",
            "Loss: 1.461 | Acc: 46.481% (9549/20544)\n",
            "Loss: 1.461 | Acc: 46.487% (9580/20608)\n",
            "Loss: 1.460 | Acc: 46.493% (9611/20672)\n",
            "Loss: 1.461 | Acc: 46.480% (9638/20736)\n",
            "Loss: 1.461 | Acc: 46.447% (9661/20800)\n",
            "Loss: 1.461 | Acc: 46.444% (9690/20864)\n",
            "Loss: 1.461 | Acc: 46.421% (9715/20928)\n",
            "Loss: 1.461 | Acc: 46.380% (9736/20992)\n",
            "Loss: 1.461 | Acc: 46.400% (9770/21056)\n",
            "Loss: 1.460 | Acc: 46.406% (9801/21120)\n",
            "Loss: 1.460 | Acc: 46.408% (9831/21184)\n",
            "Loss: 1.461 | Acc: 46.395% (9858/21248)\n",
            "Loss: 1.461 | Acc: 46.396% (9888/21312)\n",
            "Loss: 1.461 | Acc: 46.412% (9921/21376)\n",
            "Loss: 1.461 | Acc: 46.390% (9946/21440)\n",
            "Loss: 1.461 | Acc: 46.387% (9975/21504)\n",
            "Loss: 1.462 | Acc: 46.370% (10001/21568)\n",
            "Loss: 1.462 | Acc: 46.380% (10033/21632)\n",
            "Loss: 1.462 | Acc: 46.382% (10063/21696)\n",
            "Loss: 1.462 | Acc: 46.379% (10092/21760)\n",
            "Loss: 1.463 | Acc: 46.353% (10116/21824)\n",
            "Loss: 1.462 | Acc: 46.350% (10145/21888)\n",
            "Loss: 1.462 | Acc: 46.369% (10179/21952)\n",
            "Loss: 1.462 | Acc: 46.371% (10209/22016)\n",
            "Loss: 1.462 | Acc: 46.372% (10239/22080)\n",
            "Loss: 1.462 | Acc: 46.347% (10263/22144)\n",
            "Loss: 1.462 | Acc: 46.348% (10293/22208)\n",
            "Loss: 1.462 | Acc: 46.354% (10324/22272)\n",
            "Loss: 1.461 | Acc: 46.365% (10356/22336)\n",
            "Loss: 1.462 | Acc: 46.339% (10380/22400)\n",
            "Loss: 1.461 | Acc: 46.359% (10414/22464)\n",
            "Loss: 1.461 | Acc: 46.373% (10447/22528)\n",
            "Loss: 1.461 | Acc: 46.370% (10476/22592)\n",
            "Loss: 1.461 | Acc: 46.389% (10510/22656)\n",
            "Loss: 1.461 | Acc: 46.386% (10539/22720)\n",
            "Loss: 1.460 | Acc: 46.397% (10571/22784)\n",
            "Loss: 1.460 | Acc: 46.398% (10601/22848)\n",
            "Loss: 1.461 | Acc: 46.377% (10626/22912)\n",
            "Loss: 1.461 | Acc: 46.379% (10656/22976)\n",
            "Loss: 1.461 | Acc: 46.359% (10681/23040)\n",
            "Loss: 1.461 | Acc: 46.356% (10710/23104)\n",
            "Loss: 1.460 | Acc: 46.361% (10741/23168)\n",
            "Loss: 1.461 | Acc: 46.341% (10766/23232)\n",
            "Loss: 1.461 | Acc: 46.317% (10790/23296)\n",
            "Loss: 1.461 | Acc: 46.340% (10825/23360)\n",
            "Loss: 1.461 | Acc: 46.337% (10854/23424)\n",
            "Loss: 1.461 | Acc: 46.351% (10887/23488)\n",
            "Loss: 1.460 | Acc: 46.374% (10922/23552)\n",
            "Loss: 1.459 | Acc: 46.384% (10954/23616)\n",
            "Loss: 1.460 | Acc: 46.377% (10982/23680)\n",
            "Loss: 1.460 | Acc: 46.378% (11012/23744)\n",
            "Loss: 1.460 | Acc: 46.388% (11044/23808)\n",
            "Loss: 1.460 | Acc: 46.397% (11076/23872)\n",
            "Loss: 1.460 | Acc: 46.420% (11111/23936)\n",
            "Loss: 1.460 | Acc: 46.421% (11141/24000)\n",
            "Loss: 1.460 | Acc: 46.422% (11171/24064)\n",
            "Loss: 1.460 | Acc: 46.423% (11201/24128)\n",
            "Loss: 1.460 | Acc: 46.424% (11231/24192)\n",
            "Loss: 1.459 | Acc: 46.438% (11264/24256)\n",
            "Loss: 1.460 | Acc: 46.414% (11288/24320)\n",
            "Loss: 1.459 | Acc: 46.407% (11316/24384)\n",
            "Loss: 1.459 | Acc: 46.425% (11350/24448)\n",
            "Loss: 1.459 | Acc: 46.422% (11379/24512)\n",
            "Loss: 1.458 | Acc: 46.411% (11406/24576)\n",
            "Loss: 1.458 | Acc: 46.433% (11441/24640)\n",
            "Loss: 1.458 | Acc: 46.430% (11470/24704)\n",
            "Loss: 1.458 | Acc: 46.435% (11501/24768)\n",
            "Loss: 1.458 | Acc: 46.464% (11538/24832)\n",
            "Loss: 1.458 | Acc: 46.477% (11571/24896)\n",
            "Loss: 1.457 | Acc: 46.490% (11604/24960)\n",
            "Loss: 1.457 | Acc: 46.491% (11634/25024)\n",
            "Loss: 1.457 | Acc: 46.508% (11668/25088)\n",
            "Loss: 1.457 | Acc: 46.505% (11697/25152)\n",
            "Loss: 1.456 | Acc: 46.510% (11728/25216)\n",
            "Loss: 1.456 | Acc: 46.507% (11757/25280)\n",
            "Loss: 1.456 | Acc: 46.532% (11793/25344)\n",
            "Loss: 1.455 | Acc: 46.556% (11829/25408)\n",
            "Loss: 1.456 | Acc: 46.553% (11858/25472)\n",
            "Loss: 1.455 | Acc: 46.577% (11894/25536)\n",
            "Loss: 1.455 | Acc: 46.598% (11929/25600)\n",
            "Loss: 1.455 | Acc: 46.591% (11957/25664)\n",
            "Loss: 1.455 | Acc: 46.595% (11988/25728)\n",
            "Loss: 1.456 | Acc: 46.576% (12013/25792)\n",
            "Loss: 1.455 | Acc: 46.589% (12046/25856)\n",
            "Loss: 1.455 | Acc: 46.582% (12074/25920)\n",
            "Loss: 1.455 | Acc: 46.575% (12102/25984)\n",
            "Loss: 1.455 | Acc: 46.576% (12132/26048)\n",
            "Loss: 1.455 | Acc: 46.561% (12158/26112)\n",
            "Loss: 1.455 | Acc: 46.573% (12191/26176)\n",
            "Loss: 1.455 | Acc: 46.559% (12217/26240)\n",
            "Loss: 1.456 | Acc: 46.548% (12244/26304)\n",
            "Loss: 1.455 | Acc: 46.560% (12277/26368)\n",
            "Loss: 1.455 | Acc: 46.584% (12313/26432)\n",
            "Loss: 1.455 | Acc: 46.554% (12335/26496)\n",
            "Loss: 1.455 | Acc: 46.562% (12367/26560)\n",
            "Loss: 1.455 | Acc: 46.567% (12398/26624)\n",
            "Loss: 1.456 | Acc: 46.557% (12425/26688)\n",
            "Loss: 1.456 | Acc: 46.561% (12456/26752)\n",
            "Loss: 1.455 | Acc: 46.592% (12494/26816)\n",
            "Loss: 1.455 | Acc: 46.603% (12527/26880)\n",
            "Loss: 1.455 | Acc: 46.582% (12551/26944)\n",
            "Loss: 1.455 | Acc: 46.582% (12581/27008)\n",
            "Loss: 1.455 | Acc: 46.587% (12612/27072)\n",
            "Loss: 1.455 | Acc: 46.599% (12645/27136)\n",
            "Loss: 1.455 | Acc: 46.574% (12668/27200)\n",
            "Loss: 1.454 | Acc: 46.589% (12702/27264)\n",
            "Loss: 1.454 | Acc: 46.615% (12739/27328)\n",
            "Loss: 1.454 | Acc: 46.608% (12767/27392)\n",
            "Loss: 1.454 | Acc: 46.595% (12793/27456)\n",
            "Loss: 1.454 | Acc: 46.584% (12820/27520)\n",
            "Loss: 1.454 | Acc: 46.585% (12850/27584)\n",
            "Loss: 1.454 | Acc: 46.578% (12878/27648)\n",
            "Loss: 1.453 | Acc: 46.608% (12916/27712)\n",
            "Loss: 1.453 | Acc: 46.609% (12946/27776)\n",
            "Loss: 1.453 | Acc: 46.609% (12976/27840)\n",
            "Loss: 1.453 | Acc: 46.621% (13009/27904)\n",
            "Loss: 1.453 | Acc: 46.639% (13044/27968)\n",
            "Loss: 1.453 | Acc: 46.636% (13073/28032)\n",
            "Loss: 1.453 | Acc: 46.626% (13100/28096)\n",
            "Loss: 1.454 | Acc: 46.612% (13126/28160)\n",
            "Loss: 1.454 | Acc: 46.609% (13155/28224)\n",
            "Loss: 1.454 | Acc: 46.603% (13183/28288)\n",
            "Loss: 1.454 | Acc: 46.610% (13215/28352)\n",
            "Loss: 1.454 | Acc: 46.608% (13244/28416)\n",
            "Loss: 1.454 | Acc: 46.622% (13278/28480)\n",
            "Loss: 1.453 | Acc: 46.665% (13320/28544)\n",
            "Loss: 1.453 | Acc: 46.693% (13358/28608)\n",
            "Loss: 1.453 | Acc: 46.683% (13385/28672)\n",
            "Loss: 1.453 | Acc: 46.694% (13418/28736)\n",
            "Loss: 1.453 | Acc: 46.712% (13453/28800)\n",
            "Loss: 1.452 | Acc: 46.729% (13488/28864)\n",
            "Loss: 1.452 | Acc: 46.716% (13514/28928)\n",
            "Loss: 1.453 | Acc: 46.703% (13540/28992)\n",
            "Loss: 1.453 | Acc: 46.703% (13570/29056)\n",
            "Loss: 1.453 | Acc: 46.700% (13599/29120)\n",
            "Loss: 1.453 | Acc: 46.711% (13632/29184)\n",
            "Loss: 1.453 | Acc: 46.707% (13661/29248)\n",
            "Loss: 1.452 | Acc: 46.715% (13693/29312)\n",
            "Loss: 1.452 | Acc: 46.735% (13729/29376)\n",
            "Loss: 1.452 | Acc: 46.749% (13763/29440)\n",
            "Loss: 1.451 | Acc: 46.746% (13792/29504)\n",
            "Loss: 1.451 | Acc: 46.760% (13826/29568)\n",
            "Loss: 1.452 | Acc: 46.750% (13853/29632)\n",
            "Loss: 1.452 | Acc: 46.754% (13884/29696)\n",
            "Loss: 1.452 | Acc: 46.751% (13913/29760)\n",
            "Loss: 1.451 | Acc: 46.754% (13944/29824)\n",
            "Loss: 1.451 | Acc: 46.734% (13968/29888)\n",
            "Loss: 1.451 | Acc: 46.748% (14002/29952)\n",
            "Loss: 1.451 | Acc: 46.772% (14039/30016)\n",
            "Loss: 1.451 | Acc: 46.772% (14069/30080)\n",
            "Loss: 1.451 | Acc: 46.766% (14097/30144)\n",
            "Loss: 1.451 | Acc: 46.756% (14124/30208)\n",
            "Loss: 1.451 | Acc: 46.759% (14155/30272)\n",
            "Loss: 1.451 | Acc: 46.770% (14188/30336)\n",
            "Loss: 1.451 | Acc: 46.766% (14217/30400)\n",
            "Loss: 1.451 | Acc: 46.760% (14245/30464)\n",
            "Loss: 1.451 | Acc: 46.767% (14277/30528)\n",
            "Loss: 1.451 | Acc: 46.761% (14305/30592)\n",
            "Loss: 1.451 | Acc: 46.764% (14336/30656)\n",
            "Loss: 1.451 | Acc: 46.781% (14371/30720)\n",
            "Loss: 1.451 | Acc: 46.774% (14399/30784)\n",
            "Loss: 1.451 | Acc: 46.775% (14429/30848)\n",
            "Loss: 1.451 | Acc: 46.762% (14455/30912)\n",
            "Loss: 1.451 | Acc: 46.759% (14484/30976)\n",
            "Loss: 1.451 | Acc: 46.756% (14513/31040)\n",
            "Loss: 1.452 | Acc: 46.750% (14541/31104)\n",
            "Loss: 1.451 | Acc: 46.779% (14580/31168)\n",
            "Loss: 1.451 | Acc: 46.808% (14619/31232)\n",
            "Loss: 1.450 | Acc: 46.824% (14654/31296)\n",
            "Loss: 1.450 | Acc: 46.824% (14684/31360)\n",
            "Loss: 1.450 | Acc: 46.824% (14714/31424)\n",
            "Loss: 1.450 | Acc: 46.850% (14752/31488)\n",
            "Loss: 1.450 | Acc: 46.850% (14782/31552)\n",
            "Loss: 1.450 | Acc: 46.843% (14810/31616)\n",
            "Loss: 1.450 | Acc: 46.850% (14842/31680)\n",
            "Loss: 1.449 | Acc: 46.869% (14878/31744)\n",
            "Loss: 1.449 | Acc: 46.866% (14907/31808)\n",
            "Loss: 1.449 | Acc: 46.888% (14944/31872)\n",
            "Loss: 1.449 | Acc: 46.897% (14977/31936)\n",
            "Loss: 1.449 | Acc: 46.903% (15009/32000)\n",
            "Loss: 1.449 | Acc: 46.887% (15034/32064)\n",
            "Loss: 1.449 | Acc: 46.884% (15063/32128)\n",
            "Loss: 1.449 | Acc: 46.884% (15093/32192)\n",
            "Loss: 1.449 | Acc: 46.891% (15125/32256)\n",
            "Loss: 1.449 | Acc: 46.887% (15154/32320)\n",
            "Loss: 1.448 | Acc: 46.900% (15188/32384)\n",
            "Loss: 1.448 | Acc: 46.903% (15219/32448)\n",
            "Loss: 1.449 | Acc: 46.912% (15252/32512)\n",
            "Loss: 1.449 | Acc: 46.933% (15289/32576)\n",
            "Loss: 1.448 | Acc: 46.952% (15325/32640)\n",
            "Loss: 1.448 | Acc: 46.973% (15362/32704)\n",
            "Loss: 1.448 | Acc: 46.985% (15396/32768)\n",
            "Loss: 1.447 | Acc: 46.994% (15429/32832)\n",
            "Loss: 1.447 | Acc: 46.991% (15458/32896)\n",
            "Loss: 1.447 | Acc: 46.996% (15490/32960)\n",
            "Loss: 1.447 | Acc: 46.993% (15519/33024)\n",
            "Loss: 1.447 | Acc: 46.987% (15547/33088)\n",
            "Loss: 1.447 | Acc: 47.002% (15582/33152)\n",
            "Loss: 1.447 | Acc: 47.004% (15613/33216)\n",
            "Loss: 1.447 | Acc: 47.001% (15642/33280)\n",
            "Loss: 1.446 | Acc: 47.028% (15681/33344)\n",
            "Loss: 1.447 | Acc: 47.019% (15708/33408)\n",
            "Loss: 1.446 | Acc: 47.024% (15740/33472)\n",
            "Loss: 1.446 | Acc: 47.039% (15775/33536)\n",
            "Loss: 1.446 | Acc: 47.039% (15805/33600)\n",
            "Loss: 1.446 | Acc: 47.041% (15836/33664)\n",
            "Loss: 1.445 | Acc: 47.053% (15870/33728)\n",
            "Loss: 1.446 | Acc: 47.026% (15891/33792)\n",
            "Loss: 1.446 | Acc: 47.040% (15926/33856)\n",
            "Loss: 1.446 | Acc: 47.025% (15951/33920)\n",
            "Loss: 1.446 | Acc: 47.031% (15983/33984)\n",
            "Loss: 1.446 | Acc: 47.025% (16011/34048)\n",
            "Loss: 1.446 | Acc: 47.030% (16043/34112)\n",
            "Loss: 1.446 | Acc: 47.013% (16067/34176)\n",
            "Loss: 1.446 | Acc: 47.012% (16097/34240)\n",
            "Loss: 1.446 | Acc: 46.997% (16122/34304)\n",
            "Loss: 1.446 | Acc: 47.012% (16157/34368)\n",
            "Loss: 1.446 | Acc: 47.020% (16190/34432)\n",
            "Loss: 1.446 | Acc: 47.005% (16215/34496)\n",
            "Loss: 1.446 | Acc: 46.999% (16243/34560)\n",
            "Loss: 1.447 | Acc: 46.985% (16268/34624)\n",
            "Loss: 1.447 | Acc: 46.959% (16289/34688)\n",
            "Loss: 1.447 | Acc: 46.973% (16324/34752)\n",
            "Loss: 1.447 | Acc: 46.976% (16355/34816)\n",
            "Loss: 1.447 | Acc: 46.975% (16385/34880)\n",
            "Loss: 1.447 | Acc: 46.981% (16417/34944)\n",
            "Loss: 1.447 | Acc: 46.961% (16440/35008)\n",
            "Loss: 1.448 | Acc: 46.943% (16464/35072)\n",
            "Loss: 1.448 | Acc: 46.932% (16490/35136)\n",
            "Loss: 1.448 | Acc: 46.935% (16521/35200)\n",
            "Loss: 1.448 | Acc: 46.937% (16552/35264)\n",
            "Loss: 1.448 | Acc: 46.929% (16579/35328)\n",
            "Loss: 1.448 | Acc: 46.948% (16616/35392)\n",
            "Loss: 1.448 | Acc: 46.937% (16642/35456)\n",
            "Loss: 1.448 | Acc: 46.945% (16675/35520)\n",
            "Loss: 1.448 | Acc: 46.951% (16707/35584)\n",
            "Loss: 1.448 | Acc: 46.979% (16747/35648)\n",
            "Loss: 1.448 | Acc: 46.970% (16774/35712)\n",
            "Loss: 1.448 | Acc: 46.995% (16813/35776)\n",
            "Loss: 1.448 | Acc: 47.001% (16845/35840)\n",
            "Loss: 1.448 | Acc: 46.992% (16872/35904)\n",
            "Loss: 1.448 | Acc: 46.986% (16900/35968)\n",
            "Loss: 1.447 | Acc: 46.997% (16934/36032)\n",
            "Loss: 1.447 | Acc: 47.002% (16966/36096)\n",
            "Loss: 1.447 | Acc: 47.024% (17004/36160)\n",
            "Loss: 1.447 | Acc: 47.030% (17036/36224)\n",
            "Loss: 1.447 | Acc: 47.032% (17067/36288)\n",
            "Loss: 1.447 | Acc: 47.029% (17096/36352)\n",
            "Loss: 1.447 | Acc: 47.023% (17124/36416)\n",
            "Loss: 1.447 | Acc: 47.037% (17159/36480)\n",
            "Loss: 1.447 | Acc: 47.058% (17197/36544)\n",
            "Loss: 1.447 | Acc: 47.066% (17230/36608)\n",
            "Loss: 1.447 | Acc: 47.080% (17265/36672)\n",
            "Loss: 1.447 | Acc: 47.074% (17293/36736)\n",
            "Loss: 1.447 | Acc: 47.079% (17325/36800)\n",
            "Loss: 1.447 | Acc: 47.073% (17353/36864)\n",
            "Loss: 1.447 | Acc: 47.073% (17383/36928)\n",
            "Loss: 1.447 | Acc: 47.083% (17417/36992)\n",
            "Loss: 1.447 | Acc: 47.091% (17450/37056)\n",
            "Loss: 1.447 | Acc: 47.096% (17482/37120)\n",
            "Loss: 1.447 | Acc: 47.106% (17516/37184)\n",
            "Loss: 1.447 | Acc: 47.098% (17543/37248)\n",
            "Loss: 1.447 | Acc: 47.105% (17576/37312)\n",
            "Loss: 1.447 | Acc: 47.110% (17608/37376)\n",
            "Loss: 1.447 | Acc: 47.123% (17643/37440)\n",
            "Loss: 1.447 | Acc: 47.139% (17679/37504)\n",
            "Loss: 1.446 | Acc: 47.152% (17714/37568)\n",
            "Loss: 1.447 | Acc: 47.149% (17743/37632)\n",
            "Loss: 1.447 | Acc: 47.154% (17775/37696)\n",
            "Loss: 1.447 | Acc: 47.148% (17803/37760)\n",
            "Loss: 1.447 | Acc: 47.161% (17838/37824)\n",
            "Loss: 1.447 | Acc: 47.152% (17865/37888)\n",
            "Loss: 1.447 | Acc: 47.154% (17896/37952)\n",
            "Loss: 1.447 | Acc: 47.154% (17926/38016)\n",
            "Loss: 1.447 | Acc: 47.161% (17959/38080)\n",
            "Loss: 1.447 | Acc: 47.166% (17991/38144)\n",
            "Loss: 1.446 | Acc: 47.181% (18027/38208)\n",
            "Loss: 1.446 | Acc: 47.189% (18060/38272)\n",
            "Loss: 1.446 | Acc: 47.201% (18095/38336)\n",
            "Loss: 1.446 | Acc: 47.195% (18123/38400)\n",
            "Loss: 1.446 | Acc: 47.190% (18151/38464)\n",
            "Loss: 1.446 | Acc: 47.192% (18182/38528)\n",
            "Loss: 1.446 | Acc: 47.214% (18221/38592)\n",
            "Loss: 1.446 | Acc: 47.222% (18254/38656)\n",
            "Loss: 1.446 | Acc: 47.216% (18282/38720)\n",
            "Loss: 1.446 | Acc: 47.223% (18315/38784)\n",
            "Loss: 1.446 | Acc: 47.217% (18343/38848)\n",
            "Loss: 1.445 | Acc: 47.230% (18378/38912)\n",
            "Loss: 1.445 | Acc: 47.229% (18408/38976)\n",
            "Loss: 1.445 | Acc: 47.239% (18442/39040)\n",
            "Loss: 1.445 | Acc: 47.251% (18477/39104)\n",
            "Loss: 1.445 | Acc: 47.250% (18507/39168)\n",
            "Loss: 1.445 | Acc: 47.257% (18540/39232)\n",
            "Loss: 1.445 | Acc: 47.254% (18569/39296)\n",
            "Loss: 1.445 | Acc: 47.251% (18598/39360)\n",
            "Loss: 1.445 | Acc: 47.266% (18634/39424)\n",
            "Loss: 1.445 | Acc: 47.260% (18662/39488)\n",
            "Loss: 1.445 | Acc: 47.269% (18696/39552)\n",
            "Loss: 1.445 | Acc: 47.269% (18726/39616)\n",
            "Loss: 1.445 | Acc: 47.258% (18752/39680)\n",
            "Loss: 1.445 | Acc: 47.237% (18774/39744)\n",
            "Loss: 1.445 | Acc: 47.257% (18812/39808)\n",
            "Loss: 1.445 | Acc: 47.256% (18842/39872)\n",
            "Loss: 1.445 | Acc: 47.261% (18874/39936)\n",
            "Loss: 1.445 | Acc: 47.260% (18904/40000)\n",
            "Loss: 1.445 | Acc: 47.259% (18934/40064)\n",
            "Loss: 1.445 | Acc: 47.266% (18967/40128)\n",
            "Loss: 1.445 | Acc: 47.258% (18994/40192)\n",
            "Loss: 1.445 | Acc: 47.265% (19027/40256)\n",
            "Loss: 1.445 | Acc: 47.262% (19056/40320)\n",
            "Loss: 1.445 | Acc: 47.269% (19089/40384)\n",
            "Loss: 1.445 | Acc: 47.273% (19121/40448)\n",
            "Loss: 1.445 | Acc: 47.277% (19153/40512)\n",
            "Loss: 1.445 | Acc: 47.292% (19189/40576)\n",
            "Loss: 1.445 | Acc: 47.291% (19219/40640)\n",
            "Loss: 1.445 | Acc: 47.295% (19251/40704)\n",
            "Loss: 1.444 | Acc: 47.297% (19282/40768)\n",
            "Loss: 1.445 | Acc: 47.306% (19316/40832)\n",
            "Loss: 1.444 | Acc: 47.315% (19350/40896)\n",
            "Loss: 1.444 | Acc: 47.317% (19381/40960)\n",
            "Loss: 1.444 | Acc: 47.328% (19416/41024)\n",
            "Loss: 1.444 | Acc: 47.347% (19454/41088)\n",
            "Loss: 1.444 | Acc: 47.351% (19486/41152)\n",
            "Loss: 1.444 | Acc: 47.336% (19510/41216)\n",
            "Loss: 1.444 | Acc: 47.343% (19543/41280)\n",
            "Loss: 1.444 | Acc: 47.354% (19578/41344)\n",
            "Loss: 1.444 | Acc: 47.356% (19609/41408)\n",
            "Loss: 1.444 | Acc: 47.345% (19635/41472)\n",
            "Loss: 1.443 | Acc: 47.349% (19667/41536)\n",
            "Loss: 1.444 | Acc: 47.346% (19696/41600)\n",
            "Loss: 1.443 | Acc: 47.355% (19730/41664)\n",
            "Loss: 1.443 | Acc: 47.361% (19763/41728)\n",
            "Loss: 1.443 | Acc: 47.392% (19806/41792)\n",
            "Loss: 1.443 | Acc: 47.396% (19838/41856)\n",
            "Loss: 1.443 | Acc: 47.390% (19866/41920)\n",
            "Loss: 1.443 | Acc: 47.385% (19894/41984)\n",
            "Loss: 1.443 | Acc: 47.384% (19924/42048)\n",
            "Loss: 1.443 | Acc: 47.388% (19956/42112)\n",
            "Loss: 1.442 | Acc: 47.390% (19987/42176)\n",
            "Loss: 1.442 | Acc: 47.393% (20019/42240)\n",
            "Loss: 1.442 | Acc: 47.388% (20047/42304)\n",
            "Loss: 1.443 | Acc: 47.378% (20073/42368)\n",
            "Loss: 1.443 | Acc: 47.360% (20096/42432)\n",
            "Loss: 1.443 | Acc: 47.374% (20132/42496)\n",
            "Loss: 1.443 | Acc: 47.392% (20170/42560)\n",
            "Loss: 1.443 | Acc: 47.379% (20195/42624)\n",
            "Loss: 1.443 | Acc: 47.372% (20222/42688)\n",
            "Loss: 1.443 | Acc: 47.371% (20252/42752)\n",
            "Loss: 1.443 | Acc: 47.375% (20284/42816)\n",
            "Loss: 1.443 | Acc: 47.386% (20319/42880)\n",
            "Loss: 1.443 | Acc: 47.392% (20352/42944)\n",
            "Loss: 1.443 | Acc: 47.391% (20382/43008)\n",
            "Loss: 1.443 | Acc: 47.381% (20408/43072)\n",
            "Loss: 1.443 | Acc: 47.369% (20433/43136)\n",
            "Loss: 1.443 | Acc: 47.361% (20460/43200)\n",
            "Loss: 1.443 | Acc: 47.360% (20490/43264)\n",
            "Loss: 1.443 | Acc: 47.369% (20524/43328)\n",
            "Loss: 1.443 | Acc: 47.361% (20551/43392)\n",
            "Loss: 1.442 | Acc: 47.367% (20584/43456)\n",
            "Loss: 1.442 | Acc: 47.369% (20615/43520)\n",
            "Loss: 1.442 | Acc: 47.366% (20644/43584)\n",
            "Loss: 1.442 | Acc: 47.377% (20679/43648)\n",
            "Loss: 1.442 | Acc: 47.355% (20700/43712)\n",
            "Loss: 1.442 | Acc: 47.366% (20735/43776)\n",
            "Loss: 1.442 | Acc: 47.354% (20760/43840)\n",
            "Loss: 1.442 | Acc: 47.360% (20793/43904)\n",
            "Loss: 1.442 | Acc: 47.375% (20830/43968)\n",
            "Loss: 1.441 | Acc: 47.388% (20866/44032)\n",
            "Loss: 1.441 | Acc: 47.378% (20892/44096)\n",
            "Loss: 1.441 | Acc: 47.398% (20931/44160)\n",
            "Loss: 1.441 | Acc: 47.391% (20958/44224)\n",
            "Loss: 1.441 | Acc: 47.412% (20998/44288)\n",
            "Loss: 1.440 | Acc: 47.425% (21034/44352)\n",
            "Loss: 1.440 | Acc: 47.418% (21061/44416)\n",
            "Loss: 1.440 | Acc: 47.424% (21094/44480)\n",
            "Loss: 1.440 | Acc: 47.434% (21129/44544)\n",
            "Loss: 1.440 | Acc: 47.440% (21162/44608)\n",
            "Loss: 1.440 | Acc: 47.446% (21195/44672)\n",
            "Loss: 1.440 | Acc: 47.461% (21232/44736)\n",
            "Loss: 1.440 | Acc: 47.455% (21260/44800)\n",
            "Loss: 1.440 | Acc: 47.461% (21293/44864)\n",
            "Loss: 1.440 | Acc: 47.447% (21317/44928)\n",
            "Loss: 1.440 | Acc: 47.446% (21347/44992)\n",
            "Loss: 1.440 | Acc: 47.456% (21382/45056)\n",
            "Loss: 1.440 | Acc: 47.467% (21417/45120)\n",
            "Loss: 1.439 | Acc: 47.484% (21455/45184)\n",
            "Loss: 1.439 | Acc: 47.478% (21483/45248)\n",
            "Loss: 1.440 | Acc: 47.458% (21504/45312)\n",
            "Loss: 1.439 | Acc: 47.472% (21541/45376)\n",
            "Loss: 1.439 | Acc: 47.471% (21571/45440)\n",
            "Loss: 1.439 | Acc: 47.482% (21606/45504)\n",
            "Loss: 1.439 | Acc: 47.481% (21636/45568)\n",
            "Loss: 1.439 | Acc: 47.484% (21668/45632)\n",
            "Loss: 1.439 | Acc: 47.488% (21700/45696)\n",
            "Loss: 1.439 | Acc: 47.485% (21729/45760)\n",
            "Loss: 1.439 | Acc: 47.482% (21758/45824)\n",
            "Loss: 1.439 | Acc: 47.472% (21784/45888)\n",
            "Loss: 1.439 | Acc: 47.471% (21814/45952)\n",
            "Loss: 1.439 | Acc: 47.479% (21848/46016)\n",
            "Loss: 1.439 | Acc: 47.483% (21880/46080)\n",
            "Loss: 1.439 | Acc: 47.486% (21912/46144)\n",
            "Loss: 1.439 | Acc: 47.472% (21936/46208)\n",
            "Loss: 1.439 | Acc: 47.471% (21966/46272)\n",
            "Loss: 1.439 | Acc: 47.471% (21996/46336)\n",
            "Loss: 1.439 | Acc: 47.474% (22028/46400)\n",
            "Loss: 1.439 | Acc: 47.456% (22050/46464)\n",
            "Loss: 1.439 | Acc: 47.462% (22083/46528)\n",
            "Loss: 1.439 | Acc: 47.474% (22119/46592)\n",
            "Loss: 1.439 | Acc: 47.460% (22143/46656)\n",
            "Loss: 1.439 | Acc: 47.470% (22178/46720)\n",
            "Loss: 1.439 | Acc: 47.480% (22213/46784)\n",
            "Loss: 1.439 | Acc: 47.473% (22240/46848)\n",
            "Loss: 1.440 | Acc: 47.468% (22268/46912)\n",
            "Loss: 1.440 | Acc: 47.467% (22298/46976)\n",
            "Loss: 1.440 | Acc: 47.468% (22329/47040)\n",
            "Loss: 1.440 | Acc: 47.455% (22353/47104)\n",
            "Loss: 1.440 | Acc: 47.452% (22382/47168)\n",
            "Loss: 1.440 | Acc: 47.451% (22412/47232)\n",
            "Loss: 1.440 | Acc: 47.465% (22449/47296)\n",
            "Loss: 1.439 | Acc: 47.477% (22485/47360)\n",
            "Loss: 1.439 | Acc: 47.472% (22513/47424)\n",
            "Loss: 1.439 | Acc: 47.475% (22545/47488)\n",
            "Loss: 1.439 | Acc: 47.470% (22573/47552)\n",
            "Loss: 1.439 | Acc: 47.457% (22597/47616)\n",
            "Loss: 1.439 | Acc: 47.456% (22627/47680)\n",
            "Loss: 1.439 | Acc: 47.459% (22659/47744)\n",
            "Loss: 1.439 | Acc: 47.452% (22686/47808)\n",
            "Loss: 1.439 | Acc: 47.449% (22715/47872)\n",
            "Loss: 1.439 | Acc: 47.467% (22754/47936)\n",
            "Loss: 1.439 | Acc: 47.465% (22783/48000)\n",
            "Loss: 1.439 | Acc: 47.466% (22814/48064)\n",
            "Loss: 1.439 | Acc: 47.473% (22848/48128)\n",
            "Loss: 1.439 | Acc: 47.487% (22885/48192)\n",
            "Loss: 1.438 | Acc: 47.482% (22913/48256)\n",
            "Loss: 1.438 | Acc: 47.492% (22948/48320)\n",
            "Loss: 1.438 | Acc: 47.495% (22980/48384)\n",
            "Loss: 1.438 | Acc: 47.496% (23011/48448)\n",
            "Loss: 1.438 | Acc: 47.493% (23040/48512)\n",
            "Loss: 1.439 | Acc: 47.488% (23068/48576)\n",
            "Loss: 1.439 | Acc: 47.492% (23100/48640)\n",
            "Loss: 1.439 | Acc: 47.491% (23130/48704)\n",
            "Loss: 1.439 | Acc: 47.488% (23159/48768)\n",
            "Loss: 1.439 | Acc: 47.483% (23187/48832)\n",
            "Loss: 1.439 | Acc: 47.493% (23222/48896)\n",
            "Loss: 1.439 | Acc: 47.480% (23246/48960)\n",
            "Loss: 1.439 | Acc: 47.484% (23267/49000)\n",
            "Epoch 1 of training is completed, Training accuracy for this epoch is 47.48367346938775\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.154 | Acc: 57.812% (37/64)\n",
            "Loss: 1.266 | Acc: 54.688% (70/128)\n",
            "Loss: 1.327 | Acc: 51.562% (99/192)\n",
            "Loss: 1.376 | Acc: 48.438% (124/256)\n",
            "Loss: 1.383 | Acc: 47.188% (151/320)\n",
            "Loss: 1.407 | Acc: 46.354% (178/384)\n",
            "Loss: 1.439 | Acc: 44.643% (200/448)\n",
            "Loss: 1.435 | Acc: 44.922% (230/512)\n",
            "Loss: 1.402 | Acc: 45.833% (264/576)\n",
            "Loss: 1.388 | Acc: 46.875% (300/640)\n",
            "Loss: 1.393 | Acc: 46.733% (329/704)\n",
            "Loss: 1.385 | Acc: 47.396% (364/768)\n",
            "Loss: 1.380 | Acc: 47.236% (393/832)\n",
            "Loss: 1.378 | Acc: 47.433% (425/896)\n",
            "Loss: 1.375 | Acc: 48.021% (461/960)\n",
            "Loss: 1.357 | Acc: 49.219% (504/1024)\n",
            "Loss: 1.359 | Acc: 49.265% (536/1088)\n",
            "Loss: 1.352 | Acc: 49.826% (574/1152)\n",
            "Loss: 1.352 | Acc: 49.918% (607/1216)\n",
            "Loss: 1.366 | Acc: 49.453% (633/1280)\n",
            "Loss: 1.363 | Acc: 49.628% (667/1344)\n",
            "Loss: 1.354 | Acc: 49.787% (701/1408)\n",
            "Loss: 1.354 | Acc: 49.728% (732/1472)\n",
            "Loss: 1.360 | Acc: 49.870% (766/1536)\n",
            "Loss: 1.363 | Acc: 49.562% (793/1600)\n",
            "Loss: 1.367 | Acc: 49.760% (828/1664)\n",
            "Loss: 1.365 | Acc: 49.884% (862/1728)\n",
            "Loss: 1.370 | Acc: 49.777% (892/1792)\n",
            "Loss: 1.366 | Acc: 50.000% (928/1856)\n",
            "Loss: 1.366 | Acc: 50.104% (962/1920)\n",
            "Loss: 1.364 | Acc: 50.151% (995/1984)\n",
            "Loss: 1.367 | Acc: 49.951% (1023/2048)\n",
            "Loss: 1.366 | Acc: 49.858% (1053/2112)\n",
            "Loss: 1.363 | Acc: 50.000% (1088/2176)\n",
            "Loss: 1.363 | Acc: 50.000% (1120/2240)\n",
            "Loss: 1.364 | Acc: 49.913% (1150/2304)\n",
            "Loss: 1.367 | Acc: 49.747% (1178/2368)\n",
            "Loss: 1.371 | Acc: 49.712% (1209/2432)\n",
            "Loss: 1.366 | Acc: 49.840% (1244/2496)\n",
            "Loss: 1.374 | Acc: 49.766% (1274/2560)\n",
            "Loss: 1.376 | Acc: 49.695% (1304/2624)\n",
            "Loss: 1.377 | Acc: 49.740% (1337/2688)\n",
            "Loss: 1.374 | Acc: 49.746% (1369/2752)\n",
            "Loss: 1.375 | Acc: 49.574% (1396/2816)\n",
            "Loss: 1.374 | Acc: 49.514% (1426/2880)\n",
            "Loss: 1.373 | Acc: 49.490% (1457/2944)\n",
            "Loss: 1.371 | Acc: 49.734% (1496/3008)\n",
            "Loss: 1.372 | Acc: 49.772% (1529/3072)\n",
            "Loss: 1.369 | Acc: 49.713% (1559/3136)\n",
            "Loss: 1.367 | Acc: 49.875% (1596/3200)\n",
            "Loss: 1.366 | Acc: 49.877% (1628/3264)\n",
            "Loss: 1.365 | Acc: 49.910% (1661/3328)\n",
            "Loss: 1.366 | Acc: 49.912% (1693/3392)\n",
            "Loss: 1.368 | Acc: 49.797% (1721/3456)\n",
            "Loss: 1.368 | Acc: 49.688% (1749/3520)\n",
            "Loss: 1.367 | Acc: 49.637% (1779/3584)\n",
            "Loss: 1.368 | Acc: 49.753% (1815/3648)\n",
            "Loss: 1.366 | Acc: 49.758% (1847/3712)\n",
            "Loss: 1.368 | Acc: 49.762% (1879/3776)\n",
            "Loss: 1.366 | Acc: 49.948% (1918/3840)\n",
            "Loss: 1.366 | Acc: 49.923% (1949/3904)\n",
            "Loss: 1.365 | Acc: 49.924% (1981/3968)\n",
            "Loss: 1.364 | Acc: 49.876% (2011/4032)\n",
            "Loss: 1.365 | Acc: 49.927% (2045/4096)\n",
            "Loss: 1.368 | Acc: 49.784% (2071/4160)\n",
            "Loss: 1.366 | Acc: 49.882% (2107/4224)\n",
            "Loss: 1.365 | Acc: 49.813% (2136/4288)\n",
            "Loss: 1.364 | Acc: 49.931% (2173/4352)\n",
            "Loss: 1.362 | Acc: 50.045% (2210/4416)\n",
            "Loss: 1.362 | Acc: 50.000% (2240/4480)\n",
            "Loss: 1.361 | Acc: 50.022% (2273/4544)\n",
            "Loss: 1.364 | Acc: 49.870% (2298/4608)\n",
            "Loss: 1.362 | Acc: 49.979% (2335/4672)\n",
            "Loss: 1.359 | Acc: 49.958% (2366/4736)\n",
            "Loss: 1.360 | Acc: 50.000% (2400/4800)\n",
            "Loss: 1.358 | Acc: 50.000% (2432/4864)\n",
            "Loss: 1.358 | Acc: 50.081% (2468/4928)\n",
            "Loss: 1.359 | Acc: 49.980% (2495/4992)\n",
            "Loss: 1.359 | Acc: 50.020% (2529/5056)\n",
            "Loss: 1.361 | Acc: 49.941% (2557/5120)\n",
            "Loss: 1.361 | Acc: 49.981% (2591/5184)\n",
            "Loss: 1.361 | Acc: 49.943% (2621/5248)\n",
            "Loss: 1.360 | Acc: 49.962% (2654/5312)\n",
            "Loss: 1.365 | Acc: 49.814% (2678/5376)\n",
            "Loss: 1.364 | Acc: 49.871% (2713/5440)\n",
            "Loss: 1.365 | Acc: 49.836% (2743/5504)\n",
            "Loss: 1.366 | Acc: 49.784% (2772/5568)\n",
            "Loss: 1.369 | Acc: 49.698% (2799/5632)\n",
            "Loss: 1.370 | Acc: 49.666% (2829/5696)\n",
            "Loss: 1.368 | Acc: 49.705% (2863/5760)\n",
            "Loss: 1.367 | Acc: 49.708% (2895/5824)\n",
            "Loss: 1.369 | Acc: 49.626% (2922/5888)\n",
            "Loss: 1.371 | Acc: 49.580% (2951/5952)\n",
            "Loss: 1.371 | Acc: 49.568% (2982/6016)\n",
            "Loss: 1.370 | Acc: 49.556% (3013/6080)\n",
            "Loss: 1.370 | Acc: 49.544% (3044/6144)\n",
            "Loss: 1.371 | Acc: 49.452% (3070/6208)\n",
            "Loss: 1.373 | Acc: 49.330% (3094/6272)\n",
            "Loss: 1.371 | Acc: 49.432% (3132/6336)\n",
            "Loss: 1.372 | Acc: 49.391% (3161/6400)\n",
            "Loss: 1.372 | Acc: 49.381% (3192/6464)\n",
            "Loss: 1.372 | Acc: 49.433% (3227/6528)\n",
            "Loss: 1.374 | Acc: 49.348% (3253/6592)\n",
            "Loss: 1.374 | Acc: 49.264% (3279/6656)\n",
            "Loss: 1.376 | Acc: 49.182% (3305/6720)\n",
            "Loss: 1.375 | Acc: 49.233% (3340/6784)\n",
            "Loss: 1.371 | Acc: 49.328% (3378/6848)\n",
            "Loss: 1.373 | Acc: 49.248% (3404/6912)\n",
            "Loss: 1.375 | Acc: 49.255% (3436/6976)\n",
            "Loss: 1.376 | Acc: 49.233% (3466/7040)\n",
            "Loss: 1.376 | Acc: 49.212% (3496/7104)\n",
            "Loss: 1.377 | Acc: 49.135% (3522/7168)\n",
            "Loss: 1.378 | Acc: 49.115% (3552/7232)\n",
            "Loss: 1.378 | Acc: 49.041% (3578/7296)\n",
            "Loss: 1.376 | Acc: 49.117% (3615/7360)\n",
            "Loss: 1.376 | Acc: 49.030% (3640/7424)\n",
            "Loss: 1.376 | Acc: 49.038% (3672/7488)\n",
            "Loss: 1.376 | Acc: 49.007% (3701/7552)\n",
            "Loss: 1.377 | Acc: 48.923% (3726/7616)\n",
            "Loss: 1.376 | Acc: 48.997% (3763/7680)\n",
            "Loss: 1.374 | Acc: 49.109% (3803/7744)\n",
            "Loss: 1.374 | Acc: 49.129% (3836/7808)\n",
            "Loss: 1.375 | Acc: 49.149% (3869/7872)\n",
            "Loss: 1.374 | Acc: 49.118% (3898/7936)\n",
            "Loss: 1.376 | Acc: 49.075% (3926/8000)\n",
            "Loss: 1.376 | Acc: 49.070% (3957/8064)\n",
            "Loss: 1.376 | Acc: 49.016% (3984/8128)\n",
            "Loss: 1.376 | Acc: 49.060% (4019/8192)\n",
            "Loss: 1.375 | Acc: 49.055% (4050/8256)\n",
            "Loss: 1.377 | Acc: 49.014% (4078/8320)\n",
            "Loss: 1.377 | Acc: 49.010% (4109/8384)\n",
            "Loss: 1.376 | Acc: 49.029% (4142/8448)\n",
            "Loss: 1.379 | Acc: 48.990% (4170/8512)\n",
            "Loss: 1.379 | Acc: 48.997% (4202/8576)\n",
            "Loss: 1.380 | Acc: 48.970% (4231/8640)\n",
            "Loss: 1.381 | Acc: 48.966% (4262/8704)\n",
            "Loss: 1.381 | Acc: 48.962% (4293/8768)\n",
            "Loss: 1.381 | Acc: 49.004% (4328/8832)\n",
            "Loss: 1.380 | Acc: 49.045% (4363/8896)\n",
            "Loss: 1.379 | Acc: 49.074% (4397/8960)\n",
            "Loss: 1.379 | Acc: 49.080% (4429/9024)\n",
            "Loss: 1.379 | Acc: 49.054% (4458/9088)\n",
            "Loss: 1.379 | Acc: 49.071% (4491/9152)\n",
            "Loss: 1.378 | Acc: 49.121% (4527/9216)\n",
            "Loss: 1.378 | Acc: 49.127% (4559/9280)\n",
            "Loss: 1.378 | Acc: 49.133% (4591/9344)\n",
            "Loss: 1.378 | Acc: 49.150% (4624/9408)\n",
            "Loss: 1.380 | Acc: 49.124% (4653/9472)\n",
            "Loss: 1.379 | Acc: 49.140% (4686/9536)\n",
            "Loss: 1.378 | Acc: 49.188% (4722/9600)\n",
            "Loss: 1.379 | Acc: 49.183% (4753/9664)\n",
            "Loss: 1.379 | Acc: 49.219% (4788/9728)\n",
            "Loss: 1.380 | Acc: 49.173% (4815/9792)\n",
            "Loss: 1.381 | Acc: 49.158% (4845/9856)\n",
            "Loss: 1.382 | Acc: 49.052% (4866/9920)\n",
            "Loss: 1.383 | Acc: 49.008% (4893/9984)\n",
            "Loss: 1.384 | Acc: 49.010% (4901/10000)\n",
            "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 49.01\n",
            "\n",
            "Epoch: 2\n",
            "Loss: 1.358 | Acc: 53.125% (34/64)\n",
            "Loss: 1.280 | Acc: 54.688% (70/128)\n",
            "Loss: 1.277 | Acc: 54.688% (105/192)\n",
            "Loss: 1.321 | Acc: 51.953% (133/256)\n",
            "Loss: 1.345 | Acc: 50.312% (161/320)\n",
            "Loss: 1.322 | Acc: 52.083% (200/384)\n",
            "Loss: 1.341 | Acc: 52.232% (234/448)\n",
            "Loss: 1.343 | Acc: 51.953% (266/512)\n",
            "Loss: 1.319 | Acc: 52.604% (303/576)\n",
            "Loss: 1.308 | Acc: 52.812% (338/640)\n",
            "Loss: 1.301 | Acc: 52.841% (372/704)\n",
            "Loss: 1.300 | Acc: 53.125% (408/768)\n",
            "Loss: 1.297 | Acc: 53.486% (445/832)\n",
            "Loss: 1.319 | Acc: 52.121% (467/896)\n",
            "Loss: 1.302 | Acc: 53.125% (510/960)\n",
            "Loss: 1.304 | Acc: 53.516% (548/1024)\n",
            "Loss: 1.289 | Acc: 54.044% (588/1088)\n",
            "Loss: 1.281 | Acc: 54.340% (626/1152)\n",
            "Loss: 1.300 | Acc: 54.030% (657/1216)\n",
            "Loss: 1.313 | Acc: 53.438% (684/1280)\n",
            "Loss: 1.301 | Acc: 53.943% (725/1344)\n",
            "Loss: 1.287 | Acc: 54.403% (766/1408)\n",
            "Loss: 1.290 | Acc: 54.416% (801/1472)\n",
            "Loss: 1.289 | Acc: 54.427% (836/1536)\n",
            "Loss: 1.302 | Acc: 54.125% (866/1600)\n",
            "Loss: 1.304 | Acc: 54.087% (900/1664)\n",
            "Loss: 1.295 | Acc: 54.225% (937/1728)\n",
            "Loss: 1.305 | Acc: 53.850% (965/1792)\n",
            "Loss: 1.301 | Acc: 54.149% (1005/1856)\n",
            "Loss: 1.302 | Acc: 53.958% (1036/1920)\n",
            "Loss: 1.305 | Acc: 53.629% (1064/1984)\n",
            "Loss: 1.309 | Acc: 53.369% (1093/2048)\n",
            "Loss: 1.308 | Acc: 53.125% (1122/2112)\n",
            "Loss: 1.314 | Acc: 52.803% (1149/2176)\n",
            "Loss: 1.312 | Acc: 52.902% (1185/2240)\n",
            "Loss: 1.315 | Acc: 52.865% (1218/2304)\n",
            "Loss: 1.308 | Acc: 52.914% (1253/2368)\n",
            "Loss: 1.313 | Acc: 52.796% (1284/2432)\n",
            "Loss: 1.312 | Acc: 52.724% (1316/2496)\n",
            "Loss: 1.314 | Acc: 52.695% (1349/2560)\n",
            "Loss: 1.316 | Acc: 52.706% (1383/2624)\n",
            "Loss: 1.316 | Acc: 52.716% (1417/2688)\n",
            "Loss: 1.316 | Acc: 52.725% (1451/2752)\n",
            "Loss: 1.316 | Acc: 52.770% (1486/2816)\n",
            "Loss: 1.321 | Acc: 52.604% (1515/2880)\n",
            "Loss: 1.327 | Acc: 52.344% (1541/2944)\n",
            "Loss: 1.327 | Acc: 52.327% (1574/3008)\n",
            "Loss: 1.330 | Acc: 52.214% (1604/3072)\n",
            "Loss: 1.328 | Acc: 52.360% (1642/3136)\n",
            "Loss: 1.324 | Acc: 52.531% (1681/3200)\n",
            "Loss: 1.323 | Acc: 52.512% (1714/3264)\n",
            "Loss: 1.330 | Acc: 52.254% (1739/3328)\n",
            "Loss: 1.333 | Acc: 52.182% (1770/3392)\n",
            "Loss: 1.329 | Acc: 52.373% (1810/3456)\n",
            "Loss: 1.328 | Acc: 52.386% (1844/3520)\n",
            "Loss: 1.329 | Acc: 52.260% (1873/3584)\n",
            "Loss: 1.330 | Acc: 52.193% (1904/3648)\n",
            "Loss: 1.332 | Acc: 52.128% (1935/3712)\n",
            "Loss: 1.330 | Acc: 52.172% (1970/3776)\n",
            "Loss: 1.330 | Acc: 52.240% (2006/3840)\n",
            "Loss: 1.331 | Acc: 52.177% (2037/3904)\n",
            "Loss: 1.330 | Acc: 52.344% (2077/3968)\n",
            "Loss: 1.332 | Acc: 52.158% (2103/4032)\n",
            "Loss: 1.333 | Acc: 52.100% (2134/4096)\n",
            "Loss: 1.335 | Acc: 51.995% (2163/4160)\n",
            "Loss: 1.337 | Acc: 51.918% (2193/4224)\n",
            "Loss: 1.340 | Acc: 51.866% (2224/4288)\n",
            "Loss: 1.342 | Acc: 51.815% (2255/4352)\n",
            "Loss: 1.338 | Acc: 51.947% (2294/4416)\n",
            "Loss: 1.335 | Acc: 51.964% (2328/4480)\n",
            "Loss: 1.336 | Acc: 51.695% (2349/4544)\n",
            "Loss: 1.340 | Acc: 51.671% (2381/4608)\n",
            "Loss: 1.341 | Acc: 51.627% (2412/4672)\n",
            "Loss: 1.338 | Acc: 51.689% (2448/4736)\n",
            "Loss: 1.340 | Acc: 51.688% (2481/4800)\n",
            "Loss: 1.343 | Acc: 51.604% (2510/4864)\n",
            "Loss: 1.344 | Acc: 51.664% (2546/4928)\n",
            "Loss: 1.342 | Acc: 51.723% (2582/4992)\n",
            "Loss: 1.341 | Acc: 51.760% (2617/5056)\n",
            "Loss: 1.342 | Acc: 51.797% (2652/5120)\n",
            "Loss: 1.341 | Acc: 51.794% (2685/5184)\n",
            "Loss: 1.339 | Acc: 51.848% (2721/5248)\n",
            "Loss: 1.341 | Acc: 51.770% (2750/5312)\n",
            "Loss: 1.339 | Acc: 51.767% (2783/5376)\n",
            "Loss: 1.338 | Acc: 51.746% (2815/5440)\n",
            "Loss: 1.339 | Acc: 51.781% (2850/5504)\n",
            "Loss: 1.341 | Acc: 51.706% (2879/5568)\n",
            "Loss: 1.342 | Acc: 51.634% (2908/5632)\n",
            "Loss: 1.341 | Acc: 51.685% (2944/5696)\n",
            "Loss: 1.342 | Acc: 51.701% (2978/5760)\n",
            "Loss: 1.344 | Acc: 51.597% (3005/5824)\n",
            "Loss: 1.343 | Acc: 51.613% (3039/5888)\n",
            "Loss: 1.342 | Acc: 51.697% (3077/5952)\n",
            "Loss: 1.341 | Acc: 51.712% (3111/6016)\n",
            "Loss: 1.343 | Acc: 51.595% (3137/6080)\n",
            "Loss: 1.344 | Acc: 51.514% (3165/6144)\n",
            "Loss: 1.343 | Acc: 51.482% (3196/6208)\n",
            "Loss: 1.342 | Acc: 51.483% (3229/6272)\n",
            "Loss: 1.341 | Acc: 51.515% (3264/6336)\n",
            "Loss: 1.341 | Acc: 51.469% (3294/6400)\n",
            "Loss: 1.339 | Acc: 51.470% (3327/6464)\n",
            "Loss: 1.339 | Acc: 51.455% (3359/6528)\n",
            "Loss: 1.340 | Acc: 51.426% (3390/6592)\n",
            "Loss: 1.336 | Acc: 51.502% (3428/6656)\n",
            "Loss: 1.338 | Acc: 51.473% (3459/6720)\n",
            "Loss: 1.339 | Acc: 51.445% (3490/6784)\n",
            "Loss: 1.343 | Acc: 51.285% (3512/6848)\n",
            "Loss: 1.344 | Acc: 51.273% (3544/6912)\n",
            "Loss: 1.346 | Acc: 51.233% (3574/6976)\n",
            "Loss: 1.348 | Acc: 51.193% (3604/7040)\n",
            "Loss: 1.347 | Acc: 51.154% (3634/7104)\n",
            "Loss: 1.346 | Acc: 51.200% (3670/7168)\n",
            "Loss: 1.344 | Acc: 51.258% (3707/7232)\n",
            "Loss: 1.344 | Acc: 51.220% (3737/7296)\n",
            "Loss: 1.343 | Acc: 51.196% (3768/7360)\n",
            "Loss: 1.342 | Acc: 51.307% (3809/7424)\n",
            "Loss: 1.342 | Acc: 51.335% (3844/7488)\n",
            "Loss: 1.344 | Acc: 51.178% (3865/7552)\n",
            "Loss: 1.345 | Acc: 51.182% (3898/7616)\n",
            "Loss: 1.344 | Acc: 51.211% (3933/7680)\n",
            "Loss: 1.343 | Acc: 51.149% (3961/7744)\n",
            "Loss: 1.344 | Acc: 51.089% (3989/7808)\n",
            "Loss: 1.344 | Acc: 51.054% (4019/7872)\n",
            "Loss: 1.346 | Acc: 51.021% (4049/7936)\n",
            "Loss: 1.344 | Acc: 51.100% (4088/8000)\n",
            "Loss: 1.345 | Acc: 51.017% (4114/8064)\n",
            "Loss: 1.345 | Acc: 51.009% (4146/8128)\n",
            "Loss: 1.346 | Acc: 50.989% (4177/8192)\n",
            "Loss: 1.346 | Acc: 51.005% (4211/8256)\n",
            "Loss: 1.346 | Acc: 51.022% (4245/8320)\n",
            "Loss: 1.345 | Acc: 51.038% (4279/8384)\n",
            "Loss: 1.345 | Acc: 51.006% (4309/8448)\n",
            "Loss: 1.344 | Acc: 51.046% (4345/8512)\n",
            "Loss: 1.346 | Acc: 51.003% (4374/8576)\n",
            "Loss: 1.347 | Acc: 50.995% (4406/8640)\n",
            "Loss: 1.347 | Acc: 51.045% (4443/8704)\n",
            "Loss: 1.346 | Acc: 51.106% (4481/8768)\n",
            "Loss: 1.347 | Acc: 51.076% (4511/8832)\n",
            "Loss: 1.345 | Acc: 51.180% (4553/8896)\n",
            "Loss: 1.346 | Acc: 51.183% (4586/8960)\n",
            "Loss: 1.347 | Acc: 51.175% (4618/9024)\n",
            "Loss: 1.349 | Acc: 51.089% (4643/9088)\n",
            "Loss: 1.349 | Acc: 51.147% (4681/9152)\n",
            "Loss: 1.350 | Acc: 51.161% (4715/9216)\n",
            "Loss: 1.349 | Acc: 51.207% (4752/9280)\n",
            "Loss: 1.348 | Acc: 51.241% (4788/9344)\n",
            "Loss: 1.348 | Acc: 51.222% (4819/9408)\n",
            "Loss: 1.348 | Acc: 51.182% (4848/9472)\n",
            "Loss: 1.347 | Acc: 51.195% (4882/9536)\n",
            "Loss: 1.348 | Acc: 51.177% (4913/9600)\n",
            "Loss: 1.348 | Acc: 51.211% (4949/9664)\n",
            "Loss: 1.348 | Acc: 51.234% (4984/9728)\n",
            "Loss: 1.347 | Acc: 51.277% (5021/9792)\n",
            "Loss: 1.346 | Acc: 51.299% (5056/9856)\n",
            "Loss: 1.347 | Acc: 51.300% (5089/9920)\n",
            "Loss: 1.346 | Acc: 51.282% (5120/9984)\n",
            "Loss: 1.347 | Acc: 51.244% (5149/10048)\n",
            "Loss: 1.348 | Acc: 51.216% (5179/10112)\n",
            "Loss: 1.349 | Acc: 51.199% (5210/10176)\n",
            "Loss: 1.349 | Acc: 51.182% (5241/10240)\n",
            "Loss: 1.351 | Acc: 51.135% (5269/10304)\n",
            "Loss: 1.350 | Acc: 51.157% (5304/10368)\n",
            "Loss: 1.350 | Acc: 51.112% (5332/10432)\n",
            "Loss: 1.350 | Acc: 51.096% (5363/10496)\n",
            "Loss: 1.349 | Acc: 51.080% (5394/10560)\n",
            "Loss: 1.348 | Acc: 51.101% (5429/10624)\n",
            "Loss: 1.347 | Acc: 51.132% (5465/10688)\n",
            "Loss: 1.348 | Acc: 51.079% (5492/10752)\n",
            "Loss: 1.347 | Acc: 51.100% (5527/10816)\n",
            "Loss: 1.347 | Acc: 51.131% (5563/10880)\n",
            "Loss: 1.347 | Acc: 51.142% (5597/10944)\n",
            "Loss: 1.347 | Acc: 51.117% (5627/11008)\n",
            "Loss: 1.348 | Acc: 51.147% (5663/11072)\n",
            "Loss: 1.347 | Acc: 51.158% (5697/11136)\n",
            "Loss: 1.347 | Acc: 51.214% (5736/11200)\n",
            "Loss: 1.347 | Acc: 51.172% (5764/11264)\n",
            "Loss: 1.346 | Acc: 51.201% (5800/11328)\n",
            "Loss: 1.344 | Acc: 51.273% (5841/11392)\n",
            "Loss: 1.344 | Acc: 51.283% (5875/11456)\n",
            "Loss: 1.344 | Acc: 51.311% (5911/11520)\n",
            "Loss: 1.343 | Acc: 51.312% (5944/11584)\n",
            "Loss: 1.344 | Acc: 51.314% (5977/11648)\n",
            "Loss: 1.344 | Acc: 51.315% (6010/11712)\n",
            "Loss: 1.345 | Acc: 51.350% (6047/11776)\n",
            "Loss: 1.345 | Acc: 51.377% (6083/11840)\n",
            "Loss: 1.345 | Acc: 51.386% (6117/11904)\n",
            "Loss: 1.345 | Acc: 51.345% (6145/11968)\n",
            "Loss: 1.346 | Acc: 51.346% (6178/12032)\n",
            "Loss: 1.346 | Acc: 51.364% (6213/12096)\n",
            "Loss: 1.344 | Acc: 51.398% (6250/12160)\n",
            "Loss: 1.344 | Acc: 51.374% (6280/12224)\n",
            "Loss: 1.344 | Acc: 51.335% (6308/12288)\n",
            "Loss: 1.345 | Acc: 51.303% (6337/12352)\n",
            "Loss: 1.344 | Acc: 51.345% (6375/12416)\n",
            "Loss: 1.343 | Acc: 51.314% (6404/12480)\n",
            "Loss: 1.342 | Acc: 51.331% (6439/12544)\n",
            "Loss: 1.343 | Acc: 51.309% (6469/12608)\n",
            "Loss: 1.344 | Acc: 51.278% (6498/12672)\n",
            "Loss: 1.343 | Acc: 51.280% (6531/12736)\n",
            "Loss: 1.343 | Acc: 51.250% (6560/12800)\n",
            "Loss: 1.344 | Acc: 51.252% (6593/12864)\n",
            "Loss: 1.345 | Acc: 51.245% (6625/12928)\n",
            "Loss: 1.344 | Acc: 51.247% (6658/12992)\n",
            "Loss: 1.344 | Acc: 51.248% (6691/13056)\n",
            "Loss: 1.344 | Acc: 51.220% (6720/13120)\n",
            "Loss: 1.344 | Acc: 51.229% (6754/13184)\n",
            "Loss: 1.344 | Acc: 51.253% (6790/13248)\n",
            "Loss: 1.345 | Acc: 51.224% (6819/13312)\n",
            "Loss: 1.345 | Acc: 51.219% (6851/13376)\n",
            "Loss: 1.345 | Acc: 51.205% (6882/13440)\n",
            "Loss: 1.345 | Acc: 51.207% (6915/13504)\n",
            "Loss: 1.344 | Acc: 51.209% (6948/13568)\n",
            "Loss: 1.343 | Acc: 51.240% (6985/13632)\n",
            "Loss: 1.343 | Acc: 51.241% (7018/13696)\n",
            "Loss: 1.343 | Acc: 51.272% (7055/13760)\n",
            "Loss: 1.343 | Acc: 51.244% (7084/13824)\n",
            "Loss: 1.343 | Acc: 51.231% (7115/13888)\n",
            "Loss: 1.342 | Acc: 51.233% (7148/13952)\n",
            "Loss: 1.343 | Acc: 51.177% (7173/14016)\n",
            "Loss: 1.344 | Acc: 51.143% (7201/14080)\n",
            "Loss: 1.343 | Acc: 51.160% (7236/14144)\n",
            "Loss: 1.343 | Acc: 51.147% (7267/14208)\n",
            "Loss: 1.341 | Acc: 51.191% (7306/14272)\n",
            "Loss: 1.342 | Acc: 51.172% (7336/14336)\n",
            "Loss: 1.341 | Acc: 51.194% (7372/14400)\n",
            "Loss: 1.341 | Acc: 51.224% (7409/14464)\n",
            "Loss: 1.342 | Acc: 51.177% (7435/14528)\n",
            "Loss: 1.343 | Acc: 51.124% (7460/14592)\n",
            "Loss: 1.342 | Acc: 51.139% (7495/14656)\n",
            "Loss: 1.341 | Acc: 51.148% (7529/14720)\n",
            "Loss: 1.341 | Acc: 51.157% (7563/14784)\n",
            "Loss: 1.341 | Acc: 51.158% (7596/14848)\n",
            "Loss: 1.343 | Acc: 51.133% (7625/14912)\n",
            "Loss: 1.343 | Acc: 51.135% (7658/14976)\n",
            "Loss: 1.342 | Acc: 51.157% (7694/15040)\n",
            "Loss: 1.342 | Acc: 51.112% (7720/15104)\n",
            "Loss: 1.342 | Acc: 51.147% (7758/15168)\n",
            "Loss: 1.342 | Acc: 51.136% (7789/15232)\n",
            "Loss: 1.342 | Acc: 51.092% (7815/15296)\n",
            "Loss: 1.341 | Acc: 51.113% (7851/15360)\n",
            "Loss: 1.341 | Acc: 51.122% (7885/15424)\n",
            "Loss: 1.342 | Acc: 51.111% (7916/15488)\n",
            "Loss: 1.343 | Acc: 51.087% (7945/15552)\n",
            "Loss: 1.343 | Acc: 51.082% (7977/15616)\n",
            "Loss: 1.343 | Acc: 51.040% (8003/15680)\n",
            "Loss: 1.343 | Acc: 51.042% (8036/15744)\n",
            "Loss: 1.343 | Acc: 51.037% (8068/15808)\n",
            "Loss: 1.344 | Acc: 51.033% (8100/15872)\n",
            "Loss: 1.342 | Acc: 51.060% (8137/15936)\n",
            "Loss: 1.342 | Acc: 51.050% (8168/16000)\n",
            "Loss: 1.343 | Acc: 51.008% (8194/16064)\n",
            "Loss: 1.343 | Acc: 51.011% (8227/16128)\n",
            "Loss: 1.343 | Acc: 51.019% (8261/16192)\n",
            "Loss: 1.343 | Acc: 51.046% (8298/16256)\n",
            "Loss: 1.343 | Acc: 51.066% (8334/16320)\n",
            "Loss: 1.343 | Acc: 51.068% (8367/16384)\n",
            "Loss: 1.342 | Acc: 51.094% (8404/16448)\n",
            "Loss: 1.343 | Acc: 51.084% (8435/16512)\n",
            "Loss: 1.343 | Acc: 51.068% (8465/16576)\n",
            "Loss: 1.342 | Acc: 51.082% (8500/16640)\n",
            "Loss: 1.342 | Acc: 51.119% (8539/16704)\n",
            "Loss: 1.341 | Acc: 51.121% (8572/16768)\n",
            "Loss: 1.341 | Acc: 51.117% (8604/16832)\n",
            "Loss: 1.341 | Acc: 51.130% (8639/16896)\n",
            "Loss: 1.341 | Acc: 51.132% (8672/16960)\n",
            "Loss: 1.340 | Acc: 51.157% (8709/17024)\n",
            "Loss: 1.341 | Acc: 51.129% (8737/17088)\n",
            "Loss: 1.341 | Acc: 51.102% (8765/17152)\n",
            "Loss: 1.341 | Acc: 51.086% (8795/17216)\n",
            "Loss: 1.340 | Acc: 51.071% (8825/17280)\n",
            "Loss: 1.341 | Acc: 51.044% (8853/17344)\n",
            "Loss: 1.341 | Acc: 51.057% (8888/17408)\n",
            "Loss: 1.342 | Acc: 51.013% (8913/17472)\n",
            "Loss: 1.343 | Acc: 50.975% (8939/17536)\n",
            "Loss: 1.341 | Acc: 50.989% (8974/17600)\n",
            "Loss: 1.343 | Acc: 50.945% (8999/17664)\n",
            "Loss: 1.342 | Acc: 50.936% (9030/17728)\n",
            "Loss: 1.343 | Acc: 50.944% (9064/17792)\n",
            "Loss: 1.343 | Acc: 50.958% (9099/17856)\n",
            "Loss: 1.343 | Acc: 50.965% (9133/17920)\n",
            "Loss: 1.343 | Acc: 50.962% (9165/17984)\n",
            "Loss: 1.344 | Acc: 50.931% (9192/18048)\n",
            "Loss: 1.344 | Acc: 50.955% (9229/18112)\n",
            "Loss: 1.344 | Acc: 50.952% (9261/18176)\n",
            "Loss: 1.344 | Acc: 50.965% (9296/18240)\n",
            "Loss: 1.344 | Acc: 50.989% (9333/18304)\n",
            "Loss: 1.344 | Acc: 50.969% (9362/18368)\n",
            "Loss: 1.344 | Acc: 50.971% (9395/18432)\n",
            "Loss: 1.343 | Acc: 51.022% (9437/18496)\n",
            "Loss: 1.343 | Acc: 51.013% (9468/18560)\n",
            "Loss: 1.343 | Acc: 51.015% (9501/18624)\n",
            "Loss: 1.343 | Acc: 51.006% (9532/18688)\n",
            "Loss: 1.343 | Acc: 51.024% (9568/18752)\n",
            "Loss: 1.343 | Acc: 51.031% (9602/18816)\n",
            "Loss: 1.342 | Acc: 51.033% (9635/18880)\n",
            "Loss: 1.343 | Acc: 51.014% (9664/18944)\n",
            "Loss: 1.343 | Acc: 50.984% (9691/19008)\n",
            "Loss: 1.344 | Acc: 50.986% (9724/19072)\n",
            "Loss: 1.343 | Acc: 50.988% (9757/19136)\n",
            "Loss: 1.343 | Acc: 51.021% (9796/19200)\n",
            "Loss: 1.343 | Acc: 50.991% (9823/19264)\n",
            "Loss: 1.342 | Acc: 51.040% (9865/19328)\n",
            "Loss: 1.342 | Acc: 51.042% (9898/19392)\n",
            "Loss: 1.342 | Acc: 51.059% (9934/19456)\n",
            "Loss: 1.342 | Acc: 51.050% (9965/19520)\n",
            "Loss: 1.342 | Acc: 51.057% (9999/19584)\n",
            "Loss: 1.342 | Acc: 51.048% (10030/19648)\n",
            "Loss: 1.343 | Acc: 51.020% (10057/19712)\n",
            "Loss: 1.343 | Acc: 50.971% (10080/19776)\n",
            "Loss: 1.343 | Acc: 50.998% (10118/19840)\n",
            "Loss: 1.343 | Acc: 50.985% (10148/19904)\n",
            "Loss: 1.344 | Acc: 51.007% (10185/19968)\n",
            "Loss: 1.344 | Acc: 51.048% (10226/20032)\n",
            "Loss: 1.344 | Acc: 51.020% (10253/20096)\n",
            "Loss: 1.344 | Acc: 51.017% (10285/20160)\n",
            "Loss: 1.345 | Acc: 50.999% (10314/20224)\n",
            "Loss: 1.345 | Acc: 51.010% (10349/20288)\n",
            "Loss: 1.345 | Acc: 51.012% (10382/20352)\n",
            "Loss: 1.345 | Acc: 51.038% (10420/20416)\n",
            "Loss: 1.346 | Acc: 51.021% (10449/20480)\n",
            "Loss: 1.346 | Acc: 51.017% (10481/20544)\n",
            "Loss: 1.346 | Acc: 51.009% (10512/20608)\n",
            "Loss: 1.346 | Acc: 50.987% (10540/20672)\n",
            "Loss: 1.345 | Acc: 51.022% (10580/20736)\n",
            "Loss: 1.345 | Acc: 51.024% (10613/20800)\n",
            "Loss: 1.345 | Acc: 51.030% (10647/20864)\n",
            "Loss: 1.345 | Acc: 51.023% (10678/20928)\n",
            "Loss: 1.345 | Acc: 51.000% (10706/20992)\n",
            "Loss: 1.346 | Acc: 50.969% (10732/21056)\n",
            "Loss: 1.346 | Acc: 50.961% (10763/21120)\n",
            "Loss: 1.346 | Acc: 50.944% (10792/21184)\n",
            "Loss: 1.347 | Acc: 50.904% (10816/21248)\n",
            "Loss: 1.347 | Acc: 50.896% (10847/21312)\n",
            "Loss: 1.347 | Acc: 50.908% (10882/21376)\n",
            "Loss: 1.347 | Acc: 50.882% (10909/21440)\n",
            "Loss: 1.347 | Acc: 50.893% (10944/21504)\n",
            "Loss: 1.347 | Acc: 50.876% (10973/21568)\n",
            "Loss: 1.348 | Acc: 50.864% (11003/21632)\n",
            "Loss: 1.347 | Acc: 50.862% (11035/21696)\n",
            "Loss: 1.348 | Acc: 50.855% (11066/21760)\n",
            "Loss: 1.347 | Acc: 50.880% (11104/21824)\n",
            "Loss: 1.348 | Acc: 50.841% (11128/21888)\n",
            "Loss: 1.348 | Acc: 50.811% (11154/21952)\n",
            "Loss: 1.348 | Acc: 50.822% (11189/22016)\n",
            "Loss: 1.349 | Acc: 50.811% (11219/22080)\n",
            "Loss: 1.348 | Acc: 50.831% (11256/22144)\n",
            "Loss: 1.348 | Acc: 50.820% (11286/22208)\n",
            "Loss: 1.348 | Acc: 50.826% (11320/22272)\n",
            "Loss: 1.348 | Acc: 50.842% (11356/22336)\n",
            "Loss: 1.348 | Acc: 50.821% (11384/22400)\n",
            "Loss: 1.349 | Acc: 50.815% (11415/22464)\n",
            "Loss: 1.348 | Acc: 50.843% (11454/22528)\n",
            "Loss: 1.349 | Acc: 50.841% (11486/22592)\n",
            "Loss: 1.349 | Acc: 50.839% (11518/22656)\n",
            "Loss: 1.348 | Acc: 50.876% (11559/22720)\n",
            "Loss: 1.348 | Acc: 50.865% (11589/22784)\n",
            "Loss: 1.348 | Acc: 50.884% (11626/22848)\n",
            "Loss: 1.349 | Acc: 50.855% (11652/22912)\n",
            "Loss: 1.349 | Acc: 50.862% (11686/22976)\n",
            "Loss: 1.349 | Acc: 50.846% (11715/23040)\n",
            "Loss: 1.349 | Acc: 50.857% (11750/23104)\n",
            "Loss: 1.348 | Acc: 50.859% (11783/23168)\n",
            "Loss: 1.349 | Acc: 50.852% (11814/23232)\n",
            "Loss: 1.349 | Acc: 50.841% (11844/23296)\n",
            "Loss: 1.349 | Acc: 50.848% (11878/23360)\n",
            "Loss: 1.349 | Acc: 50.837% (11908/23424)\n",
            "Loss: 1.350 | Acc: 50.822% (11937/23488)\n",
            "Loss: 1.349 | Acc: 50.862% (11979/23552)\n",
            "Loss: 1.349 | Acc: 50.847% (12008/23616)\n",
            "Loss: 1.349 | Acc: 50.832% (12037/23680)\n",
            "Loss: 1.349 | Acc: 50.830% (12069/23744)\n",
            "Loss: 1.349 | Acc: 50.836% (12103/23808)\n",
            "Loss: 1.350 | Acc: 50.842% (12137/23872)\n",
            "Loss: 1.350 | Acc: 50.831% (12167/23936)\n",
            "Loss: 1.349 | Acc: 50.858% (12206/24000)\n",
            "Loss: 1.349 | Acc: 50.864% (12240/24064)\n",
            "Loss: 1.349 | Acc: 50.866% (12273/24128)\n",
            "Loss: 1.349 | Acc: 50.864% (12305/24192)\n",
            "Loss: 1.349 | Acc: 50.858% (12336/24256)\n",
            "Loss: 1.349 | Acc: 50.855% (12368/24320)\n",
            "Loss: 1.349 | Acc: 50.861% (12402/24384)\n",
            "Loss: 1.349 | Acc: 50.855% (12433/24448)\n",
            "Loss: 1.349 | Acc: 50.853% (12465/24512)\n",
            "Loss: 1.349 | Acc: 50.883% (12505/24576)\n",
            "Loss: 1.348 | Acc: 50.893% (12540/24640)\n",
            "Loss: 1.348 | Acc: 50.923% (12580/24704)\n",
            "Loss: 1.348 | Acc: 50.929% (12614/24768)\n",
            "Loss: 1.348 | Acc: 50.922% (12645/24832)\n",
            "Loss: 1.349 | Acc: 50.928% (12679/24896)\n",
            "Loss: 1.349 | Acc: 50.921% (12710/24960)\n",
            "Loss: 1.349 | Acc: 50.915% (12741/25024)\n",
            "Loss: 1.348 | Acc: 50.925% (12776/25088)\n",
            "Loss: 1.349 | Acc: 50.910% (12805/25152)\n",
            "Loss: 1.349 | Acc: 50.904% (12836/25216)\n",
            "Loss: 1.349 | Acc: 50.918% (12872/25280)\n",
            "Loss: 1.349 | Acc: 50.896% (12899/25344)\n",
            "Loss: 1.349 | Acc: 50.905% (12934/25408)\n",
            "Loss: 1.349 | Acc: 50.891% (12963/25472)\n",
            "Loss: 1.350 | Acc: 50.893% (12996/25536)\n",
            "Loss: 1.350 | Acc: 50.887% (13027/25600)\n",
            "Loss: 1.349 | Acc: 50.931% (13071/25664)\n",
            "Loss: 1.349 | Acc: 50.913% (13099/25728)\n",
            "Loss: 1.349 | Acc: 50.919% (13133/25792)\n",
            "Loss: 1.349 | Acc: 50.936% (13170/25856)\n",
            "Loss: 1.349 | Acc: 50.945% (13205/25920)\n",
            "Loss: 1.349 | Acc: 50.939% (13236/25984)\n",
            "Loss: 1.349 | Acc: 50.937% (13268/26048)\n",
            "Loss: 1.349 | Acc: 50.946% (13303/26112)\n",
            "Loss: 1.349 | Acc: 50.936% (13333/26176)\n",
            "Loss: 1.349 | Acc: 50.938% (13366/26240)\n",
            "Loss: 1.349 | Acc: 50.928% (13396/26304)\n",
            "Loss: 1.349 | Acc: 50.925% (13428/26368)\n",
            "Loss: 1.348 | Acc: 50.931% (13462/26432)\n",
            "Loss: 1.348 | Acc: 50.940% (13497/26496)\n",
            "Loss: 1.348 | Acc: 50.930% (13527/26560)\n",
            "Loss: 1.348 | Acc: 50.954% (13566/26624)\n",
            "Loss: 1.347 | Acc: 50.974% (13604/26688)\n",
            "Loss: 1.347 | Acc: 50.983% (13639/26752)\n",
            "Loss: 1.347 | Acc: 50.981% (13671/26816)\n",
            "Loss: 1.347 | Acc: 50.978% (13703/26880)\n",
            "Loss: 1.347 | Acc: 50.976% (13735/26944)\n",
            "Loss: 1.347 | Acc: 50.985% (13770/27008)\n",
            "Loss: 1.347 | Acc: 50.953% (13794/27072)\n",
            "Loss: 1.348 | Acc: 50.936% (13822/27136)\n",
            "Loss: 1.348 | Acc: 50.934% (13854/27200)\n",
            "Loss: 1.348 | Acc: 50.935% (13887/27264)\n",
            "Loss: 1.347 | Acc: 50.962% (13927/27328)\n",
            "Loss: 1.348 | Acc: 50.971% (13962/27392)\n",
            "Loss: 1.348 | Acc: 50.969% (13994/27456)\n",
            "Loss: 1.347 | Acc: 50.999% (14035/27520)\n",
            "Loss: 1.348 | Acc: 50.990% (14065/27584)\n",
            "Loss: 1.348 | Acc: 50.991% (14098/27648)\n",
            "Loss: 1.348 | Acc: 50.974% (14126/27712)\n",
            "Loss: 1.348 | Acc: 50.997% (14165/27776)\n",
            "Loss: 1.348 | Acc: 50.981% (14193/27840)\n",
            "Loss: 1.348 | Acc: 50.986% (14227/27904)\n",
            "Loss: 1.347 | Acc: 51.008% (14266/27968)\n",
            "Loss: 1.347 | Acc: 51.020% (14302/28032)\n",
            "Loss: 1.347 | Acc: 51.014% (14333/28096)\n",
            "Loss: 1.347 | Acc: 51.009% (14364/28160)\n",
            "Loss: 1.347 | Acc: 50.981% (14389/28224)\n",
            "Loss: 1.347 | Acc: 50.986% (14423/28288)\n",
            "Loss: 1.347 | Acc: 50.998% (14459/28352)\n",
            "Loss: 1.347 | Acc: 50.996% (14491/28416)\n",
            "Loss: 1.347 | Acc: 51.001% (14525/28480)\n",
            "Loss: 1.347 | Acc: 50.995% (14556/28544)\n",
            "Loss: 1.347 | Acc: 51.003% (14591/28608)\n",
            "Loss: 1.347 | Acc: 51.018% (14628/28672)\n",
            "Loss: 1.346 | Acc: 51.016% (14660/28736)\n",
            "Loss: 1.347 | Acc: 51.017% (14693/28800)\n",
            "Loss: 1.347 | Acc: 51.012% (14724/28864)\n",
            "Loss: 1.347 | Acc: 51.030% (14762/28928)\n",
            "Loss: 1.347 | Acc: 51.042% (14798/28992)\n",
            "Loss: 1.347 | Acc: 51.043% (14831/29056)\n",
            "Loss: 1.347 | Acc: 51.044% (14864/29120)\n",
            "Loss: 1.346 | Acc: 51.055% (14900/29184)\n",
            "Loss: 1.346 | Acc: 51.046% (14930/29248)\n",
            "Loss: 1.346 | Acc: 51.051% (14964/29312)\n",
            "Loss: 1.345 | Acc: 51.076% (15004/29376)\n",
            "Loss: 1.346 | Acc: 51.067% (15034/29440)\n",
            "Loss: 1.345 | Acc: 51.068% (15067/29504)\n",
            "Loss: 1.346 | Acc: 51.055% (15096/29568)\n",
            "Loss: 1.345 | Acc: 51.066% (15132/29632)\n",
            "Loss: 1.345 | Acc: 51.081% (15169/29696)\n",
            "Loss: 1.346 | Acc: 51.055% (15194/29760)\n",
            "Loss: 1.346 | Acc: 51.043% (15223/29824)\n",
            "Loss: 1.346 | Acc: 51.064% (15262/29888)\n",
            "Loss: 1.346 | Acc: 51.065% (15295/29952)\n",
            "Loss: 1.346 | Acc: 51.053% (15324/30016)\n",
            "Loss: 1.346 | Acc: 51.037% (15352/30080)\n",
            "Loss: 1.347 | Acc: 51.022% (15380/30144)\n",
            "Loss: 1.347 | Acc: 51.030% (15415/30208)\n",
            "Loss: 1.347 | Acc: 51.011% (15442/30272)\n",
            "Loss: 1.346 | Acc: 51.012% (15475/30336)\n",
            "Loss: 1.347 | Acc: 51.007% (15506/30400)\n",
            "Loss: 1.347 | Acc: 50.995% (15535/30464)\n",
            "Loss: 1.347 | Acc: 51.002% (15570/30528)\n",
            "Loss: 1.347 | Acc: 50.994% (15600/30592)\n",
            "Loss: 1.347 | Acc: 50.992% (15632/30656)\n",
            "Loss: 1.347 | Acc: 50.999% (15667/30720)\n",
            "Loss: 1.347 | Acc: 50.984% (15695/30784)\n",
            "Loss: 1.347 | Acc: 50.985% (15728/30848)\n",
            "Loss: 1.347 | Acc: 50.977% (15758/30912)\n",
            "Loss: 1.348 | Acc: 50.968% (15788/30976)\n",
            "Loss: 1.347 | Acc: 50.976% (15823/31040)\n",
            "Loss: 1.348 | Acc: 50.971% (15854/31104)\n",
            "Loss: 1.347 | Acc: 50.979% (15889/31168)\n",
            "Loss: 1.347 | Acc: 50.989% (15925/31232)\n",
            "Loss: 1.348 | Acc: 51.000% (15961/31296)\n",
            "Loss: 1.347 | Acc: 50.998% (15993/31360)\n",
            "Loss: 1.347 | Acc: 50.996% (16025/31424)\n",
            "Loss: 1.348 | Acc: 50.969% (16049/31488)\n",
            "Loss: 1.348 | Acc: 50.970% (16082/31552)\n",
            "Loss: 1.348 | Acc: 50.968% (16114/31616)\n",
            "Loss: 1.349 | Acc: 50.963% (16145/31680)\n",
            "Loss: 1.348 | Acc: 50.958% (16176/31744)\n",
            "Loss: 1.348 | Acc: 50.946% (16205/31808)\n",
            "Loss: 1.349 | Acc: 50.941% (16236/31872)\n",
            "Loss: 1.348 | Acc: 50.943% (16269/31936)\n",
            "Loss: 1.348 | Acc: 50.944% (16302/32000)\n",
            "Loss: 1.348 | Acc: 50.957% (16339/32064)\n",
            "Loss: 1.348 | Acc: 50.965% (16374/32128)\n",
            "Loss: 1.348 | Acc: 50.969% (16408/32192)\n",
            "Loss: 1.348 | Acc: 50.970% (16441/32256)\n",
            "Loss: 1.347 | Acc: 50.981% (16477/32320)\n",
            "Loss: 1.347 | Acc: 51.000% (16516/32384)\n",
            "Loss: 1.347 | Acc: 50.980% (16542/32448)\n",
            "Loss: 1.347 | Acc: 50.972% (16572/32512)\n",
            "Loss: 1.347 | Acc: 50.958% (16600/32576)\n",
            "Loss: 1.347 | Acc: 50.965% (16635/32640)\n",
            "Loss: 1.347 | Acc: 50.951% (16663/32704)\n",
            "Loss: 1.346 | Acc: 50.970% (16702/32768)\n",
            "Loss: 1.346 | Acc: 50.962% (16732/32832)\n",
            "Loss: 1.347 | Acc: 50.967% (16766/32896)\n",
            "Loss: 1.347 | Acc: 50.953% (16794/32960)\n",
            "Loss: 1.347 | Acc: 50.951% (16826/33024)\n",
            "Loss: 1.347 | Acc: 50.967% (16864/33088)\n",
            "Loss: 1.347 | Acc: 50.971% (16898/33152)\n",
            "Loss: 1.346 | Acc: 50.975% (16932/33216)\n",
            "Loss: 1.347 | Acc: 50.980% (16966/33280)\n",
            "Loss: 1.347 | Acc: 50.975% (16997/33344)\n",
            "Loss: 1.347 | Acc: 50.976% (17030/33408)\n",
            "Loss: 1.347 | Acc: 50.959% (17057/33472)\n",
            "Loss: 1.347 | Acc: 50.951% (17087/33536)\n",
            "Loss: 1.347 | Acc: 50.958% (17122/33600)\n",
            "Loss: 1.347 | Acc: 50.971% (17159/33664)\n",
            "Loss: 1.347 | Acc: 50.967% (17190/33728)\n",
            "Loss: 1.347 | Acc: 50.977% (17226/33792)\n",
            "Loss: 1.346 | Acc: 50.981% (17260/33856)\n",
            "Loss: 1.347 | Acc: 50.982% (17293/33920)\n",
            "Loss: 1.347 | Acc: 50.986% (17327/33984)\n",
            "Loss: 1.346 | Acc: 50.981% (17358/34048)\n",
            "Loss: 1.347 | Acc: 50.985% (17392/34112)\n",
            "Loss: 1.347 | Acc: 50.989% (17426/34176)\n",
            "Loss: 1.347 | Acc: 50.981% (17456/34240)\n",
            "Loss: 1.347 | Acc: 50.977% (17487/34304)\n",
            "Loss: 1.346 | Acc: 50.981% (17521/34368)\n",
            "Loss: 1.347 | Acc: 50.976% (17552/34432)\n",
            "Loss: 1.347 | Acc: 50.948% (17575/34496)\n",
            "Loss: 1.347 | Acc: 50.940% (17605/34560)\n",
            "Loss: 1.347 | Acc: 50.930% (17634/34624)\n",
            "Loss: 1.347 | Acc: 50.931% (17667/34688)\n",
            "Loss: 1.347 | Acc: 50.921% (17696/34752)\n",
            "Loss: 1.347 | Acc: 50.931% (17732/34816)\n",
            "Loss: 1.347 | Acc: 50.940% (17768/34880)\n",
            "Loss: 1.347 | Acc: 50.936% (17799/34944)\n",
            "Loss: 1.348 | Acc: 50.945% (17835/35008)\n",
            "Loss: 1.347 | Acc: 50.972% (17877/35072)\n",
            "Loss: 1.347 | Acc: 50.965% (17907/35136)\n",
            "Loss: 1.347 | Acc: 50.974% (17943/35200)\n",
            "Loss: 1.347 | Acc: 50.978% (17977/35264)\n",
            "Loss: 1.347 | Acc: 50.974% (18008/35328)\n",
            "Loss: 1.347 | Acc: 50.958% (18035/35392)\n",
            "Loss: 1.347 | Acc: 50.962% (18069/35456)\n",
            "Loss: 1.347 | Acc: 50.954% (18099/35520)\n",
            "Loss: 1.347 | Acc: 50.958% (18133/35584)\n",
            "Loss: 1.346 | Acc: 50.982% (18174/35648)\n",
            "Loss: 1.346 | Acc: 50.977% (18205/35712)\n",
            "Loss: 1.346 | Acc: 50.992% (18243/35776)\n",
            "Loss: 1.345 | Acc: 50.988% (18274/35840)\n",
            "Loss: 1.346 | Acc: 50.986% (18306/35904)\n",
            "Loss: 1.346 | Acc: 50.973% (18334/35968)\n",
            "Loss: 1.346 | Acc: 50.966% (18364/36032)\n",
            "Loss: 1.346 | Acc: 50.953% (18392/36096)\n",
            "Loss: 1.346 | Acc: 50.957% (18426/36160)\n",
            "Loss: 1.346 | Acc: 50.958% (18459/36224)\n",
            "Loss: 1.346 | Acc: 50.948% (18488/36288)\n",
            "Loss: 1.346 | Acc: 50.946% (18520/36352)\n",
            "Loss: 1.346 | Acc: 50.950% (18554/36416)\n",
            "Loss: 1.346 | Acc: 50.943% (18584/36480)\n",
            "Loss: 1.345 | Acc: 50.958% (18622/36544)\n",
            "Loss: 1.346 | Acc: 50.942% (18649/36608)\n",
            "Loss: 1.346 | Acc: 50.943% (18682/36672)\n",
            "Loss: 1.346 | Acc: 50.939% (18713/36736)\n",
            "Loss: 1.346 | Acc: 50.921% (18739/36800)\n",
            "Loss: 1.346 | Acc: 50.922% (18772/36864)\n",
            "Loss: 1.346 | Acc: 50.904% (18798/36928)\n",
            "Loss: 1.346 | Acc: 50.906% (18831/36992)\n",
            "Loss: 1.346 | Acc: 50.909% (18865/37056)\n",
            "Loss: 1.346 | Acc: 50.902% (18895/37120)\n",
            "Loss: 1.347 | Acc: 50.898% (18926/37184)\n",
            "Loss: 1.346 | Acc: 50.902% (18960/37248)\n",
            "Loss: 1.346 | Acc: 50.909% (18995/37312)\n",
            "Loss: 1.346 | Acc: 50.899% (19024/37376)\n",
            "Loss: 1.346 | Acc: 50.911% (19061/37440)\n",
            "Loss: 1.346 | Acc: 50.899% (19089/37504)\n",
            "Loss: 1.346 | Acc: 50.902% (19123/37568)\n",
            "Loss: 1.346 | Acc: 50.911% (19159/37632)\n",
            "Loss: 1.346 | Acc: 50.921% (19195/37696)\n",
            "Loss: 1.346 | Acc: 50.898% (19219/37760)\n",
            "Loss: 1.346 | Acc: 50.920% (19260/37824)\n",
            "Loss: 1.346 | Acc: 50.940% (19300/37888)\n",
            "Loss: 1.346 | Acc: 50.933% (19330/37952)\n",
            "Loss: 1.346 | Acc: 50.936% (19364/38016)\n",
            "Loss: 1.346 | Acc: 50.930% (19394/38080)\n",
            "Loss: 1.346 | Acc: 50.936% (19429/38144)\n",
            "Loss: 1.346 | Acc: 50.924% (19457/38208)\n",
            "Loss: 1.346 | Acc: 50.951% (19500/38272)\n",
            "Loss: 1.346 | Acc: 50.939% (19528/38336)\n",
            "Loss: 1.346 | Acc: 50.940% (19561/38400)\n",
            "Loss: 1.346 | Acc: 50.918% (19585/38464)\n",
            "Loss: 1.346 | Acc: 50.914% (19616/38528)\n",
            "Loss: 1.347 | Acc: 50.891% (19640/38592)\n",
            "Loss: 1.347 | Acc: 50.903% (19677/38656)\n",
            "Loss: 1.347 | Acc: 50.901% (19709/38720)\n",
            "Loss: 1.347 | Acc: 50.892% (19738/38784)\n",
            "Loss: 1.347 | Acc: 50.888% (19769/38848)\n",
            "Loss: 1.347 | Acc: 50.894% (19804/38912)\n",
            "Loss: 1.347 | Acc: 50.901% (19839/38976)\n",
            "Loss: 1.347 | Acc: 50.897% (19870/39040)\n",
            "Loss: 1.346 | Acc: 50.910% (19908/39104)\n",
            "Loss: 1.346 | Acc: 50.914% (19942/39168)\n",
            "Loss: 1.346 | Acc: 50.907% (19972/39232)\n",
            "Loss: 1.346 | Acc: 50.911% (20006/39296)\n",
            "Loss: 1.346 | Acc: 50.910% (20038/39360)\n",
            "Loss: 1.346 | Acc: 50.906% (20069/39424)\n",
            "Loss: 1.346 | Acc: 50.891% (20096/39488)\n",
            "Loss: 1.346 | Acc: 50.895% (20130/39552)\n",
            "Loss: 1.346 | Acc: 50.886% (20159/39616)\n",
            "Loss: 1.346 | Acc: 50.892% (20194/39680)\n",
            "Loss: 1.346 | Acc: 50.878% (20221/39744)\n",
            "Loss: 1.346 | Acc: 50.887% (20257/39808)\n",
            "Loss: 1.346 | Acc: 50.885% (20289/39872)\n",
            "Loss: 1.346 | Acc: 50.866% (20314/39936)\n",
            "Loss: 1.347 | Acc: 50.855% (20342/40000)\n",
            "Loss: 1.347 | Acc: 50.846% (20371/40064)\n",
            "Loss: 1.346 | Acc: 50.857% (20408/40128)\n",
            "Loss: 1.346 | Acc: 50.858% (20441/40192)\n",
            "Loss: 1.346 | Acc: 50.874% (20480/40256)\n",
            "Loss: 1.346 | Acc: 50.875% (20513/40320)\n",
            "Loss: 1.347 | Acc: 50.864% (20541/40384)\n",
            "Loss: 1.347 | Acc: 50.868% (20575/40448)\n",
            "Loss: 1.346 | Acc: 50.859% (20604/40512)\n",
            "Loss: 1.347 | Acc: 50.865% (20639/40576)\n",
            "Loss: 1.346 | Acc: 50.866% (20672/40640)\n",
            "Loss: 1.346 | Acc: 50.882% (20711/40704)\n",
            "Loss: 1.346 | Acc: 50.895% (20749/40768)\n",
            "Loss: 1.346 | Acc: 50.904% (20785/40832)\n",
            "Loss: 1.345 | Acc: 50.922% (20825/40896)\n",
            "Loss: 1.345 | Acc: 50.920% (20857/40960)\n",
            "Loss: 1.346 | Acc: 50.909% (20885/41024)\n",
            "Loss: 1.346 | Acc: 50.908% (20917/41088)\n",
            "Loss: 1.346 | Acc: 50.911% (20951/41152)\n",
            "Loss: 1.346 | Acc: 50.917% (20986/41216)\n",
            "Loss: 1.346 | Acc: 50.923% (21021/41280)\n",
            "Loss: 1.345 | Acc: 50.926% (21055/41344)\n",
            "Loss: 1.346 | Acc: 50.910% (21081/41408)\n",
            "Loss: 1.346 | Acc: 50.911% (21114/41472)\n",
            "Loss: 1.346 | Acc: 50.912% (21147/41536)\n",
            "Loss: 1.345 | Acc: 50.940% (21191/41600)\n",
            "Loss: 1.345 | Acc: 50.943% (21225/41664)\n",
            "Loss: 1.345 | Acc: 50.949% (21260/41728)\n",
            "Loss: 1.345 | Acc: 50.952% (21294/41792)\n",
            "Loss: 1.345 | Acc: 50.956% (21328/41856)\n",
            "Loss: 1.345 | Acc: 50.954% (21360/41920)\n",
            "Loss: 1.345 | Acc: 50.936% (21385/41984)\n",
            "Loss: 1.345 | Acc: 50.937% (21418/42048)\n",
            "Loss: 1.345 | Acc: 50.943% (21453/42112)\n",
            "Loss: 1.345 | Acc: 50.941% (21485/42176)\n",
            "Loss: 1.345 | Acc: 50.938% (21516/42240)\n",
            "Loss: 1.345 | Acc: 50.946% (21552/42304)\n",
            "Loss: 1.345 | Acc: 50.935% (21580/42368)\n",
            "Loss: 1.345 | Acc: 50.936% (21613/42432)\n",
            "Loss: 1.345 | Acc: 50.927% (21642/42496)\n",
            "Loss: 1.345 | Acc: 50.942% (21681/42560)\n",
            "Loss: 1.344 | Acc: 50.962% (21722/42624)\n",
            "Loss: 1.344 | Acc: 50.963% (21755/42688)\n",
            "Loss: 1.344 | Acc: 50.959% (21786/42752)\n",
            "Loss: 1.344 | Acc: 50.955% (21817/42816)\n",
            "Loss: 1.344 | Acc: 50.977% (21859/42880)\n",
            "Loss: 1.344 | Acc: 50.973% (21890/42944)\n",
            "Loss: 1.344 | Acc: 50.977% (21924/43008)\n",
            "Loss: 1.344 | Acc: 50.980% (21958/43072)\n",
            "Loss: 1.344 | Acc: 50.981% (21991/43136)\n",
            "Loss: 1.344 | Acc: 50.977% (22022/43200)\n",
            "Loss: 1.344 | Acc: 50.975% (22054/43264)\n",
            "Loss: 1.344 | Acc: 50.981% (22089/43328)\n",
            "Loss: 1.344 | Acc: 50.996% (22128/43392)\n",
            "Loss: 1.343 | Acc: 50.996% (22161/43456)\n",
            "Loss: 1.344 | Acc: 50.988% (22190/43520)\n",
            "Loss: 1.344 | Acc: 50.991% (22224/43584)\n",
            "Loss: 1.343 | Acc: 50.990% (22256/43648)\n",
            "Loss: 1.344 | Acc: 50.988% (22288/43712)\n",
            "Loss: 1.344 | Acc: 50.980% (22317/43776)\n",
            "Loss: 1.343 | Acc: 50.985% (22352/43840)\n",
            "Loss: 1.343 | Acc: 50.995% (22389/43904)\n",
            "Loss: 1.343 | Acc: 50.989% (22419/43968)\n",
            "Loss: 1.344 | Acc: 50.988% (22451/44032)\n",
            "Loss: 1.344 | Acc: 50.980% (22480/44096)\n",
            "Loss: 1.344 | Acc: 50.983% (22514/44160)\n",
            "Loss: 1.343 | Acc: 50.995% (22552/44224)\n",
            "Loss: 1.343 | Acc: 51.023% (22597/44288)\n",
            "Loss: 1.343 | Acc: 51.019% (22628/44352)\n",
            "Loss: 1.343 | Acc: 51.020% (22661/44416)\n",
            "Loss: 1.343 | Acc: 51.027% (22697/44480)\n",
            "Loss: 1.343 | Acc: 51.019% (22726/44544)\n",
            "Loss: 1.343 | Acc: 51.036% (22766/44608)\n",
            "Loss: 1.343 | Acc: 51.030% (22796/44672)\n",
            "Loss: 1.344 | Acc: 51.008% (22819/44736)\n",
            "Loss: 1.344 | Acc: 51.000% (22848/44800)\n",
            "Loss: 1.345 | Acc: 50.990% (22876/44864)\n",
            "Loss: 1.345 | Acc: 50.990% (22909/44928)\n",
            "Loss: 1.345 | Acc: 50.987% (22940/44992)\n",
            "Loss: 1.345 | Acc: 50.981% (22970/45056)\n",
            "Loss: 1.345 | Acc: 50.995% (23009/45120)\n",
            "Loss: 1.345 | Acc: 51.000% (23044/45184)\n",
            "Loss: 1.345 | Acc: 51.001% (23077/45248)\n",
            "Loss: 1.345 | Acc: 51.000% (23109/45312)\n",
            "Loss: 1.345 | Acc: 50.992% (23138/45376)\n",
            "Loss: 1.345 | Acc: 50.993% (23171/45440)\n",
            "Loss: 1.345 | Acc: 50.987% (23201/45504)\n",
            "Loss: 1.345 | Acc: 50.988% (23234/45568)\n",
            "Loss: 1.345 | Acc: 50.991% (23268/45632)\n",
            "Loss: 1.345 | Acc: 50.989% (23300/45696)\n",
            "Loss: 1.345 | Acc: 50.979% (23328/45760)\n",
            "Loss: 1.345 | Acc: 50.962% (23353/45824)\n",
            "Loss: 1.345 | Acc: 50.959% (23384/45888)\n",
            "Loss: 1.346 | Acc: 50.951% (23413/45952)\n",
            "Loss: 1.346 | Acc: 50.961% (23450/46016)\n",
            "Loss: 1.345 | Acc: 50.970% (23487/46080)\n",
            "Loss: 1.345 | Acc: 50.964% (23517/46144)\n",
            "Loss: 1.345 | Acc: 50.972% (23553/46208)\n",
            "Loss: 1.346 | Acc: 50.951% (23576/46272)\n",
            "Loss: 1.346 | Acc: 50.954% (23610/46336)\n",
            "Loss: 1.346 | Acc: 50.955% (23643/46400)\n",
            "Loss: 1.346 | Acc: 50.962% (23679/46464)\n",
            "Loss: 1.345 | Acc: 50.959% (23710/46528)\n",
            "Loss: 1.345 | Acc: 50.962% (23744/46592)\n",
            "Loss: 1.345 | Acc: 50.962% (23777/46656)\n",
            "Loss: 1.345 | Acc: 50.959% (23808/46720)\n",
            "Loss: 1.345 | Acc: 50.962% (23842/46784)\n",
            "Loss: 1.346 | Acc: 50.954% (23871/46848)\n",
            "Loss: 1.345 | Acc: 50.970% (23911/46912)\n",
            "Loss: 1.345 | Acc: 50.975% (23946/46976)\n",
            "Loss: 1.345 | Acc: 50.969% (23976/47040)\n",
            "Loss: 1.345 | Acc: 50.964% (24006/47104)\n",
            "Loss: 1.345 | Acc: 50.965% (24039/47168)\n",
            "Loss: 1.345 | Acc: 50.961% (24070/47232)\n",
            "Loss: 1.345 | Acc: 50.960% (24102/47296)\n",
            "Loss: 1.345 | Acc: 50.944% (24127/47360)\n",
            "Loss: 1.345 | Acc: 50.936% (24156/47424)\n",
            "Loss: 1.346 | Acc: 50.935% (24188/47488)\n",
            "Loss: 1.346 | Acc: 50.927% (24217/47552)\n",
            "Loss: 1.346 | Acc: 50.926% (24249/47616)\n",
            "Loss: 1.346 | Acc: 50.925% (24281/47680)\n",
            "Loss: 1.346 | Acc: 50.913% (24308/47744)\n",
            "Loss: 1.346 | Acc: 50.914% (24341/47808)\n",
            "Loss: 1.346 | Acc: 50.892% (24363/47872)\n",
            "Loss: 1.347 | Acc: 50.887% (24393/47936)\n",
            "Loss: 1.346 | Acc: 50.879% (24422/48000)\n",
            "Loss: 1.346 | Acc: 50.880% (24455/48064)\n",
            "Loss: 1.346 | Acc: 50.889% (24492/48128)\n",
            "Loss: 1.346 | Acc: 50.882% (24521/48192)\n",
            "Loss: 1.347 | Acc: 50.872% (24549/48256)\n",
            "Loss: 1.347 | Acc: 50.875% (24583/48320)\n",
            "Loss: 1.347 | Acc: 50.870% (24613/48384)\n",
            "Loss: 1.347 | Acc: 50.875% (24648/48448)\n",
            "Loss: 1.347 | Acc: 50.891% (24688/48512)\n",
            "Loss: 1.347 | Acc: 50.883% (24717/48576)\n",
            "Loss: 1.347 | Acc: 50.876% (24746/48640)\n",
            "Loss: 1.347 | Acc: 50.883% (24782/48704)\n",
            "Loss: 1.347 | Acc: 50.888% (24817/48768)\n",
            "Loss: 1.347 | Acc: 50.874% (24843/48832)\n",
            "Loss: 1.347 | Acc: 50.886% (24881/48896)\n",
            "Loss: 1.347 | Acc: 50.878% (24910/48960)\n",
            "Loss: 1.347 | Acc: 50.880% (24931/49000)\n",
            "Epoch 2 of training is completed, Training accuracy for this epoch is 50.8795918367347\n",
            "\n",
            "---- Evaluation in process ----\n",
            "Loss: 1.395 | Acc: 45.312% (29/64)\n",
            "Loss: 1.341 | Acc: 46.875% (60/128)\n",
            "Loss: 1.354 | Acc: 47.917% (92/192)\n",
            "Loss: 1.394 | Acc: 47.656% (122/256)\n",
            "Loss: 1.385 | Acc: 48.125% (154/320)\n",
            "Loss: 1.422 | Acc: 46.094% (177/384)\n",
            "Loss: 1.410 | Acc: 45.089% (202/448)\n",
            "Loss: 1.399 | Acc: 45.703% (234/512)\n",
            "Loss: 1.377 | Acc: 47.049% (271/576)\n",
            "Loss: 1.364 | Acc: 48.125% (308/640)\n",
            "Loss: 1.368 | Acc: 47.585% (335/704)\n",
            "Loss: 1.363 | Acc: 48.047% (369/768)\n",
            "Loss: 1.370 | Acc: 47.957% (399/832)\n",
            "Loss: 1.372 | Acc: 48.103% (431/896)\n",
            "Loss: 1.365 | Acc: 48.646% (467/960)\n",
            "Loss: 1.345 | Acc: 49.512% (507/1024)\n",
            "Loss: 1.346 | Acc: 49.540% (539/1088)\n",
            "Loss: 1.343 | Acc: 49.826% (574/1152)\n",
            "Loss: 1.340 | Acc: 50.164% (610/1216)\n",
            "Loss: 1.354 | Acc: 49.922% (639/1280)\n",
            "Loss: 1.351 | Acc: 49.926% (671/1344)\n",
            "Loss: 1.348 | Acc: 50.000% (704/1408)\n",
            "Loss: 1.344 | Acc: 49.728% (732/1472)\n",
            "Loss: 1.346 | Acc: 49.870% (766/1536)\n",
            "Loss: 1.342 | Acc: 50.188% (803/1600)\n",
            "Loss: 1.342 | Acc: 50.421% (839/1664)\n",
            "Loss: 1.342 | Acc: 50.579% (874/1728)\n",
            "Loss: 1.344 | Acc: 50.614% (907/1792)\n",
            "Loss: 1.344 | Acc: 50.754% (942/1856)\n",
            "Loss: 1.346 | Acc: 50.885% (977/1920)\n",
            "Loss: 1.343 | Acc: 51.159% (1015/1984)\n",
            "Loss: 1.345 | Acc: 51.172% (1048/2048)\n",
            "Loss: 1.343 | Acc: 51.278% (1083/2112)\n",
            "Loss: 1.342 | Acc: 51.103% (1112/2176)\n",
            "Loss: 1.338 | Acc: 51.027% (1143/2240)\n",
            "Loss: 1.338 | Acc: 51.128% (1178/2304)\n",
            "Loss: 1.338 | Acc: 50.971% (1207/2368)\n",
            "Loss: 1.340 | Acc: 50.781% (1235/2432)\n",
            "Loss: 1.340 | Acc: 50.801% (1268/2496)\n",
            "Loss: 1.348 | Acc: 50.742% (1299/2560)\n",
            "Loss: 1.346 | Acc: 51.067% (1340/2624)\n",
            "Loss: 1.347 | Acc: 51.116% (1374/2688)\n",
            "Loss: 1.346 | Acc: 51.126% (1407/2752)\n",
            "Loss: 1.346 | Acc: 51.065% (1438/2816)\n",
            "Loss: 1.345 | Acc: 51.111% (1472/2880)\n",
            "Loss: 1.342 | Acc: 51.155% (1506/2944)\n",
            "Loss: 1.340 | Acc: 51.230% (1541/3008)\n",
            "Loss: 1.337 | Acc: 51.204% (1573/3072)\n",
            "Loss: 1.333 | Acc: 51.276% (1608/3136)\n",
            "Loss: 1.333 | Acc: 51.281% (1641/3200)\n",
            "Loss: 1.333 | Acc: 51.103% (1668/3264)\n",
            "Loss: 1.332 | Acc: 51.112% (1701/3328)\n",
            "Loss: 1.329 | Acc: 51.179% (1736/3392)\n",
            "Loss: 1.329 | Acc: 51.244% (1771/3456)\n",
            "Loss: 1.329 | Acc: 51.136% (1800/3520)\n",
            "Loss: 1.325 | Acc: 51.339% (1840/3584)\n",
            "Loss: 1.326 | Acc: 51.261% (1870/3648)\n",
            "Loss: 1.327 | Acc: 51.320% (1905/3712)\n",
            "Loss: 1.329 | Acc: 51.165% (1932/3776)\n",
            "Loss: 1.328 | Acc: 51.120% (1963/3840)\n",
            "Loss: 1.327 | Acc: 51.076% (1994/3904)\n",
            "Loss: 1.328 | Acc: 51.084% (2027/3968)\n",
            "Loss: 1.327 | Acc: 51.066% (2059/4032)\n",
            "Loss: 1.328 | Acc: 51.074% (2092/4096)\n",
            "Loss: 1.329 | Acc: 51.130% (2127/4160)\n",
            "Loss: 1.327 | Acc: 51.231% (2164/4224)\n",
            "Loss: 1.326 | Acc: 51.096% (2191/4288)\n",
            "Loss: 1.325 | Acc: 51.172% (2227/4352)\n",
            "Loss: 1.322 | Acc: 51.336% (2267/4416)\n",
            "Loss: 1.320 | Acc: 51.429% (2304/4480)\n",
            "Loss: 1.319 | Acc: 51.496% (2340/4544)\n",
            "Loss: 1.318 | Acc: 51.519% (2374/4608)\n",
            "Loss: 1.316 | Acc: 51.584% (2410/4672)\n",
            "Loss: 1.314 | Acc: 51.562% (2442/4736)\n",
            "Loss: 1.316 | Acc: 51.562% (2475/4800)\n",
            "Loss: 1.314 | Acc: 51.706% (2515/4864)\n",
            "Loss: 1.314 | Acc: 51.725% (2549/4928)\n",
            "Loss: 1.314 | Acc: 51.703% (2581/4992)\n",
            "Loss: 1.314 | Acc: 51.661% (2612/5056)\n",
            "Loss: 1.317 | Acc: 51.484% (2636/5120)\n",
            "Loss: 1.317 | Acc: 51.601% (2675/5184)\n",
            "Loss: 1.317 | Acc: 51.620% (2709/5248)\n",
            "Loss: 1.316 | Acc: 51.600% (2741/5312)\n",
            "Loss: 1.318 | Acc: 51.562% (2772/5376)\n",
            "Loss: 1.318 | Acc: 51.544% (2804/5440)\n",
            "Loss: 1.321 | Acc: 51.526% (2836/5504)\n",
            "Loss: 1.325 | Acc: 51.419% (2863/5568)\n",
            "Loss: 1.330 | Acc: 51.332% (2891/5632)\n",
            "Loss: 1.330 | Acc: 51.352% (2925/5696)\n",
            "Loss: 1.329 | Acc: 51.337% (2957/5760)\n",
            "Loss: 1.329 | Acc: 51.219% (2983/5824)\n",
            "Loss: 1.329 | Acc: 51.138% (3011/5888)\n",
            "Loss: 1.329 | Acc: 51.159% (3045/5952)\n",
            "Loss: 1.329 | Acc: 51.164% (3078/6016)\n",
            "Loss: 1.331 | Acc: 51.069% (3105/6080)\n",
            "Loss: 1.333 | Acc: 51.058% (3137/6144)\n",
            "Loss: 1.332 | Acc: 51.063% (3170/6208)\n",
            "Loss: 1.335 | Acc: 51.004% (3199/6272)\n",
            "Loss: 1.334 | Acc: 51.057% (3235/6336)\n",
            "Loss: 1.335 | Acc: 51.000% (3264/6400)\n",
            "Loss: 1.335 | Acc: 51.006% (3297/6464)\n",
            "Loss: 1.334 | Acc: 51.088% (3335/6528)\n",
            "Loss: 1.336 | Acc: 51.032% (3364/6592)\n",
            "Loss: 1.335 | Acc: 51.067% (3399/6656)\n",
            "Loss: 1.337 | Acc: 51.042% (3430/6720)\n",
            "Loss: 1.336 | Acc: 51.120% (3468/6784)\n",
            "Loss: 1.333 | Acc: 51.139% (3502/6848)\n",
            "Loss: 1.336 | Acc: 51.027% (3527/6912)\n",
            "Loss: 1.337 | Acc: 50.989% (3557/6976)\n",
            "Loss: 1.337 | Acc: 51.009% (3591/7040)\n",
            "Loss: 1.337 | Acc: 51.028% (3625/7104)\n",
            "Loss: 1.338 | Acc: 50.949% (3652/7168)\n",
            "Loss: 1.338 | Acc: 50.996% (3688/7232)\n",
            "Loss: 1.336 | Acc: 51.001% (3721/7296)\n",
            "Loss: 1.335 | Acc: 51.114% (3762/7360)\n",
            "Loss: 1.333 | Acc: 51.185% (3800/7424)\n",
            "Loss: 1.332 | Acc: 51.162% (3831/7488)\n",
            "Loss: 1.333 | Acc: 51.152% (3863/7552)\n",
            "Loss: 1.335 | Acc: 51.129% (3894/7616)\n",
            "Loss: 1.335 | Acc: 51.107% (3925/7680)\n",
            "Loss: 1.334 | Acc: 51.162% (3962/7744)\n",
            "Loss: 1.334 | Acc: 51.204% (3998/7808)\n",
            "Loss: 1.334 | Acc: 51.143% (4026/7872)\n",
            "Loss: 1.334 | Acc: 51.172% (4061/7936)\n",
            "Loss: 1.334 | Acc: 51.150% (4092/8000)\n",
            "Loss: 1.334 | Acc: 51.128% (4123/8064)\n",
            "Loss: 1.334 | Acc: 51.156% (4158/8128)\n",
            "Loss: 1.334 | Acc: 51.160% (4191/8192)\n",
            "Loss: 1.335 | Acc: 51.114% (4220/8256)\n",
            "Loss: 1.336 | Acc: 50.998% (4243/8320)\n",
            "Loss: 1.336 | Acc: 51.050% (4280/8384)\n",
            "Loss: 1.335 | Acc: 51.065% (4314/8448)\n",
            "Loss: 1.337 | Acc: 51.046% (4345/8512)\n",
            "Loss: 1.336 | Acc: 51.096% (4382/8576)\n",
            "Loss: 1.337 | Acc: 51.088% (4414/8640)\n",
            "Loss: 1.336 | Acc: 51.114% (4449/8704)\n",
            "Loss: 1.337 | Acc: 51.038% (4475/8768)\n",
            "Loss: 1.337 | Acc: 51.030% (4507/8832)\n",
            "Loss: 1.336 | Acc: 51.000% (4537/8896)\n",
            "Loss: 1.336 | Acc: 51.004% (4570/8960)\n",
            "Loss: 1.336 | Acc: 50.997% (4602/9024)\n",
            "Loss: 1.337 | Acc: 50.913% (4627/9088)\n",
            "Loss: 1.336 | Acc: 50.907% (4659/9152)\n",
            "Loss: 1.334 | Acc: 50.998% (4700/9216)\n",
            "Loss: 1.334 | Acc: 51.056% (4738/9280)\n",
            "Loss: 1.335 | Acc: 51.049% (4770/9344)\n",
            "Loss: 1.335 | Acc: 51.095% (4807/9408)\n",
            "Loss: 1.336 | Acc: 51.077% (4838/9472)\n",
            "Loss: 1.336 | Acc: 51.080% (4871/9536)\n",
            "Loss: 1.335 | Acc: 51.146% (4910/9600)\n",
            "Loss: 1.336 | Acc: 51.097% (4938/9664)\n",
            "Loss: 1.336 | Acc: 51.079% (4969/9728)\n",
            "Loss: 1.337 | Acc: 51.021% (4996/9792)\n",
            "Loss: 1.337 | Acc: 50.994% (5026/9856)\n",
            "Loss: 1.338 | Acc: 51.008% (5060/9920)\n",
            "Loss: 1.338 | Acc: 50.982% (5090/9984)\n",
            "Loss: 1.337 | Acc: 51.000% (5100/10000)\n",
            "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 51.0\n",
            "\n",
            "Final train set accuracy is 50.8795918367347\n",
            "Final test set accuracy is 51.0\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "input_dims = 3\n",
        "hidden_dims = 128\n",
        "output_dims=10\n",
        "num_trans_layers = 4\n",
        "num_heads=4\n",
        "image_k=32\n",
        "patch_k=4\n",
        "\n",
        "network = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims, output_dims=output_dims, num_trans_layers = num_trans_layers, num_heads=num_heads, image_k=image_k, patch_k=patch_k)\n",
        "optimizer = torch.optim.Adam( network.parameters() , lr= learning_rate )\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             \n",
        "################################################################################\n",
        "\n",
        "tr_accs=[]\n",
        "test_accs=[]\n",
        "for epoch in range(3):\n",
        "    tr_acc = train(network, optimizer, loader_train)\n",
        "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
        "              .format(epoch, tr_acc))  \n",
        "    \n",
        "    test_acc = evaluate(network, loader_test)\n",
        "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
        "              .format(epoch, test_acc))  \n",
        "    \n",
        "    tr_accs.append(tr_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    \n",
        "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
        "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
      ],
      "id": "5089f05c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0543c1f3"
      },
      "outputs": [],
      "source": [],
      "id": "0543c1f3"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9abbbe7d21a941e5aebb8c5cdc615d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c14e91c65e64acbabfa94a7ebd0236d",
              "IPY_MODEL_552c3089fce24d3a8373550a8a03f51f",
              "IPY_MODEL_46742caa7e7e4ae3aa8044fa86578ac0"
            ],
            "layout": "IPY_MODEL_ecf9f1794d534c62b1dd11f3cb6db8b8"
          }
        },
        "1c14e91c65e64acbabfa94a7ebd0236d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb9a53244e5743bf9fc2bfdc654a365f",
            "placeholder": "​",
            "style": "IPY_MODEL_109ce988c5174e22b2f122819a37b982",
            "value": "100%"
          }
        },
        "552c3089fce24d3a8373550a8a03f51f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9763b792e4f4e278be75608c10f73ac",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7aa0066fd1f74602ae8787ee1e7f0740",
            "value": 170498071
          }
        },
        "46742caa7e7e4ae3aa8044fa86578ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4df72aa4154443bf9a178de78035b89a",
            "placeholder": "​",
            "style": "IPY_MODEL_d411c27c8c464da29d7a20cc2b425d2f",
            "value": " 170498071/170498071 [00:02&lt;00:00, 73096068.15it/s]"
          }
        },
        "ecf9f1794d534c62b1dd11f3cb6db8b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb9a53244e5743bf9fc2bfdc654a365f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "109ce988c5174e22b2f122819a37b982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9763b792e4f4e278be75608c10f73ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa0066fd1f74602ae8787ee1e7f0740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4df72aa4154443bf9a178de78035b89a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d411c27c8c464da29d7a20cc2b425d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}